\chapter{Serverless architecture for web applications}

The growth of cloud computing influenced the way how the applications are developed and managed nowadays by outsourcing the resource management to the cloud providers as~well as scaling the services within minutes by allocating new machines with proportional billing. The serverless paradigm can be perceived as a next step in the cloud computing progression, introducing significant mind shift from thinking about servers to building the~applications from loosely coupled services, configured to work together to perform the~processing.

There is no clear definition of serverless as described in section \ref{section:serverless-definition}, however many sources describe the features of the serverless architecture, including among the others no~need to manage infrastructure, which is handed off to the cloud platform, responsible for~provisioning resources, executing and scaling the components to meet the demand with granular billing according to resource usage. Moreover, the architecture of the solution consists of multiple components configured to work together, running entirely in~the cloud environment and introducing a fine grained development and deployment model.
The~idea of Function as a Service is used to execute code in an event-driven manner, inside an~ephemeral and stateless execution environment, within strictly limited time. Alongside the serverless functions, the Backend as a Service components are heavily used to provide various functionalities or replace parts of existing application logic by integrating with their API, as discussed in section \ref{section:serverless-components}.

Although the serverless is considered as not fully matured technology, it is already adopted by various companies and organisations that noticed numerous advantages in~using the technology, as covered in section \ref{chapter:serverless-benefits-and-challenges}.
The development and operational cost reduction, along the scaling capabilities with billing proportional to resource usage as well as gaining agility and reducing time to market, are mentioned as desirable features of the serverless paradigm.
Nevertheless, the serverless technology is not free from downsides. The stateless and anonymous nature of FaaS allows the platform to scale, but requires communication with external components to preserve the state and exchange messages as well as initiates discussions about performance. The outsourcing of the infrastructure, handed off to the cloud providers, reduces the amount of work, but on the other hand it integrates tightly with the platform, relying on it entirely in terms of reliability and~security. The serverless architecture requires to make a mind shift accordingly, when~developing and modeling the solution along with testing and monitoring its behaviour in the cloud environment.

Many cloud providers noticed the interest in the serverless technology and introduced numerous services in their portfolio, as described in section \ref{chapter:serverless-service-providers}, enabling developers to~build the serverless applications using their platform.
It led to developing numerous production grade implementations, covering a vast range of use cases and serverless platform capabilities, as briefly presented in section \ref{chapter:serverless-example-use-cases}.

The second of the discussed fields, describes the evolution of web applications over the years, introducing various architectural and development patterns to meet the requirements put on them.
The idea of web application is difficult to clearly present due to~numerous purposes they may serve and various architectures they implement, as mentioned in \ref{chapter:web-apps-definition}. However, the overall view can be summarised and cover any client-server application using the HTTP protocol, with the client running in the web browser environment, while the server can be any complex system by itself.

Despite the diversity in the perspective of the web applications, some of the defined requirements can be shared by the majority of them, with applicability extent based on the use case, as noticed in section \ref{chapter:web-apps-requirements}.
It is desired for the web application to scale according to~the workload demands, without noticeable performance degradation, preserving availability and characterising with resiliency and fault-tolerance, because failures are inevitable in more complex systems. Moreover, with the increase in the data volumes processed, along with the growth of their importance and confidentiality, the system should remain safe and compliant with various regulations. Last but not least, the maintainability of the web application is important, covering the monitoring, observability and~deployment processes from the operational point of view, along with the ease of development by removing complexity, inconsistencies and preserving good modularity to ensure a satisfying agility level when developing new features.

To fulfill the specified requirements, various approaches and architectural patterns have been introduced, as covered briefly in section \ref{section:web-apps-modern-web-application-architecture}.
With the server applications becoming more complex, they started to be split into several smaller and decoupled services, communicating with each other, leveraging microservice architecture and using event-driven approaches to model the problems and process them more effectively.
It introduced the~need for various message intermediaries, such as message brokers and message buses, along with the new models of the database management systems to meet the requirements of~the large volumes of data being stored and characterise with performance, availability and~reliability, leading to further tradeoffs in other areas.
Simultaneously, the~client application architectures evolved, providing more complex and interactive clients, communicating with the server application using the benefits of various communication protocols to fulfill the~requirements and exchange information effectively.

\section{Research}

The aim of the thesis is to analyse the applicability of the serverless processing and the~FaaS model in the field of web application development. Chapter \ref{chapter:serverless-computing} introduces thoroughly the concept of serverless computing, while chapter \ref{chapter:web-apps} describes briefly the topic of web applications and its architectures used nowadays. 

\subsection{Research questions} \label{section:research-questions}

To understand how the serverless architecture and Function as a Service model can be~applied in the web application development, the following research questions are defined and~are a subject of further research.

\begin{enumerate}
    \item Is serverless paradigm a suitable technology for building web applications?
    \item How the serverless computing and FaaS model can be applied to process the web~application workloads?
    \item What are the characteristics of the storage components used in the web applications built in the serverless paradigm?
    \item How the serverless architecture affects the communication with web application clients?
\end{enumerate}

\subsection{Research approach} \label{section:research-approach}

To answer the defined questions, further analysis of the literature, reference architectures and articles provided by various practitioners is conducted.
Based on the gathered information, the example implementations are provided to reason about different capabilities of the serverless architecture and its applicability in the field of web application development.
Detailed description of the examples and the proposed solutions can be found in section \ref{chapter:example-implementations}.

The research focus mainly on the services provided by the Amazon Web Services to narrow the scope of the thesis and provide more in depth analysis of the Amazon's cloud platform capabilities in terms of the serverless computing.
The solution has been selected due to high quality of services and development-friendly tooling provided by the cloud vendor along with numerous undergoing innovations in the field of the serverless computing, setting new standards in terms of the platform possibilities.
Moreover, the detailed documentation along with reference architectures and broad community of practitioners, provide a sufficient area to investigate the topic of serverless architecture.
Nevertheless, the portfolio of other cloud providers, mentioned in section \ref{chapter:serverless-service-providers}, present similar services with comparable capabilities, which should meet the characteristics of the offerings provided by AWS in the near future to maintain the market competitiveness.

\section{Serverless suitability for web applications} \label{chapter:serverless-suitability}

The serverless paradigm is becoming more popular and appealing to numerous companies due to benefits it can bring, such as reducing operational and development cost, shortening time to market as well as outsourcing some of the operational and management overhead.
Numerous startups decide to adopt serverless architecture to build their products faster, while larger and more traditional enterprises want to become more agile and reduce their operational costs.
Despite the fact that serverless is used extensively by some of the organisations, the applicability of the serverless paradigm is not fully comprehend and it~is~not entirely clear what problems are suitable to be implemented using this architecture.
Cloud vendors provide guides and describe use cases of serverless applicability, but neglect to present when the serverless architecture is not suitable and has not been used effectively \cite{EvaluationOfServerlessApplicationProgrammingModel}.

The serverless architecture is a fairly new approach with various benefits, but also disadvantages and limitations, which are described in more details in chapter \ref{chapter:serverless-benefits-and-challenges}.
In~order~to consider the suitability of serverless paradigm, some approach would be to create an intuition about the architecture characteristics, by leveraging the benefits it brings and~understanding the limitations it has.
However, some of the restrictions can be overcome by providing workarounds or redesigning the software to fully utilise the capabilities of~the~architecture.

The section below covers the topic in more details, summarising the knowledge about benefits and challenges along with the deeper look into use cases and examples, to answer the research question considering the serverless suitability for web based services.

\subsection{Serverless suitability}

\subsubsection{Workload types}

\paragraph{Utilisation patterns} \label{chapter:serverless-suitability-utilisation-patterns}

The significant benefit of the serverless paradigm is the autoscaling capability, which~enables platform to track the load with greater fidelity, scaling up quickly in case of demand increase and scaling down shortly after in the absence of it.
Moreover, all of the processing is billed proportionally to the services actual usage in terms of execution time, number of~processed requests or volume of transported data \cite{BerkeleyServerless}.

Mentioned characteristics make the serverless architecture applicable for dynamic and~irregular workloads and when the services experience little traffic, which can benefit from the automatic and rapid scaling capabilities and granular pricing model \cite{EvaluationOfServerlessApplicationProgrammingModel}.
It greatly mitigates the problem of underprovisioning and overprovisioning by the automatic, granular scaling handled entirely by the cloud platform \cite{MartinFowlerServerless}.
Nevertheless, moderate and~consistent load, which does not utilise the advantage of the serverless architecture, may be more suitable to be executed on the dedicated virtual machines or containerised environment due to lower overall cost of the solution, without the need to scale based on~the~execution behaviour \cite{LeveragingServerlessCloudComputingArchitectures}.

When selecting the architecture it is essential to consider the cost of resources and operational work, required to handle the expected load of the service.
The serverless architecture can be more cost efficient when handling unpredictable or occasional load due to its autoscaling capabilities. It can be considered as a good and generic rule when choosing architecture, but it is advised to perform more thorough cost estimation, taking into account the architecture utilisation under the expected load.
However, the effective cost estimation for the serverless solution may not be trivial with different pricing models for~various components and amount of data being transported, especially when handling peaking traffic or managing more complex systems \cite{EvaluationOfServerlessApplicationProgrammingModel}.
On the other hand, the~reduction of overhead related to the operational management due to the infrastructure outsourcing can be also appealing and become a tradeoff being made, from the business point of view.

\paragraph{Processing times} \label{chapter:serverless-suitability-processing-time}
 
One of the biggest flaws pointed out to the serverless architecture is the ''cold start'' caused by the resource provisioning, leading to significant latency and unpredictable response time \cite{BerkeleyServerless}.
The phenomenon, caused by resource allocation and environment bootstrapping, is a subject of various studies, dedicated to understanding it better and~mitigating its effects. Moreover, cloud vendors continuously work on the various solutions and~virtualisation techniques to reduce the latency. Some of the vendors provide dedicated services such as AWS Provisioned Concurrency, there are also other programmatic solutions for~pre-warming the serverless functions, which mitigate the problem, but comes with additional cost \cite{MartinFowlerServerless}.

The most significant impact of the ''cold start'' on the response time is visible, when multiple functions are sequentially combined to perform some workflow. Along with the~latency caused by the communication overhead, the compounded processing time may become significantly longer \cite{EvaluationOfServerlessApplicationProgrammingModel}.
To prevent these undesirable situations, it is advised to~keep the user-faced function chains as short as possible to make the response time short as well.

Both factors mentioned above may indicate that serverless paradigm is not suitable for the highly-performant and mission critical workloads that need to guarantee certain response time requirements.
It is more difficult to ensure the performance of the serverless solution, due to unpredictable performance of the underlying serverless platform, but~depending on the characteristics of the developed application, the additional latency added to the response time may be acceptable \cite{LeveragingServerlessCloudComputingArchitectures}.

The serverless solution can be effectively utilised to perform some background tasks, which does not require to directly respond to the users. Moreover, due to scaling capabilities the workloads can be effortlessly parallelised, reducing the overall processing time.

\subsubsection{Serverless processing limitations}

\paragraph{Runtime and data restrictions} \label{chapter:serverless-processing-limitations-runtime-and-data-restrictions}

Another factors significantly impacting the serverless suitability are the runtime and data restrictions of the Function as a Service processing model. These were covered in more details in chapter \ref{chapter:serverless-challenges-related-to-the-nature-of-serverless-architecture}, when considering the challenges related to the nature of the serverless architecture.
Regardless of the provider, serverless functions are designed to be effectively stateless, running code within ephemeral and isolated containers, constrained by the limited processing time. Configured amount of the memory, CPU units and network bandwidth is allocated from the shared resource pool, assigned to the container and~deprovisioned shortly after processing finishes.
Such architecture enables functions to scale horizontally, but on the other hand it requires to preserve state in external components, which can introduce significant overhead when large volumes of data are transported.
Moreover, developers do not have control over the container execution and their addressability, which requires to use external components serving as intermediate messaging brokers \cite{MartinFowlerServerless}. Lastly, various BaaS components can also have limitations, for example constraining the~operation throughput or volume of the data that can be processed.

Mentioned limitations can make the serverless paradigm not suitable for some problems, which require fine-grained data sharing or significant amount of coordination and~communication, due to overall messaging overhead or the cost of external intermediary services.
Nevertheless, various practitioners and architects analyse numerous problems and~provide ideas for limitation workarounds or redesign some of the solution architectures to~fit the~serverless paradigm or utilise it within hybrid solution \cite{BerkeleyServerless}.

When it comes to various cloud providers, the runtime restrictions can differ and it is~crucial to consider if the problem characteristics fit within the constraints of the vendor services \cite{LeveragingServerlessCloudComputingArchitectures}.

AWS Lambda \cite{AWSLambdaQuotas} as one of the most developed FaaS offerings, allows to configure the memory in range of 128 MB to 10GB, with proportionally allocated CPU up to 6~vCPU cores for multithread processing \cite{AWSLambdaRAMandCPU}.
The serverless function execution time is limited to 900 seconds, with invocation payload up to 6 MB for synchronous invocation and 256~KB for asynchronous.
The temporal, local disk storage available for the container is~limited to~512~MB.
The deployment package of the function in the form of zip archive is~restricted up to 50 MB when compressed. When uncompressed the limit is set to 250~MB, including up to 5 layers, which are a solution to share some common code across multiple functions \cite{EvaluationOfServerlessApplicationProgrammingModel}.

Lastly, major cloud providers introduced various solutions for running custom execution runtimes using container images, which may be a solution to overcome some of~the mentioned limitations, but feasibility of such solution was not entirely researched yet \cite{AWSLambdaContainerImageSupport}.

\paragraph{Operation types} \label{chapter:serverless-suitability-operation-types}

When considering the serverless billing model with granular cost proportional to the used resources, it is crucial to utilise the available computing power as efficiently as possible. The CPU bound operations including the intensive calculations, such as image manipulation or some sophisticated mathematical computation, are more suitable for the serverless processing model and can fully utilise the underlying allocated resources.

On the other hand, the I/O bound operations, like loading large volumes of the data, inserting large sets of data to the database or heavy network communication, may be less suitable from the cost perspective, because the operations can be relatively slow, extending the function processing time. Moreover, when considering the communication with some external I/O components, each container invocation may require establishing new connections, introducing additional latency \cite{LeveragingServerlessCloudComputingArchitectures}.

Due to that fact, some of the dedicated services capable of I/O operations are redesigned for serverless architecture --- the workflow can be transformed into a more event-driven approach. The processing using the third party service is initiated by one of the functions and when the action is performed, the incoming event can trigger another serverless function to process the response, which prevents the serverless function from idle waiting. Serverless functions can be configured to poll some of the BaaS storage components and be notified when a particular action is performed, such as data modification or file insertion \cite{EvaluationOfServerlessApplicationProgrammingModel}.

Another operation constraint is related to the implicit failover, provided out of the box by the serverless platform. When the function processing fails due to some error, it is retried up to several times, based on the underlying configuration. Due to the at least once delivery semantics, the operation performed by the function should be idempotent or additional mechanism needs to be implemented to prevent from subsequent processing of the event \cite{EvaluationOfServerlessApplicationProgrammingModel}.

\subsubsection{Vendor dependence} \label{chapter:serverless-suitability-vendor-dependence}

As with any outsourcing technology, the serverless paradigm relies heavily on the dependence on the service provider, which was discussed in more detail in section \ref{chapter:serverless-cloud-platform-and-vendor-challenges}.
Giving up the control of a significant part of the application, on one hand enables to hand off the infrastructure management overhead, but on the other hand leads to vendor lock-in \cite{MartinFowlerServerless}.

API level lock-in, in which application code relies on the interface of the third party components, can be partially mitigated.
Solutions such as extracting the business logic from the function handler, incorporating ports and adapters pattern to rely on abstract interfaces, rather than concrete service implementations or using some of the serverless frameworks, enables to abstract over the cloud providers interfaces to a certain degree.

The service level lock-in, in which the developed solution relies on particular feature or behaviour of the external service, can be harder to mitigate, due to the fact that there may be no alternative components with similar capabilities, blocking the migration and~requiring to redesign some of the functionalities \cite{EvaluationOfServerlessApplicationProgrammingModel}.

Moreover, the tooling related to the deployment, monitoring, logging and configuration may require to be replaced when changing the cloud vendor platform \cite{MartinFowlerServerless}.

The cloud provider is responsible for maintaining, updating and protecting the underlying architecture and generally can ensure sufficient level of services, because of the~knowledge, experience and qualified staff it has. However, incidents happen due to some unpredictable events or human errors, causing unexpected outages and connection disruption, even in the largest data centers \cite{EvaluationOfServerlessApplicationProgrammingModel}.

When deciding on the service architecture, it is essential to consider if such vendor dependence is bearable \cite{LeveragingServerlessCloudComputingArchitectures}.
The vendor lock-in in the serverless architecture can be seen as a trade-off rather than limitation, when considering whether the benefits one can obtain are sufficient to sacrifice the elasticity compared to other architectures.

\subsection{Serverless applicability for web based services}

\subsubsection{Suitability for web based workloads} \label{chapter:serverless-suitability-for-web-based-workloads}

The idea of web application has been introduced and discussed in more detail in chapter \ref{chapter:web-apps}. Modern web applications can take a variety of forms and implement numerous architectures to fulfill the requirements put in front of them.
It is difficult to generalise and unambiguously answer the question whether the serverless architecture is suitable for the web applications, because the number of variables deciding on the applicability is significant.

It is considered that the serverless paradigm can be utilised successfully for building entire systems or incorporate isolated components, implementing granular or larger tasks, into the existing system \cite{ServerlessArchitectureOnAWS}.

The examples provided in section \ref{chapter:serverless-example-use-cases} cover a significant amount of use cases, showing how the services were designed to utilise the serverless architecture and its benefits in the production grade services. The section below discusses in more detail the applicability of serverless architecture, covering the examples and their capabilities as well as providing additional information to claim the serverless suitability for described workloads.

\paragraph{MindMup} \label{chapter:serverless-suitability-mindmup}

Mindmup, serving the role of online collaboration tool for mind mapping, described in~section \ref{chapter:serverless-example-event-drive-processing}, is one of the early adopters of the serverless paradigm, which has undergone an~architectural migration from the traditional architecture running on Heroku, experiencing significant operational cost reduction \cite{ServerlessComputingEconomicAndArchitecturalImpact}.
The redesign of the exporters' services enabled developers to reduce the boilerplate code responsible for managing, logging and monitoring of the processing as well as focusing only on the converter's logic running in~the serverless functions.
Due to the fact that some of the file formats were used less frequently, all the processing logic was initially bundled into single service for cost optimisation. The~negative aspect of such coupling has been observed by exhausting all of the available resources, along with bugs propagation impacting several exporters.
The~re-architecturing of the flow to event-driven approach, with the logic orchestration pushed to the client as~well~as the further migration of the application backend, contributed to~the~cost reduction by 50\% even with the increase in number of active users --- resulting in the overall cost savings estimated to 66\%.
The MindMup example of exporters processing benefits from utilising the serverless architecture for the high-throughput task, utilising fine-grained serverless function scaling.

\paragraph{Yubl} \label{chapter:serverless-suitability-yubl}

Another example of the early adopter of the serverless technology was the London based, social networking company named Yubl \cite{ServerlessComputingEconomicAndArchitecturalImpact}.
Initially, their solution was implemented as a monolithic system, serving as a backend for a mobile application.
The significant problem of the platform was the spiky social media traffic, exceeding up to 70 times the~normal usage pattern, especially when the advertisers or influencers posted some news.
The autoscaling feature of the AWS Elastic Compute Cloud has been enabled by engineers, but the time required to provision new instances was too long to handle the instant increase in demand.
After lowering the threshold responsible for triggering the scaling, the rapid traffic growth could be partially handled, but in the long run it led to notable resource overprovisioning.

The migration to serverless architecture was based on the cost reduction, possibility of~more frequent deployments and minimising the operational overhead related to~the~maintenance of the system.
Despite the fact that Yubl has been closed, because of not securing another round of funding, the engineers migrated significant part of the application backend, using around 170 different serverless functions in production, with the cost savings estimated to 95\%.
Moreover, breaking the monolithic system into separate functions reduced the number of conflicts when integrating new features and improved the agility, enabling developers to introduce new features quicker.
Working on smaller units of the~system allowed to an increase in the number of production deployments from 4-6 to about 80 on a monthly basis.

\paragraph{CloudGuru} \label{chapter:serverless-suitability-cloudguru}

The CloudGuru educational platform is an example of a fully-fledged web application, serving the role of a scalable application backend, including numerous workloads with different characteristics, as described in more detail in section \ref{chapter:serverless-example-application-backend}.

The simpler tasks include API calls, triggering the AWS Lambda to execute the code responsible for business logic, communicating with external BaaS services to perform some action or preserve the state in the database.
The video processing task is an example of~a~more complex workflow, initiated after uploading the video directly to S3 bucket, preceded by obtaining the presigned URL granting secure access.
The instructor,~who~uploaded the video can be further notified, when the asynchronous action is completed, following with the transcoded video being uploaded to the S3 bucket, along with~the~update of its status in the database which makes the video available for students.
The~real-time communication capabilities of Firebase are a convenient solution, enabling users to~exchange messages, while the background jobs are further managed to index the forum entries for search purpose.

What is interesting, is the fact that the described architecture includes services from multiple providers --- Netlify as a host for the static web application, Auth0 as identity provider responsible for authenticating and authorizing users and Firebase as a main database, directly accessed from the client. Nevertheless, all of the largest cloud providers, covered in more details in section \ref{chapter:serverless-service-providers}, include in their portfolio similar services, allowing developers to build the complete, production ready application using resources from a~single platform.

The initial architecture of the CloudGuru platform can be described as a serverless, RESTful monolith based on the API Gateway and AWS Lambda for processing and Firebase as a primary database.
The example describes the real, sophisticated and production-grade system, supporting thousands of users on a daily basis and scaling to the demand, while keeping low operational cost (on a monthly basis the API Gateway and AWS Lambda cost was estimated below 1000\$).

Despite the fact that the system was performing well, the architecture has undergone a significant transformation, as described in the second edition of the ''Serverless Architecture on AWS'' book \cite{ServerlessArchitectureOnAWSSecondEdition}, initially written by Peter Sbarski \cite{ServerlessArchitectureOnAWS}.
The CloudGuru case study describes an interesting example of re-architecturing the large, serverless-based web application.
As the major problems mentioned as a migration reason were tight coupling between different parts of the system as well as shallow isolation of the domains' boundaries.
Making some changes to a single, primary Firebase database most frequently affected multiple functions and was leading to conflicts and overlaps during development.

The company decided to split the monolithic, serverless architecture into multiple parts, resembling the microservices approach based on different parts of the product, in~which each service ensured proper boundaries and owned its data for particular business context.
The GraphQL has been used to prevent from overfetching and multiple round trips when accessing the data by the clients.
The Backend for Frontend pattern enabled engineers to expose different endpoints for web and mobile clients, running custom GraphQL servers in AWS Lambda and combining the responses from multiple microservices.
Each serverless microservice has been split into stateless components (API Gateway, AWS Lambda) and stateful components (DynamoDB, S3) deployed independently.
Common services, such as AWS Virtual Private Cloud, AWS Web Application Firewall and Amazon Redshift serving as data warehouse, have been extracted and loosely coupled with~specific microservices.

The desired outcome of the migration has been achieved --- teams are working more independently without affecting others, owning responsibility for their microservices and at the same time noticing performance improvements.
The serverless architecture enabled developers to migrate the platform gradually, implementing new features while keeping the~old one serving the production traffic.
The developers could focus on the migration process without worrying about provisioning resources and their cost, by adjusting the~services configuration described in the Infrastructure as a Code approach and letting the~platform provide new resources \cite{ServerlessArchitectureOnAWS}.

\paragraph{Temenos} \label{chapter:serverless-suitability-temenos}

The banking system developed by Temenos, effectively utilises the event-driven architecture, implementing to some degree the Event Sourcing approach along the Command Query Responsibility Segregation, described in section \ref{chapter:event-driven-architecture}.
The first part of the processing pipeline is responsible for the Online Transactions Processing (OLTP), accepting the transactions data and serving the role of command operations. It consists of the API Gateway, AWS Elastic Load Balancer, AWS Fargate which is running containerised application, including the business logic and saving the transactions in the database, hosted by~AWS~Relational Database Service.
Later, the AWS RDS is streaming the data to~Amazon Kinesis, which aggregates the records and triggers the AWS Lambda to process them, storing the results in the DynamoDB in the form of a read-optimized model, which is~a~subject of performing query operations.

The serverless solution developed by Themenos enabled to decouple the processing, providing proper scalability level, effectively incorporating the serverless paradigm for high-throughput data ingestion tasks performed in the background, as part of the larger hybrid solution.

\subsubsection{Comparison with traditional architectures} \label{chapter:comparison-with-traditional-architectures}

The common use-case for web application servers is to provide an API used by the clients running in the web browsers.
There are several research \cite{ServerlessComputingAnInvestigationOfDeploymentEnvironmentsForWebAPIs} \cite{MicroservicesvsServerlessAPerformanceComparisonOnCloudNativeWebApplication} analysing the~performance, scalability, reliability and cost of the API built using serverless, microservices and monolithic architectures.

Mentioned papers compare simple API services, performing basic operations using the~AWS Lambda, communicating with the database or processing some CPU-bound computation.
They simulate user behaviour with different traffic characteristics and workload volumes within a specified period of time, measuring the response time of the implementations, using different architectures with comparable resource configuration. The results of research are categorized and concluded below.

\paragraph{Serverless is more agile in terms of scalability and with more stable performance characteristics}

During the initial phase of load testing, it was noticeable that the serverless implementation suffered from the ''cold start'' impacting the response time.
After the period of 2 minutes, the performance has stabilized and the response time remained almost constant until the~tests completion.
The implementation utilising serverless technology characterise with instant scaling, matching the increase in workloads.
The higher number of requests or~unexpected spikes are handled automatically, with small impact on the response time and slight performance degradation observed \cite{ServerlessComputingAnInvestigationOfDeploymentEnvironmentsForWebAPIs}.

Which is in contrast to the solution implemented using the microservice architecture, which starts to scale after reaching preconfigured utilisation criteria.
Moreover, for some types of the traffic increase, the delay of responsiveness to re-balance the current workloads was greater, which lead to notable increase in the response time \cite{MicroservicesvsServerlessAPerformanceComparisonOnCloudNativeWebApplication}.

\paragraph{Microservices suffer from load balancing and traffic distribution problems}

The solution implemented in the microservice and monolithic architecture was capable of maintaining the performance level for the stable and smaller loads, but characterized with longer response time under more rapid traffic growth. After the autoscaling utilisation threshold is met, more instances are added into the cluster, improving the response time, which with further scaling could match the performance of serverless implementation \cite{ServerlessComputingAnInvestigationOfDeploymentEnvironmentsForWebAPIs}.

Moreover, the high peak response time was observed for the microservices implementation, scattered randomly during tests, which could be potentially caused by the traffic distribution within the cluster, when the new instances were added or deprovisioned. Due~to~that, the serverless solution, despite the initial ''cold start'' period, notes more stable and favourable performance characteristics \cite{MicroservicesvsServerlessAPerformanceComparisonOnCloudNativeWebApplication}.

\paragraph{Serverless performance is comparable with the microservices or monolithic architecture}

Overall, the serverless implementation was characterised with comparable performance, leading in some of the tests over the microservices, especially when heavily affected by~the~load~increase.
However, the microservices handled the small size and repetitive tasks more efficiently, which could be a result of smaller processing overhead, related with the virtualisation stack and involving multiple components in the serverless implementation \cite{MicroservicesvsServerlessAPerformanceComparisonOnCloudNativeWebApplication}.

Moreover, with the growth of the load the performance degradation of the monolithic implementation significantly impacted the response time, while the solution based on~microservices degraded performance much slower. \cite{ServerlessComputingAnInvestigationOfDeploymentEnvironmentsForWebAPIs}

\paragraph{Cost effectiveness depends on workload characteristics}

The cost analysis conducted by one of the authors of the papers \cite{ServerlessComputingAnInvestigationOfDeploymentEnvironmentsForWebAPIs} claims that the~solution utilising the virtual machines is more cost efficient --- resulting with the estimated cost to be 12 times lower without autoscaling and 6 times lower with autoscaling enabled, which still could not match the serverless performance for some tests.
Nevertheless, the~additional cost of the data transfer and other components, such as load balancer, storage, database are not included into the research.
Some of the mentioned components are specific to the architecture based on the virtual machines, which could increase the~cost, along with the additional labour related to the maintenance of the solution.
On~the~other~hand, when the traffic is stable and the virtual machines are configured to match the workload, the solution can be much cheaper and preserve the suitable performance characteristics.
For one of the tests purposes, the serverless architecture used AWS Lambda configured for~the~1.5~GB of memory allocation. Optimising the function configuration in terms of the~performance to cost ratio, could reveal that other setting could yield similar performance with lower cost, because AWS Lambda price is established based on the memory size per execution time basis.
The research concluded that there is no clear choice and~one need to carefully measure the cost based on the estimated traffic characteristics and~include the price of all components used in both implementations.

\subsection{Fulfilment of the web application requirements} \label{section:fulfilment-of-the-web-application-requirements}

The discussion on the serverless suitability and further analysis of the example implementations, using the serverless architecture for a variety of web based workloads, can indicate that the serverless paradigm finds the applicability for web based workloads.
Some~of~the~thoroughly designed implementations can bring significant benefits in terms of the solution scalability, performance, cost reduction and development agility.
The section covers in~more detail how the serverless paradigm is applicable for the web application requirements, presented in the chapter \ref{chapter:web-apps-requirements}.

\subsubsection{Performance and scalability} \label{chapter:serverless-suitability-performance-and-scalability}

The stateless and short-lived design of the serverless function makes it convenient for horizontal scaling, however the performance from the point of view of the single request is~still debatable, considering the ''cold start'' phenomenon, as described in sections  \ref{chapter:serverless-startup-latency-and-platform-improvements} and \ref{chapter:serverless-suitability-processing-time}.
Moreover, the ephemeral nature of the processing and lack of addressability requires to use external components to preserve the state and communicate with other services, as noticed in section \ref{chapter:serverless-challenges-related-to-the-nature-of-serverless-architecture}.
The time required to allocate the resources and~bootstrap the runtime of the function, along with the communication overhead between the~components that make up the workload processing, can significantly impact the overall processing time.
However, the architects and developers with the thoroughly designed application architecture can partially alleviate the undesired latency to make it~acceptable for the developed web application.
Moreover, the cloud providers tend to~introduce further improvements to mitigate the unpredictability of the serverless platform and~provide dedicated functionalities to make the execution and response time foreseeable.

On the other hand, when thinking about the performance of the system, the users are~not using the application alone.
Most commonly, the solution is utilised by several users at the same time, who perform various actions in the application.
The research discussed in section \ref{chapter:comparison-with-traditional-architectures} covers the analysis of performance and scalability of the serverless based systems, compared with the more traditional architectures.
It results with conclusions that the performance of the serverless implementations is comparable with more traditional architectures, with more stable characteristics and lower degradation ratio, especially when the application experiences increase in the traffic and needs to scale accordingly to meet the workload demand as well as maintain the satisfactory response time.

The serverless components, based on their definition described in section \ref{section:serverless-definition}, are characterized by fine-grained scaling capabilities, enabling instant horizontal scaling to meet the demand.
The scaling is handled automatically by the cloud platform in response to~the~events coming to the system, affecting only the components required to process them, as~mentioned in sections \ref{chapter:serverless-autoscaling-with-proportional-cost} and \ref{chapter:serverless-suitability-utilisation-patterns}.
The serverless architecture is designed to track the load and respond to it with greater fidelity, compared to scaling of~the~solution based on containers or virtual machines, as discussed in section \ref{chapter:comparison-with-traditional-architectures}.
From the application examples covered in section \ref{chapter:serverless-suitability-for-web-based-workloads}, the Yubl social media platform leveraged the scaling capabilities to meet their spiky traffic, significantly reducing the cost when migrating their product to the serverless architecture.

\subsubsection{Reliability} \label{chapter:serverless-suitability-reliability}

Considering the serverless as the outsourcing technology, the significant amount of~the~overhead and responsibility, required to manage the system and ensure it is running properly, is handed off to the cloud provider, as described in section \ref{chapter:serverless-suitability-vendor-dependence}. Having the knowledge, experience and qualified staff, the cloud provider is able to ensure the reliability level, exceeding the services clients could guarantee by themselves. Nevertheless, as mentioned in section \ref{chapter:serverless-suitability-vendor-dependence}, unpredictable events or human errors happen and the disruption and~outages are inevitable. To prevent the service downtimes, the serverless solution can be~deployed and replicated into multiple Availability Zones managed by the cloud provider. Each of the datacenters is independently supplied with the electricity and network access, which gives a possibility to redirect the traffic between them when some disruption happens.

The section \ref{section:serverless-definition} describing the serverless components characteristics, mentions that the services are designed with implicit high availability. Multiple serverless components provide various retry mechanics in case of errors occurring during the processing, related to~issues with the application logic or hardware faults. For example, by default AWS~Lambda invoked asynchronously retries the event processing 2 times upon processing failure. If~the~processing is not completed successfully after retries, based on the~configuration the event can be placed in Dead Letter Queue (DLQ), which is a dedicated Amazon~SQS instance for handling failed messages.

Along with the programmatic solutions, establishing processes including proper monitoring, observability and triggering alarms in case of the undesired components behaviour could be beneficial, mitigating further administrator intervention or even restoring the~system automatically to the appropriate state.
For the production running application, the~solutions similar to discussed in the section \ref{section:web-apps-microservices} considering the microservices architecture as well as all of the efforts required to ensure proper monitoring and observability described in more details in section \ref{chapter:serverless-monitoring-and-observability} could be applied.
The automated tests running as part of the deployment pipeline, on the environment mirroring the one available in~production, along with the load tests can indicate various discrepancies.

The serverless solution reliability can further benefit from the higher level deployment techniques and instant rollback capability, in case of observing issues after the release. Lastly, the serverless function can utilise the services providing dynamic configuration, enabling them to alter their behaviour by toggling certain features or implementing a solution similar to the circuit breaker pattern, preventing from the whole system degradation in case of bugs introduced into the implementation.

\subsubsection{Security and compliance} \label{chapter:serverless-suitability-security-and-compliance}

As described in section \ref{chapter:serverless-security-concerns}, the serverless paradigm opens a lot of security related questions, having the field not fully researched yet. The cloud vendors provide numerous guidelines and recommendations regarding their services, aiming to maintain appropriate security level for applications running in the cloud and their configuration.
One~of~the~factors that cloud providers need to take into account is the multitenancy aspect, mentioned in section \ref{chapter:serverless-multitenancy-problem}, that requires preserving appropriate level of isolation between tenants' workloads to guarantee compliance.
AWS introduces a shared responsibility model, in~which the security of the underlying infrastructure including the hardware, default software and the network configuration is maintained and protected by the cloud vendor, while the client is responsible for ensuring the compliance of the application and~its configuration.

Moreover, numerous services and their configuration help developers with ensuring security for serverless applications. As an example, the API Gateway can be configured to~perform authentication and authorisation for requests, provide data protection and~input validation, while the AWS Web Application Firewall can provide DDoS protection and rate limiting to restrict the undesired and dangerous behaviour \cite{EvaluationOfServerlessApplicationProgrammingModel}.

Nevertheless, the new attacks considering the serverless architecture are emerging and~highlighting the security problems of the serverless applications. The greater granularity of the serverless function deployments, makes their dependencies updated less often, reducing the frequency of security patches of the third party libraries.
The direct access to the BaaS components is restricted by default, but their improper configuration can lead to various incidents, exposing confidential information or leading to spreading a malware.
Other types of the attacks include improper authentication configuration, insecure secrets storage, function event’s data injection and inadequate logging and monitoring setup \cite{EvaluationOfServerlessApplicationProgrammingModel}.

Most of the problems with the security of the serverless solutions results from the misconfiguration, however the security awareness is developing along with guidelines on how to protect the serverless workloads and guarantee their compliance with various security regulations.

\subsubsection{Maintainability} \label{chapter:serverless-suitability-maintainability}

As described in the definition in section \ref{section:serverless-definition}, the serverless paradigm transfers the need to~maintain, provision and manage resources and execute applications to the cloud platform. It significantly reduces the operational costs of running the applications as well as the~labour cost related with the operational aspects, as noticed in sections \ref{chapter:serverless-reduced-development-and-operational-cost} and \ref{chapter:serverless-easier-operational-management}. The deployment process is also simplified, in which the developers need to provide the artifact with the proper configuration and the platform is responsible for executing the application.

Section \ref{chapter:serverless-monitoring-and-observability} indicates that the granularity of the components and the distributed nature of the serverless processing requires to setup proper monitoring and ensure appropriate level of observability to make sure the application is working properly.
Leveraging the tools capable of distributed tracing, along with alarming when some metrics are over the predefined limits, can give more insight into the application behaviour.

Moreover, leveraging automation allows to configure multiple checks and perform automation tests in the deployment pipelines to give confidence when deploying the serverless solution. Due to the ephemeral nature of the serverless processing, which is performed in the cloud, the end-to-end tests conducted against the mirrored testing environment running in the cloud are required to verify the correctness of the software. Along with the~advanced operational tooling capabilities, the releases can be enhanced with the canary deployment or the A/B testing, driven by the appropriate monitoring and ensuring that the application works properly, as covered in section \ref{chapter:serverless-testing}.

The developers can benefit from multiple BaaS components, introducing new opportunities and reducing the development time, as highlighted in section \ref{chapter:serverless-development-opportunities}. On the other hand, the serverless paradigm is a fairly new approach of building services with the new ideas and patterns emerging and best practices created to further guide the development \ref{chapter:serverless-development-and-debugging}.
The development process can be harder initially, because of the greater granularity of the functions compared to traditional architectures and a new approach of running and ~testing the applications in the cloud environment \cite{EvaluationOfServerlessApplicationProgrammingModel}. Nevertheless, the ecosystem of tools evolved over the years along with the knowledge base of the cloud providers and~practitioners, describing reference architectures and good patterns.

Lastly, the serverless architecture makes it easier to introduce changes and adjust the~architectures to the current needs with very little effort. The example of CloudGuru migration, described in section \ref{chapter:serverless-suitability-cloudguru}, shows that serverless makes the evolvability of~the~application easier, along with encouraging to experiment and introduce new features in~a~convenient way.

\section{Example implementations} \label{chapter:example-implementations}

The further research takes up two example web services implemented in the serverless architecture.
The selected problems consider some of the existing web applications and~tasks they perform, which are shortly introduced in the section below, along with the overview of~the~implemented serverless solutions.
The provided examples are used to~discuss some features of the serverless architecture along the implemented patterns to~analyse in~a~greater detail the implication of the serverless paradigm in the developed solution.

\subsection{Receipt processing} \label{chapter:examples-receipt-processing}

\subsubsection{Problem description}

First example considers the service similar to PanParagon\footnote{\url{https://panparagon.pl/}}, which is a mobile application that enables users to scan and collect fiscal receipts, which are further analysed to~gather some statistics or can be used for the return or complaint of the products, or~as~a~reminder of the return date or warranty periods.
The application is capable of obtaining the information from the receipt, such as name of the shop, purchase date and amount of~money. Based on the retrieved data, the receipt can be further categorised and included in~the~spending statistics, giving users more insight into their shopping history. Furthermore, the application provides integration with several external services, allowing users to~show current offers and promotions or electronically insure their equipment.

The service can be simply reimplemented as the web application, which uses the~same backend for the calculations, business logic and integration with external services. The~more demanding task would be to implement the receipt processing flow. Taking into consideration the variety of services available in the cloud, some of the components could be incorporated in the application to perform the document and text analysis, or~even help with the data classification using the natural languages processing techniques. Moreover, the~cloud provider takes responsibility for hosting and scaling the application along with its~more demanding tasks, alleviating the need of managing the services and developing them from the ground up.

\subsubsection{Designed solution}

\begin{figure}[]
    \centering
    \includegraphics[width=1\textwidth]{assets/04-serverless-for-web-apps/paragoneArchitecture.png}
    \caption{Architecture of receipt processing web application}
    \label{fig:paragone-web-app}
\end{figure}

Key points of the suggested architecture:

\begin{itemize}
    \item \textbf{GraphQL as API} --- Presented example shows the basic backend of the web application built using the serverless technology. AWS AppSync is used to expose the~GraphQL API, which is configured to trigger the AWS~Lambda or communicate directly with some external services, such as DynamoDB. Moreover, the integration with AWS Cognito introduces the authentication and authorization capabilities to~the~API. Each of the GraphQL operations, such as queries, mutations and subscriptions can be restricted, to ensure that only logged in users have access to~their private data. Once the receipt processing is completed and the data is stored in~the~database, the user is notified in real-time about the results of the asynchronous operation, based on the established GraphQL subscription. The AWS~Lambda, that is polling the DynamoDB for the event stream, including inserts and updates for~a~given table, triggers the GraphQL mutation, which in turn pushes the new data to~the~client.
    \item \textbf{Secure and direct access to services storing the files} --- The GraphQL API, built on top of the AWS AppSync services, is not designed with intent to handle the operations on the files, including downloads and uploads. Instead, the mechanism of~presigned URLs is incorporated. It enables the client application to directly communicate with the Amazon S3 in a secure way, to access or store the files with predefined names and within a limited and strictly defined period of time. It alleviates the need of using the AWS Lambda to process and insert the data into the bucket upon the~user request, which could be more time consuming operation compared to generating the presigned URL.
    \item \textbf{Asynchronous receipt processing} --- The receipt processing flow is performed in an asynchronous manner, similarly to many other processing flows, utilising the~serverless paradigm. Uploading the receipt to Amazon S3 triggers AWS~Lambda, which is responsible for inserting metadata about the image to the database and~sending the receipt to Amazon Textract performing the document analysis task. Once the~processing is completed, the Amazon SNS receives the event with the job identifier, which further triggers the AWS Lambda, subscribing to the particular topic. The serverless function retrieves the data and based on a simple heuristics, analyses the results of the text extraction process and stores the information further in~DynamoDB.
    \item \textbf{Web application hosting} --- The backend of the application, composed from the~combination of configured with each other services mentioned earlier, is effectively hosted in the cloud. Similarly, the client application once built, it~can~be~stored in~a~publicly available Amazon S3 bucket, in the form of the static assets. Amazon~CloudFront distribution can be additionally integrated to provide the capabilities of Content Delivery Network, delivering the client application to users in a more convenient way.
\end{itemize}

Along with the simple heuristics retrieving the information based on the text extraction process, the more sophisticated services could be incorporated in the receipt processing flow, for example to categorise the expenses, or provide more detailed spending statistics. Nevertheless, the other services would also operate in the same, asynchronous manner, extending the presented flow and performing the computation in a form of a distributed system, composed of several components hosted in the cloud.

\subsubsection{Discussion}

The proposed solution presents the fully fledged web application, which is built using the serverless architecture, including the serverless backend responsible for processing and storing the data along with the client application hosted in the cloud environment.
The~presented architecture is also a good illustration of different execution models of the AWS Lambda, covered in more details in section \ref{chapter:lambda-function-execution-model}, including synchronous execution to obtain the presigned URL, creating an asynchronous receipt processing flow as well as polling the stream of updates from the DynamoDB.

The asynchronous processing choreography, described in more details in section \ref{chapter:serverless-processing-function-composition-and-orchestration}, is a common pattern in the serverless architecture and refers to composition of multiple components, responsible for particular chunks of the processing and forming effectively a~distributed system.
The AWS Lambda is used to incorporate the business logic of the application, transforming the data when desired and using other services to orchestrate the flow or handle the processing, without waiting idle for the results, but rather being triggered in an event-driven manner.

The use of Amazon Textract brings new development opportunities, allowing developers to integrate the service in the processing flow with convenient billing per usage and~eliminating the need to maintain similar self-hosted service.

AWS AppSync serves as a feature-rich gateway of the application backend, triggering Lambda functions or directly communicating with other services to retrieve the data, but~also it allows to update the client in real-time using the subscriptions, which~is~a~beneficial feature for the asynchronous processing.
All of the processing is performed in~a~secure and compliant way, thanks to the authentication and authorization capabilities, introduced by~the~integrating AWS AppSync with the Amazon Cognito, along with the presigned URL mechanism, enabling application to safely store and retrieve the files from Amazon S3 bucket.

\subsection{Generating interactive slideshow based on LaTeX files} \label{chapter:examples-generating-interactive-slideshow-based-on-latex-files}

\subsubsection{Problem description}

The second example covers an implementation of web application similar to the Pitagoras Korepetytor\footnote{\url{http://pitagoras-korepetytor.pl/}}, which is a service playing a role of the virtual math tutor for high school students.
It provides a large collection of exercises, which can be further adjusted by~users when changing the values of the exercise parameters.
Based on the user input, the~service generates an interactive slideshow, which is further previewed by the user in the interactive client application.
The slides present the following steps required to solve the exercise, with the additional audio commentary, explaining the performed mathematical operations.

\subsubsection{Proposed processing flow} \label{chapter:examples-generating-interactive-slideshow-based-on-latex-files-proposed-processing-flow}

When inspecting the processing results of the original solution, the user is given a set of~slides along with the audio files, which are later processed by the presentation player on the client side.
Due to the large number of the parameters combinations, that can be specified by the user and their value domains, generating the slides and commentary for~all~possible input values can occupy a lot of memory and be unprofitable.
Moreover, some of the presentations include advanced math equations, graphics visualising more sophisticated geometry problems or even considering some conditional logic based on parameters, upon which the solution for the exercise is generated.

Taking into consideration the problem and its requirements described above, the LaTeX \cite{latex} typesetting language and its processor is one of the possible solutions for the~problem, which is well-known and frequently used in the academia community.
It~enables the presentation creator to easily define the mathematical computations, present graphics showing the geometric tasks, which can be also extended based on the defined variables, including even some computations performed by the LaTeX processor or conditional logic.

Taking into account the selected tool, the processing flow for the user request could take the following:

\begin{enumerate}
   \item The initial step, taking place before the processing of the file is started, can~be~used to~validate the user input. The entered values can be compared with the constraints of the exercise, along with verifying if the user can preview the particular presentation, based on the application logic of the service.
   \item Once the input is verified and the user is allowed to request the presentation, the~exercise variables from the input can be injected into the LaTeX file. 
   Next, the~presentation is generated to the PDF file, which is further split into separate slides in~the~form of SVG files.
   \item Along with the slideshow generation, the initial presentation file can include additional directives with the commentary, which are extracted and supplemented with the variables from the user input. The processed text can be transformed further into the human-like audio commentary, using the voice synthesis software.
   \item Additionally, some metadata about the presentation can be retrieved, including the~slide durations for the slideshow or the information when to play the audio commentary, which will be sent and used later by the client application.
   \item The steps 2-4 can be performed in parallel. Once they are completed, the result can~be~afterwards sent to the client to preview the slideshow with the exercise to~the~user.
\end{enumerate}

The described flow presents a rather extraordinary task incorporated in a web application, which plays a significant role in fulfilling the business logic of the proposed services.
In the traditional architecture, the responsibility of performing the presentation processing could be extracted into a separate service, that triggers one of the worker based on~the~user requests.
With the limited traffic, the approach with a separate service communicating with a set of workers would be suitable, but when a larger group of users would submit a request to process a request within a short period of time, the scalability of such a~system should require to scale the number of available workers accordingly.
The instant scalability of the serverless architecture, with a pay-as-you-go billing model, would be beneficial to meet the variable demand of the presentation processing requests.
Moreover, the high-quality services hosted in the cloud, that are capable of performing the voice synthesis, could be integrated in the solution.
However, processing the LaTeX files using the serverless function is a non-standard type of computation task, which requires the function runtime to be properly enhanced to fulfill the task.

\subsubsection{Designed solution}

The designed solution presented in the diagram, covers the presentation processing workflow and communication with the client application, responsible for displaying the slideshow based on the generated data.
The practical research focused on configuring the serverless function, along with the processing optimisations of the described flow, is presented in~more~detail in section \ref{chapter:latex-processing-optimisation}.

\begin{figure}[H]
   \centering
   \includegraphics[width=1\textwidth]{assets/04-serverless-for-web-apps/euclidArchitecture.png}
   \caption{Architecture of presentation processing}
   \label{fig:euclid-web-app}
\end{figure}

Key points of the suggested architecture:

\begin{itemize}
   \item \textbf{Function orchestration using the AWS Step Function} --- The presentation processing flow is built on top of several AWS Lambda instances, with AWS~Step~Function orchestrating the entire process.
   AWS~Lambda performs the initial task by~retrieving the exercise details, validating the user input against the exercise constraints and defining the tasks for subsequent serverless function invocations, if~the~provided input satisfies the limitations.
   Based on the output of the first task, the further processing is executed or the user is notified about the invalid request.
   The~presentation processing task is further separated into two chunks. 
   The first one generates the slideshow for several initial slides, which is pushed to the client as soon as possible once the processing is completed and starts previewing it to the user. While the~second~part, including the remaining chunk of the presentation, is delivered to~the~client later, when the user is busy watching the initial portion of the slideshow.
   For~each of~the~chunks, three AWS Lambda instances are executed concurrently. They~are~responsible for processing the LaTeX file and extracting the slides in form of the SVG files that are later uploaded to the Amazon S3, generating the voice commentary using the Amazon Polly, which are further stored in Amazon S3 as well as extracting some metadata about the generated slideshow.
   Once all three tasks for~a~given chunk are completed, the output of the functions is formated and another serverless function is invoked to push the results of the processing to the client using the~WebSocket API
   \item \textbf{Pushing the update to the client} --- AWS Step Function orchestrates the processing in an asynchronous manner, which makes it necessary to notify the client once the processing is finished.
   Instead of using the polling techniques to periodically check whether the presentation is generated, the push based approach is used.
   Initially, the client application invokes the AWS Step Function and gets its execution ID.
   Next, it connects to the WebSocket API and sends a message with the~obtained identifier, invoking the AWS Lambda that stores the execution ID along with the~associated connection ID in the DynamoDB.
   When the chunk of the presentation is~generated, the serverless function retrieves the data based on the execution ID and sends the message with the processing results to the associated client application.
   Finally, the slides and audio files, stored in the Amazon S3 bucket under predefined path, are retrieved.
\end{itemize}

\subsubsection{Discussion}

The proposed solution describes an interesting and non-standard use case of the serverless processing pipeline, responsible for preparing slideshow with the audio commentary based on the LaTeX files.
The predefined slideshows are requested by the user, making the desired outcome of the processing similar to the MindMup’s exporters service, mentioned in section \ref{chapter:serverless-suitability-mindmup}.

The serverless architecture is an appealing technology for the file processing tasks that are scheduled in an event-driven manner.
The resource allocation based on demand, along the autoscaling capabilities with proportional billing and scaling down to zero in absence of requests, makes the serverless technology an applicable and cost efficient solution to use.
However, the processing flow for the described example is more sophisticated and consists of several steps, which require thoughtful coordination.
During the research, It was re-architectured across several subsequent iterations to leverage the benefits of the serverless architecture and make the slideshow processing more efficient.
The design process along with the performance comparison and cost calculation is presented in more detail in section \ref{chapter:latex-processing-optimisation}.

The proposed solution relies on the orchestration of the processing, that consists of the serverless function invocations, performing various tasks on different steps of the processing.
AWS Step Function enables the processing to preserve state between the functions execution along with orchestrating the flow, which can include some branching, tasks executed in parallel, mapping over a list of subtasks and gathering the results of the group of~tasks.
The solution brings quite some elasticity when defining the processing schema and provides visualisation of the workflow, giving better insight into the executed processing.
However, the size of accumulated state in the AWS Step Function is strictly limited, which is insufficient to store the results of LaTeX files processing there, and requires the~use~of~Amazon S3 to preserve the processed files from different stages of~the~computation to make them later available to the user.

Similarly to the previous example, the processing is performed in an asynchronous manner and requires some additional solution to update the client once the results are~ready. The proposed solution invokes the AWS Step Function using the REST API and later connects via the WebSocket API to receive notification when the slideshow is available.

Another challenge with the proposed processing flow is related with the LaTeX files processing. The use of Lambda layer, including the additional dependencies, makes it~possible to perform such custom tasks and enhance the Lambda runtime with additional capabilities.

Lastly, the use of the AWS Polly provided another development opportunity, enabling to easily enhance the slideshow with the audio commentary, without the need to host similar service or incorporate some other third party service, running outside of the cloud platform.

\section{Server Tier}

\subsection{AWS Lambda as a compute resource} \label{chapter:lambda-function-as-a-compute-resource}

The idea of Function as a Service has been already introduced in section \ref{chapter:serverless-faas}.
An example of such FaaS offering is AWS Lambda \cite{AWSLambda}, which is described as a compute service that requires no resource management. It provides the autoscaling capabilities, built-in high-availability and pay-per-use billing model, based on the function execution and the~processing time with a granularity of milliseconds. 

The following section analyses in more detail the FaaS capabilities, using AWS~Lambda as one of the implementations of this model, taking into account its limitations, runtime possibilities, execution models and possible optimisations.
Based on the overview of AWS~Lambda functionalities, the intuition of FaaS model capabilities as an execution environment can be established.

\subsubsection{Function limitations} \label{chapter:lambda-function-limitations}

The AWS Lambda execution is constrained by quotas referring to the compute and storage resource that can be used by the serverless function.
Some of the AWS Lambda restrictions have been already discussed in section \ref{chapter:serverless-processing-limitations-runtime-and-data-restrictions}.
The list below includes additional limitations along with explanation of their impact on the processing of the serverless workloads \cite{AWSLambdaQuotas}.

\begin{itemize}
   \item \textbf{Concurrent executions} --- 1000 executions --- The serverless paradigm ensures about the infinite scalability of the underlying platform, however the default maximum number of functions that can be invoked simultaneously is limited. It~is~essential to keep that in mind when developing some serverless solution extensively using the Lambda, that there is some threshold above which the function execution will be throttled. The limit can be increased further by the cloud provider.
   \item \textbf{Function memory allocation} --- from 128 MB to 10240 MB --- The function memory allocation can be configured, which translates into the available CPU and network bandwidth. When the memory of the function is set above the 1769 MB threshold, it provides processors with multiple cores capable of multithreading processing. Therefore, further increase for single threaded computation will increase the memory or bandwidth, which will be suitable accordingly for memory-bound or~I/O-bound processing \cite{BecomeAServerlessBlackBelt}.
   \item \textbf{Function timeout} --- 900 seconds (15 minutes) --- The Lambda execution time is strictly limited, which makes it not suitable for processing various long-running tasks.
   \item \textbf{Deployment package size} --- 50 MB (zipped) and 250 MB (unzipped, including layers) --- The function deployment package, including the function code and its dependencies, is constrained as well. The function execution environment can be additionally extended by the Lambda layers, including additional code, libraries or~dependencies. When the size constraints do not allow to pack all of the required dependencies, the function can be also based on the custom container image, limited up to 10 GB. The topic of Lambda layer and container image is discussed further~in section \ref{chapter:lambda-custom-runtimes}.
   \item \textbf{Invocation payload} --- 6 MB (synchronous invocation), 256 KB (asynchronous invocation) --- The invocation payload limit is related to the size of an event invoking the Lambda function.
   The synchronous invocation model requires the serverless function to return some value to the service triggering the function, for example the AWS API Gateway invoking the function based on user request, contrary to~the~asynchronous invocation, triggered for example by uploading some file to~Amazon~S3.
   The constraint prevents from passing large amounts of data in the event payload and~requires the function to communicate with external components to retrieve or~save the processing results if required.
   \item \textbf{Temporary directory storage} --- 512 MB --- Each of the invoked function environment is assigned a temporary storage, enabling the function to retrieve some data and~perform the required processing in the local file system, which due to~the ephemeral nature of the function containers, is further deprovisioned. The results of~the processing need to be preserved in some dedicated storage component, depending on the type of the data being stored. Alternatively, under some circumstances, AWS~Lambda can utilise the larger storage space when integrated with Amazon Elastic File System, which is a fully managed, shared file system service \cite{AWSLambdaEFS}.
\end{itemize}

Besides the appealing features mentioned initially, such as the Lambda scaling capabilities with proportional billing and the high-availability without the need for managing infrastructure, the further quotas describe how the Lambda runtime is limited.
The constrained execution time shows the limitation of the serverless architecture, along with the~limited invocation payload, which requires the function to communicate with external services from Lambda after the invocation. However, thanks to improvements in other areas, the Lambda can be configured to utilise greater amount of memory and multi-core processors, the deployment package can be extended when using the custom container image along with connecting the temporal disc space to the shared file system.
Mentioned enhancement makes it possible to use the AWS Lambda in various, non-standard processing tasks, while leveraging its benefits.

\subsubsection{Function runtime} \label{chapter:lambda-custom-runtimes}

AWS Lambda supports natively several programming languages and their runtime environments, including Node.js, Python, Java, C\#, Go and Ruby, with some of them including different versions of the runtime \cite{AWSLambdaRuntimes}. Most frequently the function code and its dependencies are compressed and packed by the deployment tool into a single zip archive and~further stored by the cloud provider. When the Lambda function is invoked, the serverless platform allocates the resources to form the function environment, downloads the function code, bootstrap the language runtime with function dependencies and execute the function. However, when the function has been recently triggered, the execution environment from a previous invocation can be reused, saving time required to prepare the environment and reusing some resources, such as established connections with external services or the temporary files.

Besides the standard functions' packing option and environment, AWS Lambda can use Lambda layers, providing a capability to share dependencies between several functions as well as custom container images.
Both of the approaches are described in more details below.

\paragraph{Lambda container image}

AWS Lambda allows to deploy the serverless functions based on custom Docker or OCI~images, which enable developers to build serverless functions arbitrary from the supported programming languages and required dependencies \cite{AWSLambdaContainerImage}. Moreover, it allows to process other workloads depending on sizable dependencies, such as machine learning or other data intensive tasks.
Container images are constrained up to 10 GB in size, contrary to the aforementioned limit of the zipped function artifact, which make container images suitable to include custom runtimes, binaries and libraries, that can be used by the serverless function.
AWS provides a set of base images for supported runtimes as well as supported Linux base image, however the developers can use any container image if it supports Lambda~Runtime~API, which is a simple HTTP-based protocol, responsible for invoking the function and retrieving its results.
The custom container images can be built using familiar tooling and later uploaded to Amazon Elastic Container Registry, from which the~image will be retrieved once the function is invoked.

Compared to other services enabling to run container based workloads, the Lambda container image support allows to scale workloads on per request basis and scale to zero with no additional cost if there is no traffic, providing more granular pricing model than other services. Moreover, the Lambda function is integrated with many AWS services, which means that it can be invoked from numerous event sources. 
Nevertheless, the~Lambda still needs to satisfy the aforementioned limitations, other than the artifact size \cite{RunningContainerImagesInAWSLambda}.

\paragraph{Lambda layer}

When taking into consideration the encapsulated nature of the FaaS, several Lambda functions operating in the same part of the application logic can have some common methods, utilities and dependencies, effectively duplicating the same code in every deployed artifact.
Lambda layer is effectively a zip archive including additional code, dependencies and libraries, that can be shared across multiple Lambda functions, reducing the overall size of individual function artifacts.
When including the layer in a function, its content is extracted into function's local file system and can be further accessed by the function code, enabling functions to share common files and mitigating the problem of code duplication.
Moreover, the Lambda layer can include other, operating-system specific libraries and binaries used by the function code as well as custom runtimes, enabling developers to add required dependencies and execute code developed in any programming language in Lambda functions.
The function can include up to 5 layers, with the total size of the function code and layers summing up to 250 MB after decompressing \cite{AWSLambdaLayer}. \\

The serverless function introduces an interesting approach in terms of developing and~deploying the function code, which is further managed and executed by the cloud platform. Despite the fact that the set of predefined and supported languages along with provided libraries is quite broad, the AWS Lambda execution runtime is limited.
Its extensions, including the Lambda functions based on the custom container images, allows developers to build the Lambda with known container technology, increasing the artifact limit to~10GB, extending the suitability of the serverless function to other, data-heavy types of~workloads.
Moreover, the use of Lambda layers makes it possible to share dependencies across multiple functions easier as well as introduce external libraries or even custom runtimes, thanks to the Lambda Runtime API.
Both of the described approaches allow developers to adjust the Lambda runtime to perform numerous non-standard tasks.

The second example implementation, responsible for generating the interactive presentation based on the LaTeX files, covered in section \ref{chapter:examples-generating-interactive-slideshow-based-on-latex-files}, uses Lambda layers to enhance the serverless function environment with the LaTeX distribution.
Moreover, the performance analysis of both approaches for the aforementioned application is covered in more detail in section \ref{chapter:latex-processing-optimisation}.

\subsubsection{Function execution} \label{chapter:lambda-function-execution-model}

The AWS Lambda is executed in an event-driven manner. 
The serverless function can be invoked by each of the defined triggers, configured based on the various event sources, including numerous services from the AWS cloud platform and their operations. 
The~generic categorisation of data sources and invocation types for the serverless processing is described in section \ref{section:serverless-function-invocation}. 
AWS Lambda execution model fits into the mentioned classification and distinguish the following types of the function invocation \cite{OptimiseYourServerlessApplications}.
All~of~the~mentioned execution models are present in the example implementation of the receipt processing application, mentioned in section \ref{chapter:examples-receipt-processing}.

\begin{itemize}
    \item \textbf{synchronous (push based)} --- Most frequently refers to the components acting as~an~API~Gateway, invoking the Lambda based on the request and when the function completes the execution, it returns the results to the API Gateway, which passes the response back to the client.
    If Lambda fails during the event processing, the error is returned to the service calling the function.
    In case of the receipt processing application, the AWS AppSync invokes the Lambda functions in a synchronous manner, when user performs query to obtain the presigned URL.
    \item \textbf{asynchronous (event based)} --- Covers all cases in which the service makes a~request to the Lambda function, which is consuming an event, but once the execution is~completed, it has no possibility to return the information to the event source. 
    When the processing fails, the function is retried two more times by default.
    After~that, the erroneous event can be placed in the Dead Letter Queue or it can~be directed to other destination, if configured.
    The Lambda function, triggered after the receipt is uploaded to S3 bucket as well as the other one, invoked based on the notification from SNS topic, are the examples of the asynchronous function invocation.
    \item \textbf{stream (poll based)} --- When assigning the stream based services as the data source, Lambda service runs a poller that is watching for the messages and once they are available, the stream of events forming a batch, can be passed to the function via the asynchronous event.
    If the processing fails for some reason, the whole batch can be retried based on maximum record age or maximum retry attempts or the malformed data can be extracted from the rest of the shard and retried separately from the correctly processed events. When the retry limit is reached, the shard can be sent to the separate SQS queue or SNS topic.
    In the receipt processing application, the Lambda is polling the DynamoDB stream and based on received records, it is responsible for performing the mutation to push the update to the client.
\end{itemize}

\subsubsection{Function optimisations}

The Function as a Service model, which is heavily used in the serverless paradigm as~a~compute resource, at a first glance seems to be limited with the ability to configure the function and possibilities to optimise its execution, because the cloud provider takes responsibility for provisioning the resources and executing the function.
However, the thorough design of the application architecture along with the proper composition of the function code and its configuration, regarding the event sources and underlying resources, has a significant impact on the performance of the executed function, which transfers directly to~the~cost of~execution.
The possible optimisations and configuration adjustment suggested by various practitioners for optimising the Lambda function execution and improving the configuration are discussed below.

\paragraph{Lambda execution}

In terms of the Lambda function execution, the improvement possibilities can be divided between the cloud provider and the developers implementing the solution. Taking into consideration the function lifecycle, resource allocation along with the downloading function code is handled by the cloud provider, however the size of deployed artifact can have an impact on the cold start duration. Furthermore, the architecture of the function code can have an influence on initialising the function runtime along with required dependencies as well as executing the function code itself.

\begin{itemize}
   \item \textbf{Concise function logic} --- Focusing on developing functions with single purpose and well defined responsibility enables developers to create leaner functions with fever dependencies, which results with a smaller package once bundled.
   On~the~contrary to the ''monolithic'' functions, that include some branching logic based on the invocation event to execute the particular part of the application logic, which take longer to initialise and can extend the cold start duration \cite{BecomeAServerlessBlackBelt}.
   \item \textbf{Reduce the function dependencies} --- Similarly to the previous point, the~amount and size of dependencies transfers to the function initialisation time.
   Depending on~the~programming language, only selected modules or functions can be imported to incorporate only selected subcomponents of libraries or use techniques like treeshaking to remove unused parts of imported dependencies.
   It allows to obtain smaller function artifacts, with fewer dependencies impacting the function start and execution time \cite{OptimiseYourServerlessApplications}.
   \item \textbf{Use function to transform data, not to transport it} --- Instead of using the Lambda function to transport the data between services, some serverless components from the AWS offering can be integrated to transfer the data directly between each other.
   Similarly, when retrieving the data from external services to the serverless function, it is beneficial to request only the required data instead of performing additional filtering in the function code. It reduces the time, when the function is~waiting for an I/O operation, for example when reading the data from DynamoDB or files from S3, that directly reduces the function execution cost \cite{BecomeAServerlessBlackBelt}.
   \item \textbf{Leverage container reuse} --- The concept of the serverless function assumes that every function execution is stateless and should not rely on existing state from the previous execution.
   However, due to optimisations of the function executions, when the subsequent event triggers the function in a short time after the previous execution is finished, the container environment can be reused along with the initialised runtime and its dependencies.
   The prehandler logic can be used to initialise the connection with external services, such as databases or obtain secrets required in~the~function handler logic, which can be performed once per runtime initialisation, instead of~for every function execution.
   Similarly, the function global state can be used as a~cache for rarely-changing data or lazy-loading variables and dependencies, reducing the~work and processing time for subsequent invocations \cite{AWSLambdaPerformanceOptimization}.
\end{itemize}

\paragraph{Lambda configuration}

\begin{itemize}
   \item \textbf{Configure function resources intentionally} --- As mentioned before, the amount of memory can be configured for each function, which transforms proportionally to~the~CPU and network bandwidth configuration.
   Depending on the type of workloads, the function performance can benefit from additional memory for computation, using multiple cores for multithreading processing, when the memory is configured above the 1.8 GB threshold as well as from additional bandwidth for I/O~bound processing.
   In terms of the cost the Lambda function is billed per function invocation and execution time, proportionally to the amount of configured memory.
   It~is~essential to consider, that the function can process the event faster when having more memory, while having similar cost as the function with lower memory configuration that takes longer to finish the processing.
   To better understand the function execution behaviour depending on configuration and take a data-driven approach, the~AWS~Lambda~Power~Tuning can be used, which is an open-source tool invoking the AWS Step Function, executing the function with a set of predefined configurations, measuring the execution time and its cost \cite{BecomeAServerlessBlackBelt}.
   \item \textbf{Select appropriate event sources} --- When configuring the event trigger for the function, it is beneficial to use the configuration options to discard the uninteresting events. For example the message filter for SNS can select only messages with desired payload to invoke the function, while S3 trigger can specify the action performed on~the~bucket as well as prefix and suffix for the object key, that can be used to~trigger the function, based on operation on a file with particular path, filename or extension \cite{BecomeAServerlessBlackBelt}.
\end{itemize}

Despite the fact that the cloud provider takes responsibility for managing the underlying infrastructure and execution of function code, the way how the function is structured and configured, have a significant impact on its performance.
Applying the knowledge of the Lambda design, by keeping the function lean and without unneeded dependencies along with configuring the application logic in an event-driven manner and leveraging the~container reuse to execute the function code more effectively, it can greatly improve the~performance of the serverless solution.
It is beneficial to keep in mind that the CPU and network bandwidth is allocated proportionally to the configured memory, which can~impact how long it takes for the function to execute.
Moreover, proper function triggers configuration enable it to filter out undesired events.

Both of the provided example implementations follow the indications covered in section above, limiting the unneeded dependencies, leveraging the container reuse to maintain the~connection with the cloud service clients as well as designing the serverless function to~serve a single, well-defined purpose.
The first example, covered in section \ref{chapter:examples-receipt-processing}, includes the asynchronous receipt processing pipeline, which combines the subsequent executions into a workflow, transforming the data each time, instead of making the serverless function wait idle for the results of the image processing task.
The second example, which refers to the generation of interactive presentations, discussed in section \ref{chapter:examples-generating-interactive-slideshow-based-on-latex-files}, benefits from the~altered resource allocation, including additionally allocated CPU to allow the function process the presentations more efficiently.

\subsection{Serverless processing model}

Having described the AWS Lambda as an example implementation of the Function~as~a~Service model, it is essential to consider how the serverless function can be incorporated in~the~application logic.
The event-driven architecture requires some components to trigger the serverless function and as a result receive the response or invoke other components, based on the configuration and code of the function.
Moreover, the stateless and ephemeral nature of the serverless function requires it to communicate with external services to preserve its state.
The serverless architecture relies on various services provided by the cloud platform, which along with the serverless function, make up the application logic \cite{EvaluationOfServerlessApplicationProgrammingModel}.
The simpler application can be composed from the handful of components configured to~work together. 
However, when the complexity of the system grows, the application architecture composed from numerous independent components, communicating with each in an event-driven manner, creates a more complex distributed system, that can benefit from adhering to microservice architecture patterns, discussed in section \ref{section:web-apps-microservices}.

\subsubsection{Serverless microservices}

The architecture of the web based services built with the serverless architecture can be divided into two distinctive categories based on their purpose \cite{AWSReinventBuildingMicroservicesWithAWSLambda}:

\begin{itemize}
   \item \textbf{Public interface} --- Responsible for exposing rich and flexible APIs, most frequently using HTTP protocol to communicate directly with the web based clients. It is using the API Gateway component or other services responsible for load balancing the requests, executing the serverless function, most frequently in a synchronous manner and returning its results to the client.
   \item \textbf{Internal services} --- Supporting the public interfaces by providing various application capabilities. Moreover, the underlying architecture can utilise the asynchronous processing, passing events with more opinionated structure, leveraging other serverless components with simpler interfaces to decouple the internal services and exchange messages using intermediaries.
\end{itemize}

\paragraph{Public interface} \label{chapter:public-interfaces}

The simpler web services can effectively be based on a handful of components to provide the basic functionalities. Services such as API Gateway can trigger the Lambda function, which in turn communicates with the downstream service, responsible for storing the data, performing certain operations and returning back with the response, as presented in~the~Figure \ref{fig:simple-web-service-diagram}.

\begin{figure}[H]
   \centering
   \includegraphics[width=0.7\textwidth]{assets/04-serverless-for-web-apps/simpleWebService.png}
   \caption{Architecture diagram of simple web service using API Gateway, Lambda and DynamoDB}
   \label{fig:simple-web-service-diagram}
\end{figure}

There are several components provided by the AWS, which can serve a role of~the~public, client-faced interface of the web application, integrating with Lambda and other hosted services directly \cite{AWSReinventBuildingMicroservicesWithAWSLambda}. The components and their capabilities are listed below:

\begin{itemize}
   \item Amazon API Gateway ---
   Fully managed service enabling developers to define APIs, serving as a front door to the application, integrating with various services hosted in the cloud platform.
   The service distinguished several types of the API, including REST API (providing variety of additional features such as API caching, additional transformation and validation of requests, tighter integration with other services), WebSocket API as well as HTTP API (simpler and cheaper compared to REST APIs, providing only basic additional features).
   Moreover, The API Gateway can~also provide additional capabilities such as caching, request throttling, different usage tiers for specified clients, security control as well as integration with other services incorporating authentication and authorisation capabilities.
   \item Application Load Balancer ---
   Belongs to the family of the load balancer offering and it is supporting the HTTP traffic, regardless of the transmission protocol used on~top~of~it.
   The requests are forwarded, based on the URL and HTTP method, to the Lambda or other services according to the configuration.
   Using Application Load Balancer can be cost effective when the service is experiencing high throughput and the requests can be passed directly to the associated services.
   \item AWS AppSync ---
   Managed service, providing the capabilities to define GraphQL APIs, which can map the particular requests to various services, pulling the data from various data sources for a single request.
   Moreover, it can incorporate caching on the server and client side to improve performance, leverage GraphQL subscriptions for real-time communication as well as integrate with other services to provide authentication and authorization capabilities.
\end{itemize}

All of the mentioned components provide the capabilities of exposing an API, which serves as an entrypoint for the application logic.
Besides accepting the user requests and~redirecting them to specified components, some of the services enable additional capabilities such as caching, traffic management, request filtering and transformation, authentication and authorization as well as throttling the requests to protect the backend services.
The definition of an API can be generated along with the documentation, when using tools like Swagger \cite{ImplementingMicroservicesOnAWS}.
The additional capabilities of mentioned components are covered in more detail in section \ref{chapter:serverless-client-communication-patterns-for-serverless-architecture}.

Another frequently considered topic, when using the services like API Gateway, is~where to perform the routing of the request.
The Lambda code can effectively use one of the frameworks to provide some additional routing inside the handler logic, granting better portability.
However, such approach introduces several challenges, which include applying the security constructs and performance configuration to the whole monolithic function as~well~as making the proper logging, monitoring and debugging more difficult to implement \cite{AWSReinventBuildingMicroservicesWithAWSLambda}.

\paragraph{Internal services} \label{chapter:interal-services}

The synchronous model of the public interface can work effectively with simpler services, communicating with other serverless components and returning the response back to the client.
However, when the communication is more complex and it includes various services, passing messages to each other, there are several places when the errors can occur, making some of the components waiting for response \cite{AWSReinventBuildingMicroservicesWithAWSLambda}.
When the application is experiencing heavier load, the services communicating in a synchronous manner and leveraging the serverless technologies can observe timeouts and throttling as well as increased cost related with the Lambda functions executing longer and waiting for responses from the polled services.
The API Gateway is configured with maximum integration timeout for synchronous function invocation equal to 29 seconds, after which it returns an error to the client.
Moreover, the function execution time is limited to 15 minutes, which along with its rapid scalability can be more challenging when communicating with some external or legacy services, that are not capable to scale accordingly, requiring to manage the number of active connections \cite{ServerlessAtScaleDesignPatternsAndOptimizations}.

One of the solutions for the mentioned problem, is to rethink how the communication between components works and redesign it to incorporate asynchronous workflows.
It introduces a similar approach as with microservice architecture, making the components decoupled by using some intermediaries like message brokers to exchange and preserve the events occuring in the system in a reliable way.
The API Gateway request can trigger the AWS Lambda, which in turn communicates with some other service, starting the asynchronous processing and returning the invocation status to the API Gateway, responding to the user.
The processing flow can further get through several services, responsible for processing the application logic and preserving the state accordingly, using some of the serverless platform components to exchange the messages reliably, without tight coupling between services and serverless functions.
The communication with external or legacy systems can be appropriately buffered and throttled to prevent from losing events and affecting their performance \cite{ServerlessAtScaleDesignPatternsAndOptimizations}.
Some of the more complex workflows may benefit from the orchestration of the processing using services such as AWS Step Function, which is described in more detail in section \ref{chapter:serverless-processing-function-composition-and-orchestration}. \\

Components provided by AWS, serving the role of queues, message brokers and message buses as well as services streaming the events, enables the developers to decouple the services and redesign the flow to embrace reliable asynchronous processing, are described below:

\begin{itemize}
   \item Simple Notification Service (SNS) ---
   Managed publish-subscribe messaging service with an ability to reliably exchange messages with low latency, pushing and delivering the messages to other components subscribing to a predefined topic.
   SNS provides high throughput with autoscaling capabilities, allowing to fanout the messages, storing them durably across multiple Availability Zones, but providing no additional persistence beyond retry logic, tracking failures when lambda is not available.
   The events can be filtered based on the message attributes, pushing the event only to selected subscribers.   
   The messages can trigger Lambda function or be published to HTTP endpoints, services such as Amazon SQS or Amazon Kinesis Data Firehose, email, SMS or mobile push \cite{ChoosingEventsQueuesTopicsAndStreamsInYourServerlessApplication}.
   In case of serverless functions, each of the published messages translates to single Lambda execution, however its concurrency can be limited as well as additional configuration can be applied to expose parameters allowing to ensure the FIFO ordering and deduplication of events, however by default the SNS does not provide such guarantees.
   SNS is designed for unidirectional and fanout communication, targeted primarily to Lambda function or HTTP based endpoints.

   \item Simple Queue Service (SQS) ---
   Managed message queue service, storing and exchanging the messages between various software components, reliably preserving them without requiring the service to be available.
   SQS is scaling automatically with configurable batch size, storing the messages durably across multiple Availability Zones with an ability to persist them from 60 seconds to 14 days.
   The batch of messages is polled by one of the consumers, becoming unavailable for others. Once the messages from the whole batch are successfully processed, they are removed from the queue, otherwise they become available to be picked up again after the visibility timeout expires.
   It enables the queue to buffer and rate limit the incoming events, securing the connected downstream services from bursty traffic.
   With standard configuration the SQS provides best effort message ordering, with at least once delivery, guaranteeing high throughput with lower cost. However the service can be configured to expose additional information, enabling downstream services to order and deduplicate the incoming events.
   SQS finds applicability when the incoming traffic needs to be stored in a reliable way and buffered to be further consumed by downstream services \cite{AWSReinventScalableServerlessEventDrivenArchitecturesWithSNSSQSLambda}.

   \item Amazon Kinesis Streams ---
   Highly scalable, managed services responsible for collecting and processing real-time streams of data coming from various sources, including data and video streams, enabling the application to preserve the data and analyse it.
   The data stream is split into multiple shards, based on defined buffer capacity and time interval, passing the payload to the Lambda functions which are polling the stream, providing the data to multiple services.
   The data throughput can be limited by batch size or Lambda concurrency, durably storing the messages across multiple Availability Zones and persisting them up to 7 days if configured.
   The processing failures can be handled by retrying the events in order from a shard based on a checkpoints.
   The shard is processed until successful completion or it can be split to extract the erroneous records, handled separately \cite{ChoosingEventsQueuesTopicsAndStreamsInYourServerlessApplication}.
   Amazon Kinesis Streams are frequently used for real-time processing with massive throughput of data, tracking the order of records and gathering the data stream from multiple consumers, such as logs, sensor data or financial transactions.
  
   \item Amazon EventBridge ---
   Serving a role of serverless event bus, observing the events from various data sources, including AWS services, developed applications and integrated SaaS providers. The events can be further filtered out based on defined rules and routed to the downstream consumers, which receive the events reliably, while their producers can remain decoupled.
   EventBridge enables the application components to exchange messages together by ingesting and routing the data across underlying infrastructure, applications as well as external services.
   The messages are stored durably across multiple Availability Zones, however the service does not provide additional events persistence.
   EventBridge finds applicability when the application requires to connect a lot of different services, including external integrations, while providing granular target rules to filter out the traffic \cite{ChoosingEventsQueuesTopicsAndStreamsInYourServerlessApplication}.
\end{itemize}

The mentioned services are not mutually exclusive and can be combined together to utilise their features and provide desired capabilities to the web application architecture as well as enable effective communication by invoking serverless functions in an event-driven manner.
Some of the patterns including described components and leveraging their capabilities are covered below:

\begin{itemize}
   \item \textbf{Throttling and buffering} --- The autoscaling capabilities of the Lambda function can be a problem for the downstream services, including for example the hosted relational databases, which may not be able to scale accordingly or exhaust the available connections pool.
   SQS and Kinesis Data Streams can be used to persist and batch the incoming events.
   Both of the mentioned approaches can limit the concurrency of incoming traffic, serving a role of buffer, aggregating the incoming requests and executing the function with a single batch or shard, leading to lower cost per function invocation.
   DLQ can be configured to store failed records. Alternatively, Lambda Destination can be leveraged to execute another Lambda function with an event including context of the failure \cite{ServerlessArchitecturalPatternsAndBestPractices}.
  
   \begin{figure}[H]
       \centering
       \includegraphics[width=0.8\textwidth]{assets/04-serverless-for-web-apps/buffer.png}
       \caption{Architecture diagram of service using Kinesis Data Streams to throttle and buffer the incoming data}
       \label{fig:throttling-and-buffering-diagram}
   \end{figure}
   
   \begin{figure}[]
       \centering
       \includegraphics[width=0.8\textwidth]{assets/04-serverless-for-web-apps/fanout.png}
       \caption{Architecture diagram of the Fanout pattern including SNS and SQS to reliably send out events}
       \label{fig:fanout-diagram}
   \end{figure}

   \item \textbf{Fan-Out} --- When the payload does not require additional processing, the event can be sent directly to SNS, which pushes the message to subscribers based on the configured filtering. SQS can be configured to subscribe for the events published by SNS, batch the incoming data and share it further with connected Lambda functions or other services.
   Alternatively, for the traffic characterized with high, consistent throughput and large payloads, Kinesis Data Stream can be a more cost efficient solution \cite{ServerlessArchitecturalPatternsAndBestPractices}.
   
   \item \textbf{Streaming} --- The data coming from various data sources can be effectively processed and stored for further analytical purposes. SNS can be used to discard unwanted events, filter the incoming events based on the different business areas and pass the data to Kinesis Firehose to preserve the data, for example in S3 \cite{ServerlessArchitecturalPatternsAndBestPractices}.
   % https://aws.amazon.com/blogs/compute/introducing-message-archiving-and-analytics-for-amazon-sns/
  
   \begin{figure}[H]
       \centering
       \includegraphics[width=0.8\textwidth]{assets/04-serverless-for-web-apps/streamer.png}
       \caption{Architecture diagram of SNS and Kinesis Firehose preserving the incoming stream of events in S3 bucket}
       \label{fig:streaming-diagram}
   \end{figure}
\end{itemize}

\paragraph{Serverless microservice principles}

Besides the components serving the role of the entrypoint to the web application and the internal services, responsible for messaging between application components, there are also other components taking an integral role in the applications built in the serverless paradigm as well as shaping its architecture.
The databases capable of preserving the data and reacting to its changes by streaming the updates to polling services are described in more detail in section \ref{chapter:serverless-datastores}.
Moreover, numerous services hosted in the cloud provide various capabilities, including for example image processing, video transcoding or natural language processing, which can be easily integrated in the developed web applications.

The first of the described examples, presented in section \ref{chapter:examples-receipt-processing}, shows how the S3 and DynamoDB are incorporated in the application logic to preserve the images and results of the processing as well as notifying other components about data changes.
Textract is incorporated in the asynchronous processing flow to analyze the uploaded receipts and later publish the results to SNS, fitting into the asynchronous event-driven processing.

When the serverless solution evolves and grows in complexity, it is beneficial to split the application logic into separate domains with well-defined boundaries based on the business capabilities.
It resembles the microservice architectural pattern, described in section \ref{section:web-apps-microservices}, which refers to a set of small services running on its own, communicating via message passing and focused around specific business capabilities.

The serverless microservices are going a step further, splitting the business logic across several serverless functions.
Each of them is responsible for handling and responding to specific events, with the application logic isolated for particular workflow along with granular configuration in terms of access to external services, underlying resource configuration as well as individual selection of technology and libraries to serve its role effectively.
However, most frequently the serverless functions working within the same service boundary and using the same data sources, define some shared library of methods, extracting the business logic from the function handler, responsible for executing the function, and abstracting away the communication with underlying resources.
Each of the serverless functions include the required methods from the shared library to perform the processing, which are packed along the function handler in the deployment process, reducing the functionalities duplication.

Each of the microservices can operate on its own domain and communicate with others via message passing.
To achieve that, each service should expose an interface to make other services retrieve information or perform defined actions.
Moreover, if the operation in one of the services should affect others, it can publish a message to some intermediaries, notifying other components subscribing for changes to react for them appropriately.
Some serverless functions can be designed to serve as a router, providing the API for other services to communicate with as well as receiver, subscribing for the messages from other components, such as SNS or EventBridge, to trigger the further processing.
It enables the serverless microservices to preserve the loose coupling between different microservices, making the services know as little as possible about the implementation of others, communicate via well-defined interfaces, as well as maintain high cohesion within the same service to effectively manage the business domains \cite{AnIntroductionToServerlessMicroservices}. \\

The described serverless microservice architecture enables developers to leverage the benefits mentioned for the microservice architecture itself.
By decomposing and separating the systems into independent components, it is easier to maintain the system over time, introduce required changes and evolve the application. Different teams can manage and develop different parts of the application, focusing on desired capabilities of the services, deciding on programming languages, libraries, data stores to meet the service requirements as well as deploying, configuring and scaling the services independently.

Moreover, various processes and design principles from the microservices architectural pattern can be also applied to the application built using the serverless paradigm.
The decentralized data management allows services to preserve their state independently, but propagating the more demanding operations in an asynchronous way, requires more sophisticated coordination mechanisms to reliably process the transactions, handle failures and retries, utilising the eventual consistency.
Decoupling components make it easier to develop services independently, however such a distributed system requires some communication mechanisms between its elements.
Exchanging messages via intermediaries, service contracts as well as designing for failure, are necessary to make the serverless web application work properly.
Lastly, the automation that is set up properly, providing continuous integration and deployment, is necessary to make the development process manageable.
Automated end to end testing on the deployed environment is essential to give enough confidence that the system works properly as well as proper logging, monitoring and observability are required to track the system capabilities and trigger alarms in case of inconsistencies. \\

Summarising, according to Jeremy Daly in his article describing the various serverless patterns \cite{ServerlessMicroservicePatternsForAWS}, the~serverless microservices benefits from adhering the following principles:

\begin{itemize}
   \item \textbf{Ownership of the service private data} --- Each of the serverless microservices should own its data. If another service requires the same data, it should be replicated or it is an indication that the services should be combined together or the application architecture should be rethought.
   \item \textbf{Independent deployments} --- The microservices should be independent and self contained. The dependencies and communication should be based on well defined interfaces and handled by intermediaries to ensure the services are loosely coupled. Maintaining this capability allows developers to independently develop, evolve and deploy the services.
   \item \textbf{Utilising eventual consistency} --- The data replication and denormalisation are commonly used in the microservice architecture and the same solution can be applied in the serverless architecture.
   \item \textbf{Using asynchronous workloads} ---
   With the Lambda function billed for the processing time, it is cost effective to consider the asynchronous processing, instead of making the function wait idle or making the processing to reach the timeout.
   Frequently, the processing can be redesigned to hand off the task, which is reliably processed in the background in an asynchronous manner.
   For use cases that require more complicated coordination, the services provided by the cloud platform can be used, keeping the serverless function small, having a single and well-defined purpose.
   \item \textbf{Keeping the service small, but valuable} ---
   With the granularity and elasticity of the serverless architectures, it is applicable to create even some smaller microservices including only a few functions, if they serve their role well and deliver business value.
   Based on the changing requirements and evolution of the application these can be easily relocated to suit the need.
\end{itemize}

\subsubsection{Function choreography and orchestration} \label{chapter:serverless-processing-function-composition-and-orchestration}

The architecture of the serverless function is suitable for the event-driven workflows, in which communication between subsequent components is managed by services such as SNS, SQS and EventBridge, is asynchronous and loosely coupled.
However, some of the operations require a more fine-grained orchestration approach, with capability to preserve the state between subsequent invocations and give more insight into the business context of performed operations.

AWS Step Function is an example of a service providing capabilities of orchestrating the workflow and managing its state.
Step Function enables to define a state machine, responsible for resembling the business workflow as well as triggering individual components and Lambda functions to avoid additional coordination in the code of the services.
It provides necessary abstraction to define sequential and parallel tasks, branching logic, mapping over a set of tasks as well as handling errors and retry logic.
Moreover, the service allows developers to reason about the state of the executed workflow by visualising the components of the orchestrated flow along their state at individual stages of processing \cite{ImplementingMicroservicesOnAWS}.

In the sections below, the choreography and orchestration of the workflows are described in more detail, as two frequently distinguished modes of the interaction in the serverless microservices architecture.

\paragraph{Choreography}

\begin{figure}[]
   \centering
   \includegraphics[width=0.8\textwidth]{assets/04-serverless-for-web-apps/choreography.png}
   \caption{The food delivery workflow modeled using the choreography approach}
   \label{fig:choreography-diagram}
\end{figure}

The choreography refers to an asynchronous, event-driven processing, in which services work independently, using intermediaries in form of message brokers and queues to pass the events to other components invoked further, forming the chain of component invocation performing the business logic.
The interaction model is suitable for simpler operations, when services are not closely related and most frequently belong to separate business domains.
The choreography approach is used in the receipt processing flow, presented in section \ref{chapter:examples-receipt-processing}.

The benefits of such an approach include loose coupling of the services, which can be changed and scaled independently. There is no single point of failure in the system, in which the events can bring additional business value, when processed in the business intelligence reports.
On the other hand, monitoring and reporting in the choreography approach is more challenging, along handling errors, retry mechanisms and timeouts. The business flow is not captured explicitly, instead it is spread across several loosely coupled services \cite{ChoreographyVsOrchestrationInServerlessMicroservices}.

An example architecture diagram of the food ordering workflow, modeled in the choreography approach based on the article written by Yan Cui \cite{ChoreographyVsOrchestrationInTheLandOfServerless}, is presented in Figure \ref{fig:choreography-diagram}.

\paragraph{Orchestration}

\begin{figure}[]
   \centering
   \includegraphics[width=0.8\textwidth]{assets/04-serverless-for-web-apps/orchestration.png}
   \caption{The food delivery workflow modeled using the orchestration approach}
   \label{fig:orchestration-diagram}
\end{figure}

\begin{figure}[]
   \centering
   \includegraphics[width=0.5\textwidth]{assets/04-serverless-for-web-apps/saga.png}
   \caption{The multistep booking workflow using the distributed saga pattern}
   \label{fig:saga-orchestration-diagram}
\end{figure}


Instead of configuring services to call each other, the orchestration approach refers to using some dedicated orchestrator component, serving a central role in the processing, coordinating the execution of components, handling interactions between them and maintaining state of the processing, along with handling retries and failures.
Workflow orchestration finds applicability in more complex operations, that require sophisticated flow control, aggregation of resources from different sources and handling the cooperation between microservices in a reliable way.
The orchestration approach is used in the second example implementation, considering the orchestration of generating interactive slideshows, covered in section \ref{chapter:examples-generating-interactive-slideshow-based-on-latex-files}.

The orchestration approach enables the developers to capture the business flow of the process, apply error handling and retry policies, along with removing the need to provide additional logic to handle it in the services.
The workflow logic is contained in one place, providing additional monitoring and visualising capabilities to reason about the status of the processing.
The downside of orchestration include additional cost related with work of the orchestrator, which is a single point of failure in the processing and makes the incorporated components more tightly coupled \cite{ChoreographyVsOrchestrationInServerlessMicroservices}.

Aforementioned example of the food ordering workflow, modeled in the orchestration approach, is presented in Figure \ref{fig:orchestration-diagram}.

Moreover, the orchestration approach using the Step Function is suitable for using the distributed saga pattern \cite{SagaPattern} to coordinate the distributed transactions across multiple microservices including independent datastores.
It can be applied to coordinate some more complex processes including multiple operations, which can fail during the processing and require orchestrating a series of compensating transactions, reverting the executed operations and cleaning up the state.
An example of such orchestration workflow is presented in the Figure \ref{fig:saga-orchestration-diagram}, covering the multistep booking process.

\subsection{Case study - Generating interactive slideshow based on LaTeX files} \label{chapter:latex-processing-optimisation}

The following section describes in more details the optimisation process behind the second example implementation of the web application using the serverless technology introduced in section \ref{chapter:examples-generating-interactive-slideshow-based-on-latex-files}.
The Step Function is used in the designed solution to coordinate the processing of several AWS Lambda instances that perform the processing of the individual stages in the entire flow.

In the presented research, the tests and measurements of the individual processing steps and their configurations are conducted based on the two example presentations prepared using the LaTeX typesetting system.
Both of the sample presentations, described in more details below, resemble the exercises which could be used in the mentioned service and characterise with different levels of complexity due to their content.

\begin{itemize}
   \item Exercise 1 --- Covers the 71 page long presentation that includes the solution derivation for the trigonometric equation.
   \item Exercise 2 --- Is more demanding and larger, 103 page long presentation. The first part incldues graphics and describes the required steps for solving a geometry problem, with additional calculations included in the second part of the exercise.
\end{itemize}

From the initial research and experiments with the possible solutions, the process of exporting presentations to the PDF format has been identified as a bottleneck in the entire processing, taking significantly longer than the other steps when performed in parallel.
The conducted performance tests consider mainly the processing duration from the client point of view, which is calculated as a time from requesting the invocation of Step Function, until the processing results are received by the client, once the slideshow generation is completed and the assets are available in the S3 bucket.
Along with measuring the processing time, several other metrics have been gathered, giving more insight into the performance of individual steps of the processing.
Measurements preserve the preconditions listed below:

\begin{itemize}
   \item Each of the tests is performed 10 times and the presented results are the average of the conducted measurements.
   \item Initially, the infrastructure has been redeployed after each execution to ensure colds starts, however thanks to the horizontal scalability of the serverless components and further observation, the same results in terms of the processing time have been obtained when the client application performed 10 requests at the same time. 
   Each of the functions executed in parallel included the information about cold start.
   Such behaviour confirms the scaling capabilities of the application based on the serverless architecture as well as resembles the situation when several users requested the presentation within short period of time, determining the worst case in terms of observable performance when the cold starts occured.
   \item The LaTeX files related to generating the presentation are removed from the temporal disc space of the function execution container once the processing is completed.
   The operation is performed to prevent a situation, when the subsequent function execution utilises the same instance of the serverless function and can benefit from the intermediate processing results from the previous invocation, which could affect the warm start execution results.
\end{itemize}

\subsubsection{Custom Lambda container image and Lambda layer}

As mentioned before, generating of the presentation based on the LaTeX file can be classified as a custom type of processing for the serverless function runtime, that goes beyond the standard set of its tasks. The section \ref{chapter:lambda-custom-runtimes} discusses in more details the possibilities of the serverless platform in terms of performing such non-standard tasks. The following section presents the results of generating the PDF file with presentation using two approaches:

\begin{itemize}
   \item Container image --- uses the official Amazon container image for Node.js runtime with additional LaTeX distribution and other libraries required for the processing.
   \item Lambda layer --- includes the LaTeX distribution extracted from the aforementioned container image into the separate archive along with the additional Lambda layer including the Perl interpreter required for the file processing.
\end{itemize}

Both of the approaches execute the same function code running in the Node.js runtime, using the provided LaTeX distribution to generate the presentation.

\pgfplotstableread{
% xaxis1, xaxis2, Layer Cold, Layer Warm, Image Cold, Image Warm
0.225 0.275 5.455 2.147 7.943 2.209
0.725 0.775 9.469 5.235 10.832 5.210
}\datasetLayerVsImage

\begin{figure}[]
    \begin{center}
    \begin{tikzpicture}
    \begin{axis}[ybar,
        width=.8\textwidth,
        ymin=0,
        ymax=12,        
        xmin=0,
        xmax=1,        
        ylabel={Response time [s]},
        bar width=4ex,
        xtick=data,
        xticklabels = {Exercise 1, Exercise 2},
        x tick label style={align=center,xshift=2ex},
        major x tick style = {opacity=0},
        minor tick length=1ex,
        legend style={at={(0.56,0.99)},anchor=north east},
        ]
            
    \addplot[draw=black,fill=blue!40,error bars/.cd,y dir=both,y explicit] 
        table[x index=0,y index=2] \datasetLayerVsImage; % Layer Cold
    \addplot[draw=black,fill=red!40,error bars/.cd,y dir=both,y explicit] 
        table[x index=0,y index=3] \datasetLayerVsImage; % Layer Warm
    \addplot[draw=black,fill=blue!70,error bars/.cd,y dir=both,y explicit] 
        table[x index=1,y index=4] \datasetLayerVsImage; % Image Cold
    \addplot[draw=black,fill=red!70,error bars/.cd,y dir=both,y explicit] 
        table[x index=1,y index=5] \datasetLayerVsImage; % Image Warm
    \legend{Lambda layer - cold start, Lambda layer - warm start, Container image - cold start, Container image - warm start}
    \end{axis}
    \end{tikzpicture}
    \caption{Response time comparison of the solutions using custom container image for AWS Lambda and Lambda layers with the LaTeX distribution}
    \label{chart:step-function-lambda-layer-vs-container-image}
    \end{center}
\end{figure}

Key takeaways of the container image and Lambda layer processing comparison:

\begin{itemize}
    \item \textbf{Cold start times} --- The significant processing time difference can be noticed when comparing the executions with cold and warm starts.
    The cold start time can be further divided into two parts. The first one considers the overhead related with the resource allocation from the large resource pool hosted by the cloud provider, downloading the function code and starting new, ephemeral container to execute the function.
    The second part is related with bootstrapping the function runtime as well as initialising dependencies required to execute the function handler.
    The detailed results of the cold start analysis are presented in the Table \ref{table:impact-of-the-cold-start-on-the-presentation-processing-task-for-approach-using-lambda-layer} and \ref{table:impact-of-the-cold-start-on-the-presentation-processing-task-for-approach-using-lambda-container-image}.

    \begin{table}[h]
        \centering
        \begin{tabular}{ |c|c|c|c|c|c| } 
        \hline
        Exercise & Execution & Function [ms] & Environment [ms] & Runtime [ms] & Execution [ms] \\
        \hline
        \multirow{2}{*}{Exercise 1} & cold start & 3609 & 914 & 273 & 2422 \\
        & warm start & 1614 & 34 & 0 & 1580 \\
        \hline
        \multirow{2}{*}{Exercise 2} & cold start & 7802 & 468 & 279 & 7055 \\
        & warm start & 4661 & 52 & 0 & 4609 \\
        \hline
        \end{tabular}
        \caption{Impact of the cold start on the presentation processing task for approach using Lambda layer}
        \label{table:impact-of-the-cold-start-on-the-presentation-processing-task-for-approach-using-lambda-layer}
    \end{table}

    \begin{table}[h]
        \centering
        \begin{tabular}{ |c|c|c|c|c|c| } 
        \hline
        Exercise & Execution & Function [ms] & Environment [ms] & Runtime [ms] & Execution [ms] \\
        \hline
        \multirow{2}{*}{Exercise 1} & cold start & 6548 & 637 & 1108 & 4803 \\
        & warm start & 1638 & 45 & 0 & 1593 \\
        \hline
        \multirow{2}{*}{Exercise 2} & cold start & 8945 & 316 & 810 & 7819 \\
        & warm start & 4614 & 35 & 0 & 4579 \\
        \hline
        \end{tabular}
        \caption{Impact of the cold start on the presentation processing task for approach using custom container image for Lambda}
        \label{table:impact-of-the-cold-start-on-the-presentation-processing-task-for-approach-using-lambda-container-image}
    \end{table}

    The runtime initialisation time of the solution using Lambda layer is comparable for both exercises and it is equal on the average to 273 ms and 279 ms for Exercise 1 and Exercise 2 accordingly. 
    For the processing using the container image solution, the runtime initialisation time is on the average equal to 1108 ms and 810 ms, which has a visible impact on the overall processing time. Moreover, the runtime initialisation time for the solution based on the container image is additionally billed accordingly to AWS Lambda pricing, on the contrary to the approach using the Lambda layer in which the cold start time is not included into the cost.
    The processing performance evaluation for the Exercise 1 preceded the Exercise 2, which could explain the decrease of the time required to download the function code and allocate resources.
    \item \textbf{Warm start times} --- When considering the prewarmed function environments, there is no visible difference in the overall processing time between both of the tested solutions.
    \item \textbf{Step Function overhead} --- The orchestration overhead computed as the overall processing time of the Step Function decreased by the time of particular tasks defined in the Step Function flow, ranges from 400 ms and 450 ms, for both cold and warm starts.    
\end{itemize}

Based on the results of the comparison the Lambda layer is selected as a more suitable solution for further workflow optimisation.

\subsubsection{Function chain length}

In the previous section, the task of generating PDF presentations from the LaTeX files is discussed, however the flow proposed in section \ref{chapter:examples-generating-interactive-slideshow-based-on-latex-files-proposed-processing-flow} describes the additional step of extracting the individual slides of the presentation to SVG files. To achieve this, the lightweight \textit{pdf2svg} \cite{pdf2svg} library is used. The following scenario measures the processing performance in two configurations:

\begin{itemize}
   \item Single function --- the conversion is performed in a single function that generates the presentation and extracts the slides, which are stored further in the S3 bucket.
   \item Function chain --- the processing is performed in the separate functions, storing the intermediate PDF file in the S3 bucket.
\end{itemize}

\pgfplotstableread{
% xaxis1, xaxis2, Single Cold, Single Warm, Chain Cold, Chain Warm
0.225 0.275 6.242 2.724 7.084 2.970
0.725 0.775 10.039 5.731 11.612 5.963
}\datasetChainVsSingle

\begin{figure}[]
    \begin{center}
    \begin{tikzpicture}
    \begin{axis}[ybar,
        width=.8\textwidth,
        ymin=0,
        ymax=12,        
        xmin=0,
        xmax=1,        
        ylabel={Response time [s]},
        bar width=4ex,
        xtick=data,
        xticklabels = {Exercise 1, Exercise 2},
        x tick label style={align=center,xshift=2ex},
        major x tick style = {opacity=0},
        minor tick length=1ex,
        legend style={at={(0.53,0.99)},anchor=north east}
        ]

    \addplot[draw=black,fill=blue!40,error bars/.cd,y dir=both,y explicit] 
        table[x index=0,y index=2] \datasetChainVsSingle; % Single Cold
    \addplot[draw=black,fill=red!40,error bars/.cd,y dir=both,y explicit] 
        table[x index=0,y index=3] \datasetChainVsSingle; % Single Warm
    \addplot[draw=black,fill=blue!70,error bars/.cd,y dir=both,y explicit] 
        table[x index=1,y index=4] \datasetChainVsSingle; % Chain Cold
    \addplot[draw=black,fill=red!70,error bars/.cd,y dir=both,y explicit] 
        table[x index=1,y index=5] \datasetChainVsSingle; % Chain Warm
    \legend{Single function - cold start, Single function - warm start, Function chain - cold start, Function chain - warm start}
    \end{axis}
    \end{tikzpicture}
    \caption{Response time comparison of the solutions using single function and function chain to process the presentation}
    \label{chart:step-function-single-function-vs-function-chain}
    \end{center}
\end{figure}

Key takeaways of the comparison:

\begin{itemize}
   \item \textbf{Keep the function chain short} --- When chaining several functions to perform some workflow, the overhead related to the communication and the cold start compounds. Additionally, the intermediate results need to be stored in some external component, which introduces additional work and latency that can be noticeable, especially when working with large volumes of the data.
   \item \textbf{Lambda layers constraints} --- The small size of the \textit{pdf2svg} library makes it suitable for adding it and its dependencies to the Lambda layer, that includes the LaTeX distribution already. However, it is crucial to take into account the limitations of the approach using the Lambda layers, described in more detail in section \ref{chapter:lambda-custom-runtimes}, because at some point it may be not possible to extend the function further.
\end{itemize}

\subsubsection{Parallel processing} \label{section:case-study-parallel-processing}

The serverless architecture is characterized by the capabilities of instant autoscaling to meet the demand of the processing.
Some types of the computation can be re-designed to utilise such feature of the architecture and improve the overall processing time by parallelising the workload.
Generating the presentation from the LaTeX files to the PDF format is a custom type of processing task, that under some circumstances can be parallelised to a certain degree as well.
The predefined range of slides from the presentation prepared using the Beamer package \cite{beamer}, can be selected using the \textit{\textbackslash includeonlyframes} directive.
However, the LaTeX files processing requires to repeat some initial and common part of the processing regardless of the selected subset of slides, which causes some part of the processing to be repeated for each of the presentation chunk.

The taken approach splits the LaTeX processing into several serverless function executions running in parallel. Each of the functions is assigned to generate the slides for predefined presentation chunks, containing 5 consecutive slides. Once the processing of all serverless functions is completed, along with the commentary transcription and metadata extraction, the user is notified about the results of the processing. The Step Function processing graph is presented in Figure \ref{fig:step-function-processing-the-presentation-in-parallel}, indicating that the presentation generation is divided into chunks, that are mapped to independent function calls and executed in parallel.

\begin{figure}[]
    \centering
    \includegraphics[width=0.6\textwidth]{assets/04-serverless-for-web-apps/stepFunctionGraphParallel.png}
    \caption{Step Function processing graph --- Generating the presentation chunks in parallel}
    \label{fig:step-function-processing-the-presentation-in-parallel}
\end{figure}

\pgfplotstableread{
% xaxis1, xaxis2, Single Cold, Single Warm, Parallel Cold, Parallel Warm
0.225 0.275 6.242 2.724 4.996 2.136
0.725 0.775 10.039 5.731 7.477 3.186
}\datasetSingleVsParallel

\begin{figure}[]
    \begin{center}
    \begin{tikzpicture}
    \begin{axis}[ybar,
        width=.9\textwidth,
        ymin=0,
        ymax=12,        
        xmin=0,
        xmax=1,        
        ylabel={Response time [s]},
        bar width=4ex,
        xtick=data,
        xticklabels = {Exercise 1, Exercise 2},
        x tick label style={align=center,xshift=2ex},
        major x tick style = {opacity=0},
        minor tick length=1ex,
        legend style={at={(0.5175,0.99)},anchor=north east}
        ]

    \addplot[draw=black,fill=blue!40,error bars/.cd,y dir=both,y explicit] 
        table[x index=0,y index=2] \datasetSingleVsParallel; % Single Cold
    \addplot[draw=black,fill=red!40,error bars/.cd,y dir=both,y explicit] 
        table[x index=0,y index=3] \datasetSingleVsParallel; % Single Warm
    \addplot[draw=black,fill=blue!70,error bars/.cd,y dir=both,y explicit] 
        table[x index=1,y index=4] \datasetSingleVsParallel; % Parallel Cold
    \addplot[draw=black,fill=red!70,error bars/.cd,y dir=both,y explicit] 
        table[x index=1,y index=5] \datasetSingleVsParallel; % Parallel Warm
    \legend{Single function - cold start, Single function - warm start, Parallel processing - cold start, Parallel processing - warm start}
    \end{axis}
    \end{tikzpicture}
    \caption{Response time comparison of the solutions using single function to generate the complete presentation and the parallelised processing of separate chunks}
    \label{chart:step-function-single-function-vs-parallel-processing}
    \end{center}
\end{figure}

Key takeaways of the parallel processing compared to the single function execution:

\begin{itemize}
    \item \textbf{Parallelising the LaTeX files processing} --- Even though the presentation generation processing cannot be effectively parallelised, the overall processing time is reduced for both of the provided examples.
    The complexity of the selected examples has a significant impact on the total processing time.
    When measuring and comparing the processing time on the local machine, generating the chunks independently for the Exercise 1 reduced the overall processing time by 32\%, while for the Exercise 2 it reached approximately 74\%, that confirms the previous statement.
  
    The average execution time for the presentation generation task of the chunks processed in separate functions compared to processing of the complete presentation in a single function is presented in Table \ref{table:overhead-of-the-presentation-processing-task-for-the-whole-presentation-and-chunks-including-5-slides}

    \begin{table}[h]
        \centering
        \begin{tabular}{ |c|c|c|c|c|c| } 
        \hline
        Exercise & Execution & Complete [ms] & Avg. of chunks [ms] & Time reduction [\%] \\
        \hline
        \multirow{2}{*}{Exercise 1} & cold start & 3349 & 2748 & 17.95 \\
        & warm start & 1641 & 1100 & 32.97 \\
        \hline
        \multirow{2}{*}{Exercise 2} & cold start & 7887 & 4310 & 45.35 \\
        & warm start & 4679 & 1313 & 71.94 \\
        \hline
        \end{tabular}
        \caption{Time reduction of the presentation processing task for the complete presentation and chunks including 5 slides}
        \label{table:overhead-of-the-presentation-processing-task-for-the-whole-presentation-and-chunks-including-5-slides}
    \end{table}

   \item \textbf{Orchestrating parallel processing brings additional overhead} --- It is essential to consider the additional overhead related to the orchestration of several serverless functions processing the presentation chunks in parallel. In the following example, the orchestration overhead is calculated as the Step Function duration decreased by the average tasks duration. It is equal to 873 ms (cold start) and 890 ms (warm start) for Exercise 1 as well as 1765 ms (cold start) and 1745 ms (warm start) for Exercise 2.
   \item \textbf{Cost of the parallel processing} --- Besides the processing overhead, the parallel execution brings additional cost for each of the executed serverless functions and its execution time. Moreover, for the given example the processing cannot be effectively parallelised and some part of the processing and retrieving the file is repeated for each invoked function, impacting the overall cost of the computation.
\end{itemize}

\subsubsection{Updating the client with processed batches} \label{section:case-study-updating-the-client-with-processed-batch}

The approach presented in the previous section enables to effectively parallelise the processing and reduce the overall processing time, but it is restricted by the fact that all of the presentation chunks needs to be processed to push the update to the client. When taking into consideration how the presentations are prepared, most frequently the first several slides include the introduction to the exercise that usually takes more than 10 seconds. From the user perspective, only the first batch is required to start presenting the slideshow, while the remaining part of the assets can be delivered later.

The further improvements of the processing flow takes into account the aforementioned assumption and parallelises the presentation processing on the higher level, to deliver the updates of the processed presentation chunks independently. The Step Function processing graph of suggested approach is presented in Figure \ref{fig:step-function-pushing-updates-to-the-client-with-partially-processed-presentation}. The further measurements reflects the response time for two configurations of the flow, described below:

\begin{itemize}
   \item First batch with 5 initial slides processed and the second batch with the remaining part of the presentation --- the processing is effectively parallelised in two branches.
   \item Each of the batches include information about 5 consecutive slides --- the processing is parallelised depending on the size of the presentation, with each of the branches processing 5 consecutive slides.
\end{itemize}

The results of the processing are compared with processing time of the complete presentation by a single serverless function and when the processing is parallelised, as shown in the previous section. The results are presented in the figures \ref{chart:step-function-initial-batch} and \ref{chart:step-function-batches}.

\begin{figure}[]
    \centering
    \includegraphics[width=0.6\textwidth]{assets/04-serverless-for-web-apps/stepFunctionGraphInitialBatch.png}
    \caption{Step Function processing graph --- Delivering updates to the client with independently processed presentation chunks}
    \label{fig:step-function-pushing-updates-to-the-client-with-partially-processed-presentation}
\end{figure}

\pgfplotstableread{
% xaxis, Single Cold, Single Warm, Parallel Cold, Parallel Warm, FirstBatch Cold, RestBatch Cold, FirstBatch Warm, RestBatch Warm
0.25 6.242 2.724 4.996 2.136 4.315 4.680 1.710 2.718
0.75 10.039 5.731 7.477 3.186 5.611 9.755 1.803 5.814
}\datasetUploadInitialBatch

\begin{figure}[]
    \begin{center}
    \begin{tikzpicture}
    \begin{axis}[ybar,
        width=.9\textwidth,
        ymin=0,
        ymax=12,        
        xmin=0,
        xmax=1,
        bar width=3ex,  
        ylabel={Response time [s]},
        xtick=data,
        xticklabels = {Exercise 1,Exercise 2},
        major x tick style = {opacity=0},
        minor x tick num = 1,
        minor tick length=1ex,
        legend style={at={(0.5075,0.99)},anchor=north east}
        ]
    \addplot[draw=black,fill=blue!40,error bars/.cd,y dir=both,y explicit] 
        table[x index=0,y index=1] \datasetUploadInitialBatch; % Single Cold
    \addplot[draw=black,fill=red!40,error bars/.cd,y dir=both,y explicit] 
        table[x index=0,y index=2] \datasetUploadInitialBatch; % Single Warm
    \addplot[draw=black,fill=blue!70,error bars/.cd,y dir=both,y explicit] 
        table[x index=0,y index=3] \datasetUploadInitialBatch; % Parallel Cold
    \addplot[draw=black,fill=red!70,error bars/.cd,y dir=both,y explicit] 
        table[x index=0,y index=4] \datasetUploadInitialBatch; % Parallel Warm
    \addplot[draw=black,fill=blueDark!50,error bars/.cd,y dir=both,y explicit] 
        table[x index=0,y index=5] \datasetUploadInitialBatch; % FirstBatch Cold
    \addplot[draw=black,fill=blueDark!80,error bars/.cd,y dir=both,y explicit] 
        table[x index=0,y index=6] \datasetUploadInitialBatch; % RestBatch Cold
    \addplot[draw=black,fill=redDark!50,error bars/.cd,y dir=both,y explicit] 
        table[x index=0,y index=7] \datasetUploadInitialBatch; % FirstBatch Warm
    \addplot[draw=black,fill=redDark!80,error bars/.cd,y dir=both,y explicit] 
        table[x index=0,y index=8] \datasetUploadInitialBatch; % RestBatch Warm
    \legend{Single function - cold start, Single function - warm start, Parallel execution - cold start, Parallel execution - warm start, First batch - cold start, Second batch - cold start, First batch - warm start, Second batch - warm start}
    \end{axis}
    \end{tikzpicture}
    \caption{Response time comparison of the solution delivering the update to the client in the first batch including initial 5 slides and the second batch with remaining data}
    \label{chart:step-function-initial-batch}
    \end{center}
\end{figure}

\pgfplotstableread{
% xaxis, Single Cold, Single Warm, Parallel Cold, Parallel Warm, FirstBatch Cold, RestBatch Cold, FirstBatch Warm, RestBatch Warm
0.25 6.242 2.724 4.996 2.136 12.615 12.867 11.792 12.366
0.75 10.039 5.731 7.477 3.186 22.563 22.948 22.209 22.374
}\datasetUploadBatches

\begin{figure}[]
    \begin{center}
    \begin{tikzpicture}
    \begin{axis}[ybar,
        width=.9\textwidth,
        ymin=0,
        ymax=24,        
        xmin=0,
        xmax=1,
        bar width=3ex,  
        ylabel={Response time [s]},
        xtick=data,
        xticklabels = {Exercise 1,Exercise 2},
        major x tick style = {opacity=0},
        minor x tick num = 1,
        minor tick length=1ex,
        legend style={at={(0.5075,0.99)},anchor=north east}
        ]
    \addplot[draw=black,fill=blue!40,error bars/.cd,y dir=both,y explicit] 
        table[x index=0,y index=1] \datasetUploadBatches; % Single Cold
    \addplot[draw=black,fill=red!40,error bars/.cd,y dir=both,y explicit] 
        table[x index=0,y index=2] \datasetUploadBatches; % Single Warm
    \addplot[draw=black,fill=blue!70,error bars/.cd,y dir=both,y explicit] 
        table[x index=0,y index=3] \datasetUploadBatches; % Parallel Cold
    \addplot[draw=black,fill=red!70,error bars/.cd,y dir=both,y explicit] 
        table[x index=0,y index=4] \datasetUploadBatches; % Parallel Warm
    \addplot[draw=black,fill=blueDark!50,error bars/.cd,y dir=both,y explicit] 
        table[x index=0,y index=5] \datasetUploadBatches; % FirstBatch Cold
    \addplot[draw=black,fill=blueDark!80,error bars/.cd,y dir=both,y explicit] 
        table[x index=0,y index=6] \datasetUploadBatches; % RestBatch Cold
    \addplot[draw=black,fill=redDark!50,error bars/.cd,y dir=both,y explicit] 
        table[x index=0,y index=7] \datasetUploadBatches; % FirstBatch Warm
    \addplot[draw=black,fill=redDark!80,error bars/.cd,y dir=both,y explicit] 
        table[x index=0,y index=8] \datasetUploadBatches; % RestBatch Warm
    \legend{Single function - cold start, Single function - warm start, Parallel execution - cold start, Parallel execution - warm start, First batch - cold start, All batches - cold start, First batch - warm start, All batches - warm start}
    \end{axis}
    \end{tikzpicture}
    \caption{Response time comparison of the solution delivering the update to the client in the separate batches}
    \label{chart:step-function-batches}
    \end{center}
\end{figure}

Key takeaways of the proposed approach:

\begin{itemize}
   \item \textbf{Improved response time of the initial batch} --- For the first of the considered configurations, with the results presented in Figure \ref{chart:step-function-initial-batch}, the response time for the first batch including the first slides is shorter.
   Once it is received by the client, the user can start previewing the slideshow, while the second batch including the remaining data can be processed and received by the client later.
   Such a solution enables the user to start previewing the exercise quicker, while the processing time of the complete presentation is hidden from the user perspective.
   \item \textbf{Increase of the Step Function overhead for the fine-grained workflow coordination} --- In the second configuration that splits the presentation chunks equally and parallelises the processing, the overhead of the Step Function responsible for the orchestration makes the processing significantly longer.
   Moreover, the batch including 5 initial slides is frequently delivered as one of the latests, significantly postponing the time when the user can start previewing the exercise, as it is presented in the Figure \ref{chart:step-function-batches}.
\end{itemize}

\subsubsection{Comparing serverless solution with the traditional architecture}

When deciding on the serverless architecture, there is always a question how the solution will perform compared to the solution implemented using the more traditional architectures.
To answer that question for the given problem, a similar service based on a simple server running within the container has been created and hosted on the AWS Fargate. 
The server is executed in the Node.js runtime, similarly to the serverless functions from the previous steps of the study.
Moreover, it is reusing a significant part of the logic responsible for the initial validation of the task, processing the LaTeX presentation to PDF and then extracting separate slides to the SVG files, using the same libraries as well as uploading the results to S3 once the processing is completed.
The container is configured with the same amount of memory and CPU as the serverless function used for the processing in the serverless implementation.

The figure \ref{chart:step-function-compared-with-traditional-architecture} presents how the solution using the traditional architecture performs compared with the various configurations of the serverless solution.

\pgfplotstableread{
% xaxis, Single Cold, Single Warm, Parallel Cold, Parallel Warm, FirstBatch Cold, RestBatch Cold, FirstBatch Warm, RestBatch Warm
0.25 6.242 2.724 4.996 2.136 4.315 4.680 1.710 2.718 2.636
0.75 10.039 5.731 7.477 3.186 5.611 9.755 1.803 5.814 6
}\datasetCompareContainer

\begin{figure}[]
    \begin{center}
    \begin{tikzpicture}
    \begin{axis}[ybar,
        width=.9\textwidth,
        ymin=0,
        ymax=12,        
        xmin=0,
        xmax=1,
        bar width=3ex,  
        ylabel={Response time [s]},
        xtick=data,
        xticklabels = {Exercise 1,Exercise 2},
        major x tick style = {opacity=0},
        minor x tick num = 1,
        minor tick length=1ex,
        legend style={at={(0.5075,0.99)},anchor=north east}
        ]
    \addplot[draw=black,fill=blue!40,error bars/.cd,y dir=both,y explicit] 
        table[x index=0,y index=1] \datasetCompareContainer; % Single Cold
    \addplot[draw=black,fill=red!40,error bars/.cd,y dir=both,y explicit] 
        table[x index=0,y index=2] \datasetCompareContainer; % Single Warm
    \addplot[draw=black,fill=blue!70,error bars/.cd,y dir=both,y explicit] 
        table[x index=0,y index=3] \datasetCompareContainer; % Parallel Cold
    \addplot[draw=black,fill=red!70,error bars/.cd,y dir=both,y explicit] 
        table[x index=0,y index=4] \datasetCompareContainer; % Parallel Warm
    \addplot[draw=black,fill=blueDark!50,error bars/.cd,y dir=both,y explicit] 
        table[x index=0,y index=5] \datasetCompareContainer; % FirstBatch Cold
    \addplot[draw=black,fill=blueDark!80,error bars/.cd,y dir=both,y explicit] 
        table[x index=0,y index=6] \datasetCompareContainer; % RestBatch Cold
    \addplot[draw=black,fill=redDark!50,error bars/.cd,y dir=both,y explicit] 
        table[x index=0,y index=7] \datasetCompareContainer; % FirstBatch Warm
    \addplot[draw=black,fill=redDark!80,error bars/.cd,y dir=both,y explicit] 
        table[x index=0,y index=8] \datasetCompareContainer; % RestBatch Warm
    \addplot[draw=black,fill=green!60,error bars/.cd,y dir=both,y explicit] 
        table[x index=0,y index=9] \datasetCompareContainer; % RestBatch Warm
    \legend{Single function - cold start, Single function - warm start, Parallel execution - cold start, Parallel execution - warm start, First batch - cold start, Second batch - cold start, First batch - warm start, Second batch - warm start, Container environment}
    \end{axis}
    \end{tikzpicture}
    \caption{Response time comparison of the solutions using the serverless and traditional architectures}
    \label{chart:step-function-compared-with-traditional-architecture}
    \end{center}
\end{figure}

Key takeaways of the serverless implementation comparison with the solution using the simple, containerized server application:

\begin{itemize}
    \item \textbf{The performance of serverless implementation is comparable, when it is not affected by the cold starts} --- The initial serverless solution, orchestrating the flow and using single function to process the LaTeX file, performs similarly when executed with the warm start, compared to the solution using the simple server hosted by AWS Fargate in the container. However, when the execution is affected by the cold starts, the response time is significantly longer as presented in Figure \ref{chart:step-function-compared-with-traditional-architecture}.
    It is beneficial to consider the interest in the service as well as the traffic pattern to estimate how often such a phenomenon may take place, when deciding on architecture selection or mitigating the negative influence by pre-warming the containers programmatically or by the services, such as Provisioned Concurrency for AWS Lambda.
    \item \textbf{Re-designing the serverless processing to utilise its benefits can improve performance} --- The presented configurations of the serverless solution confirms that remodeling the processing to use the benefits of the serverless paradigm can bring visible benefits in terms of the performance, compared to the solution using the more traditional architecture.
    However, some aspects and tasks of the proposed flow are required for the serverless solution, such as uploading the results of processing to S3, which could be further accessed by the user.
    The solution using the server could aggregate the results of processing locally, without the need to communicate with external component to store the files and return the results directly to the user, reducing the amount of work and cost.
    \item \textbf{Built-in scalability of the serverless architecture} --- One of the features of the serverless architecture is the possibility of almost instant and effortless processing parallelisation, that reduces the overall processing time in that case.
    As mentioned in the introduction to the section, the performance tests of the executions characterised by the cold starts have been measured by sending 10 requests simultaneously, resulting with similar performance as the single executions interleaved with redeploying the infrastructure. 
    This approach allows to infer about the behaviour of the service, when several users will use the service simultaneously, while showing the worst case in terms of the service performance.
    Further research could investigate in more details, how the implementations using both of the architectures are performing under the load, resembling the behaviour of the potential users of the system.
    When considering the serverless solution, the impact could be visible by increased number of cold starts, when the traffic pattern is changing or it is occasional. However, the solution using the simple server hosted in the container could note the performance decline similarly, when many subsequent requests would be submitted to a single worker instance or when the cluster of workers would need to scale to meet the demand.
\end{itemize}

\subsubsection{Cost of the designed solution}

Another, frequently mentioned factor when deciding on the architecture choice is the maintenance and execution cost. The serverless architecture makes it more difficult to precisely measure the cost of the processing, due to the number of components and the variety of granularity in pricing model for used services. However, the cost can be quite accurately estimated, when the traffic pattern is known along with the execution cost of particular workflow.

The section calculates the cost of a single execution of the entire processing flow for Exercise 1, considering two configurations described previously:

\begin{itemize}
   \item The parallel presentation processing describes in section \ref{section:case-study-parallel-processing}.
   \item The first configuration described in section \ref{section:case-study-updating-the-client-with-processed-batch}, in which the presentation processing is divided into two branches --- first with initial slides and the second with remaining data for the slideshow completeness.
\end{itemize}

The cost of services used in the implementation is summarised in Table \ref{table:case-study-service-spendings-summary}, based on the pricing defined for region Europe (Frankfurt) and excluding the Free Tier provided by the AWS.
Cost of some of the services, such as Amazon CloudWatch for storing logs, API Gateway for the incoming traffic in form of the user requests or data transfer for DynamoDB is omitted due to negligible use and very small granularity to notice the difference in teh values of cost statement.


\begin{table}[]
    \centering
    \scalebox{0.9}{
    \begin{tabular}{ |c|c|c| } 
    \hline
    Service & Billed service usage & Pricing \\
    \hline
    \multirow{3}{*}{API Gateway} & REST API - API Calls & \$3.70 (per million) \\ 
    & WebSocket API - Connection Minutes & \$0.285 per million connection minutes \\ 
    & WebSocket API - Message Transfers & \$1.14 (per million) \\ 
    \hline
    Data Transfer & From Amazon S3 To Internet & \$0.09 per GB \\
    \hline
    \multirow{2}{*}{DynamoDB} & Read request & \$0.305 per million read request units \\
    & Write request & \$1.525 per million write request units \\
    \hline
    \multirow{2}{*}{AWS Lambda} & Duration & \$0.0000166667 for every GB-second \\
    & Requests & \$0.20 per 1M requests \\
    \hline
    Polly & Standard voices & \$4.00 per 1 million characters \\
    \hline
    \multirow{2}{*}{Simple Storage Service} & PUT, COPY, POST, LIST requests & \$0.0054 (per 1,000 requests) \\
    & GET, SELECT, and all other request & \$0.00043 (per 1,000 requests) \\
    \hline
    Step Function & State transtition & \$0.025 per 1,000 state transitions\\
    \hline
    \end{tabular}
    }
    \caption{Pricing summary for services used in the implementations}
    \label{table:case-study-service-spendings-summary}
\end{table}


The usage of the services for both implementations and their cost is summarised in Table \ref{table:case-study-service-parallel-spending-summary} and \ref{table:case-study-service-initial-batch-spending-summary}.

\begin{table}[]
    \centering
    \begin{tabular}{ |c|c|c| } 
    \hline
    Service usage & Usage & Cost [\$] \\
    \hline
    REST API - API Calls & 2 requests & 0.0000074 \\
    WebSocket API - Connection Minutes & 2 minutes & 0.00000057 \\
    WebSocket API - Message Transfers & 4 messages & 0.00000456 \\
    \hline
    Data Transfer From Amazon S3 To Internet & 0.003 GB & 0.00027 \\
    \hline
    DynamoDB Read request & 1 Read Request Unit & 0.0000000305 \\
    DynamoDB Write request & 2 Write Request Unit & 0.0000000305 \\
    \hline
    AWS Lambda - Duration & 61.057 GB-second & 0.00102 \\
    AWS Lambda - Requests & 22 requests & 0.0000044 \\
    \hline
    Amazon Polly - Standard voices & 1323 characters & 0.00529 \\
    \hline
    S3 PUT, COPY, POST, LIST requests & 86 requests & 0.000464 \\
    S3 GET, SELECT, and all other request & 108 requests & 0.0000464 \\
    \hline
    Step Function - state transitions & 24 transitions & 0.0006 \\
    \hline
    \multicolumn{1}{c|}{} & \textbf{Total} & 0.00771 \\
    \cline{2-3}
    \end{tabular}
    \caption{Cost summary for the approach using parallel presentation processing}
    \label{table:case-study-service-parallel-spending-summary}
\end{table}

\begin{table}[]
    \centering
    \begin{tabular}{ |c|c|c| } 
    \hline
    Service usage & Usage & Cost [\$] \\
    \hline
    REST API - API Calls & 2 requests & 0.0000074 \\
    WebSocket API - Connection Minutes & 4 minutes & 0.00000114 \\
    WebSocket API - Message Transfers & 5 messages & 0.0000057 \\
    \hline
    Data Transfer From Amazon S3 To Internet & 0.003 GB & 0.00027 \\
    \hline
    DynamoDB Read request & 1,5 Read Request Unit & 0.0000000456 \\
    DynamoDB Write request & 2 Write Request Unit & 0.0000000305 \\
    \hline
    AWS Lambda - Duration & 11.649 GB-second & 0.000194 \\
    AWS Lambda - Requests & 12 requests & 0.0000024 \\
    \hline
    Amazon Polly - Standard voices & 1323 characters & 0.00529 \\
    \hline
    S3 PUT, COPY, POST, LIST requests & 86 requests & 0.000464 \\
    S3 GET, SELECT, and all other request & 98 requests & 0.0000421 \\
    \hline
    Step Function - state transitions & 17 transitions & 0.000425 \\
    \hline
    \multicolumn{1}{c|}{} & \textbf{Total} & 0.00671 \\
    \cline{2-3}
    \end{tabular}
    \caption{Cost summary for the approach delivering the updates to the client in separate batches}
    \label{table:case-study-service-initial-batch-spending-summary}
\end{table}

Takeaways of the cost summary of designed solutions:

\begin{itemize}
   \item \textbf{AWS Lambda processing is a small fraction of overall cost} --- Cost summaries for both of the presented solutions indicate that the text-to-voice transcoding service takes a significant amount of the overall cost.
   The usage of the AWS Lambda processing is greater for the solution using parallel processing.
   Even though some part of the presentation processing is repeated in the serverless functions executed in parallel, the overall cost of the AWS Lambda related services barely exceeds 0.001\$ for the analysed example per single workflow execution.
   Moreover, it is crucial to consider the cost of other services incorporated in the processing as well as the data transfer.
   For example, the summary of the second configuration indicates that the cost of uploading results to Amazon S3 and transferring them to the client exceeded the cost of the processing of the AWS Lambda.
   \item \textbf{Serverless pay-per-use billing model} --- Considering the Exercise 2, which is a more demanding task in terms of the processing and the summaries listed above, the overall cost of the processing for a single presentation could be estimated as not exceeding 0.01\$ per execution.
   Moreover, when taking into account the benefits of the serverless architecture, such as autoscaling with granular billing model based on usage and without paying for idle as well as the fact that the infrastructure management is handled by the cloud provider, the serverless solution with pay-per-use pricing can be an appealing alternative to more traditional architectures.
\end{itemize}

\section{Data Tier} \label{chapter:serverless-datastores}

The perspective of data tier in the web application field is discussed in more detail in section \ref{chapter:database-paradigms}.
Despite the fact that the relational database management systems established their position over the years, the scale of modern web application showed that the relational model reaches some limitation in terms of scaling and maintaining at the same time satisfactory level of availability.
NoSQL databases emerged as a response to the need of the Internet scale applications, handling scaling and the increased data volumes more efficiently as well as offering better performance and availability, while sacrificing other capabilities such as operations consistency and query flexibility.

The developer's task is to ensure that the architected web application will meet defined requirements and scale effectively.
Selecting the appropriate database is one of the design choices for newly developed applications.
With the growth of popularity of microservice architecture patterns, presented in section \ref{section:web-apps-microservices}, when the web applications are broken down into smaller services, each of them should manage its own data.
It allows developers to make a decision about choosing the suitable database for the service independently from the rest of the system, providing a favourable data model, which fulfills the assumed requirements \cite{AOneSizeFitsAllDatabaseDoesntFitAnyone}.
The serverless microservices can leverage such approach as well, drawing on the rich offer of the cloud providers to select the database which will fulfill the requirements as well as ensure high-performance and scalability, while at the same time handing off the need to manage the datastore to the cloud provider.

The offerings of cloud providers are presented in section \ref{chapter:serverless-service-providers}, including the database components suitable for the serverless workloads.
Nevertheless, the portfolio of the cloud providers is much broader, covering numerous database categories and their diverse data models, which are suitable for various use cases in the web application field.
The extended list of the available database components provided by AWS is presented according to the current offering below \cite{DatabasesOnAWS}.

\begin{itemize}
   \item Amazon Relational Database Service (RDS) --- relational database, with several database instance types optimized for memory, performance or I/O operations, supporting six familiar database engines, including Amazon Aurora, PostgreSQL, MySQL, MariaDB, Oracle Database and SQL Server
   \item Amazon Aurora --- relational database, compatible with MySQL and PostgreSQL, fully managed by Amazon RDS, including on-demand and autoscaling configuration suitable for serverless workloads
   \item Amazon Redshift --- relational database, designed for data warehousing
   \item Amazon DynamoDB --- key-value database, supporting wide column structure for attributes, which can be simple types as well as more complex documents, heavily used for the serverless workloads with autoscaling and granular billing model, based on number of operations and data transfer
   \item Amazon ElastiCache --- in-memory, key-value datastore with sub-millisecond latency, compatible with Redis and Memcached
   \item Amazon DocumentDB --- MongoDB-compatible, document database service
   \item Amazon Keyspaces --- wide-column, managed Cassandra-compatible database
   \item Amazon Neptune --- graph database
   \item Amazon Teamstream --- time series database
   \item Amazon QLDB --- managed ledger database, providing immutable and cryptographically verifiable transaction log
   \item Amazon Elasticsearch Service - search engine based on Elasticsearch service
\end{itemize}

Most of the mentioned components can be also incorporated in the web application developed in the serverless paradigm, providing desired capabilities and fulfilling the requirements of various services.
For example the web application serving a role of bookstore can use DynamoDB to store information about books, using simple lookups for known keys identifying the books.
When the data about available products is updated, the DynamoDB Streams can replicate the data to Amazon Elasticsearch Service, providing the fulltext search capabilities.
The leaderboard, including most frequently purchased items, can be implemented using Redis, hosted on Amazon ElastiCache, and its sorted set data structure.
Lastly, the graph database in the form of Amazon Neptune can be utilised to provide a recommendation engine, when traversing the graph of connections for the purchased books \cite{DatabasesOnAWSTheRightToolForTheRightJob}.

However, the serverless paradigm heavily relies on the ephemeral compute in the form of Function as a Service, which leads to some complications when used with the long-lived, TCP-based connection model of traditional databases.
Moreover, most of the databases do not fulfill the serverless assumption about autoscaling capabilities to meet the demand as well as granular per-per-use billing model, requiring instead to scale and configure databases on a per instance basis, billed accordingly.

\subsection{Issues when integrating traditional databases with serverless solutions}

Most of the traditional relational database management systems are not designed to work effectively with the ephemeral compute represented by Function as a Service.
Such databases are more suitable to work with long-running compute instances, initialising the TCP-based connection pool and reusing the connections across the operations performed by the application.
The ephemeral nature of serverless functions along its scaling capabilities, makes it not applicable to maintain the connection pool on the application level, because each of the functions is logically and physically separated from others.
It leads to establishing a large number of active connections, because each executed function needs to connect to the database separately, bringing additional overhead when initially establishing the connection.
Once the function execution is completed, the connection is not closed to make it available when the function container will be reused, but depending on load it may not be utilised \cite{WhyThePIETheoremIsMoreRelevantThanTheCAPTheorem}.

There are several approaches to alleviate the consequences of such unsuitability \cite{BuildingResilientServerlessSystemsWithNonServerlessComponents}, which are listed and discussed briefly below:

\begin{itemize}
   \item \textbf{Adjust database connection configuration} ---
   Some of the database configuration parameters can be adjusted to affect the communication between serverless function and the database.
   The maximum number of available connections can be increased, however it is not a recommended solution, which can lead to dangerous spikes in database memory and CPU utilisation.
   Moreover, lowering the connection timeout can be used to drop the established connections more eagerly and make them available sooner.
   \item \textbf{Adjust function configuration} ---
   The Lambda function concurrency can be limited, leading to reduction in number of requests to the downstream services, however when the lambda is configured to be executed in a synchronous manner, it will throttle the execution and return error to the upstream clients, which may be not convenient for them.
   The Lambda function code can handle establishing the connection and closing it after the execution. Nevertheless, such a solution has some downsides, including additional overhead related with establishing and closing the connection for the function as well as the database.
   In that case, when leveraging the warm starts, the database connection would not be reused.
   \item \textbf{Implementing good caching strategies} ---
   The database service should be designed to scale by itself, however some solution could be to utilise caching services as write-through cache to mitigate the connection issues, such as Amazon ElastiCache.
   When the serverless function is executed, it could make a request to the cache to obtain the data and in case of cache miss, it could establish the connection to the database.
   When the data from the database would be retrieved, it could be stored in the cache for further function executions.
   It is essential to ensure proper cache invalidation and use Time To Live (TTL) parameter to prevent the data from being stale.
   \item \textbf{Buffering events for throttling requests and durability} ---
   Heavy write workloads that do not need immediate feedback, can be effectively redesigned to process the requests in an asynchronous manner.
   The SQS queue can be used to buffer the events along with the lambda function configured with limited concurrency, consuming the events in a steady manner not to overwhelm the throughput of the downstream database.
   Similar approach is described in more detail in section \ref{chapter:interal-services}.
   Moreover, the solution adds another level of durability and reliability when the database would have troubles processing the operations.
   The queue can buffer the events or return them to the associated DLQ when the processing fails.
   \item \textbf{Utilising proxy services} ---
   AWS provides Amazon RDS Proxy, which is a managed database proxy for database instances running on Amazon RDS.
   Its responsibility is to manage the warm connection pool on behalf of the application and sharing the connections between the clients, alleviating the problem of the increase of opened database connections.
   RDS Proxy can handle the connectivity details of the downstream database, simplifying the logic related with connectivity management in Lambda functions, while multiplexing the function requests within the shared pool of active connections \cite{UsingRelationalDatabasesWithAWSLambdaEasyConnectionPooling}.
   \item \textbf{Managing connections programmatically} ---
   Managing the database connection programmatically is not a trivial task, however the community provides libraries, helping with handling and managing the database connections in the serverless environment.
   The libraries utilise the database metadata to count the number of open connections, close them if their number exceeds predefined threshold by selecting connections with highest duration as well as provide the improved mechanism to establish the connection with the database, retrying it with exponential backoff in case of errors.
\end{itemize}

The connection limit is not the only problem with traditional database management systems when integrating them with serverless technologies.
Preferably the databases are protected by strict firewall rules, restricting the clients which can access them, which is also problematic for the ephemeral compute services, bringing additional latency when initialising the communication.

The instance-based database solutions are much harder to scale, especially when considering the autoscaling nature of the serverless architecture.
Most of the time, the instances are not able to scale up and down according to the demand and require to be prepared for the peak traffic, which leads to overprovisioning.
Such an approach is not cost efficient, compared to characteristics of the serverless components.

Lastly, the provisioning and configuring the more traditional databases requires additional configuration on the database level, including for example additional users and roles definitions.
With the serverless paradigm utilising heavily the Infrastructure as a Code approach for defining and configuring the resources, such additional database setup is harder to coordinate properly and leads to spreading the configuration across multiple tools \cite{ServerlessAuroraWhatItMeansAndWhyItsTheFutureOfData}.

\subsection{Serverless databases}

Thanks to technology evolution, including faster network and better serialization protocols, the distributed, API-driven architectures using managed services can work more effectively.
It contributes to building services, serving a role of interceptors between the databases and its clients.
Some databases provide such interfaces, which makes them more suitable for the serverless workloads.
The idea of serverless databases describes the datastores, which can be integrated effectively with other serverless components as well as provide additional desired capabilities, which are fitting the serverless paradigm \cite{ServerlessAuroraWhatItMeansAndWhyItsTheFutureOfData}.

The list of features of the serverless databases is listed below:

\begin{itemize}
   \item \textbf{HTTP-based communication} ---
   Most frequently the serverless databases introduce other components, serving a role of proxy between the database and providing the communication interface for the clients.
   It alleviates the issues related with networking and managing connections to the database as well as integrates well with other services of the serverless platform \cite{PickingDatabaseForYourServerlessApplicationIn2021}.
   Moreover, some of the serverless databases such as Firebase, mentioned in section \ref{chapter:serverless-reduced-development-and-operational-cost}, allow web and mobile clients to communicate directly with the database and access or modify the data as well as to receive updates in real time, based on the data changes.
  
   \item \textbf{Autoscaling with no infrastructure management} ---
   With the growth of the data volumes or the requirements for processing operations, the serverless database should scale automatically to meet the demand, similarly to other serverless components.
   It also frees from estimating the proper instance size along with overprovisioning resources to meet the peak traffic \cite{WhatFrontEndDevelopersNeedToKnowAboutServerlessDatabases}.
  
   \item \textbf{Pricing model based on usage} ---
   Along with the autoscaling capabilities, it is desired for the database to have the granular pay-per-use billing model, according to the volume of stored data and number of operations \cite{WhatFrontEndDevelopersNeedToKnowAboutServerlessDatabases}.

   \item \textbf{Security model based on IAM} ---
   When using the HTTP based communication, the integration between the database and the serverless components making the request should be secured by the Identity and Access Management (IAM), compared to the traditional databases which provide secrets required to establish the connection directly.
   Such approach fits the security model of the application built in the serverless architecture, which relies on IAM rules, granting granular permissions to the various components interacting with each other and their operations \cite{PickingDatabaseForYourServerlessApplicationIn2021}.

   \item \textbf{Provisioning and configuring based on IaC} ---
   Instead of handling the additional configuration after the database is provisioned, the IaC tools should be responsible for defining the database configuration in a declarative way, similarly to other serverless components \cite{PickingDatabaseForYourServerlessApplicationIn2021}.

   \item \textbf{Stream based activity log} ---
   The feature of capturing the sequence of modified, created or removed items from the database table is a desired capability, allowing developers to incorporate it in the event-driven architecture of the serverless application and react to data changes accordingly \cite{FauxSQLOrNoSQLExaminingFourDynamoDBPatternsInServerlessApplications}.
   The event-driven flow of data modification can be used to trigger various workloads, for example triggering the serverless function, responsible for notifying the web client, similarly to updates in the receipt processing application covered in section \ref{chapter:examples-receipt-processing}.
\end{itemize}

Some of the databases from the AWS platform satisfy the capabilities mentioned above, which makes them suitable solutions for serverless workloads.
The characteristics of Amazon DynamoDB and Amazon Aurora are discussed in the section below.

\subsubsection{Amazon DynamoDB}

DynamoDB \cite{DynamoDB} is a fully managed NoSQL key-value and document database, suitable for workloads with high throughput, which require predictable performance.
It provides autoscaling capabilities based on defined read and write capacity as well as allowing to configure on-demand scaling, that tracks the load and scales the capacity accordingly.
DynamoDB stores 3 copies of the data in separate Availability Zones in a region, providing greater resiliency and fault-tolerance.

The tables in DynamoDB consist of items, which are built of attributes without predefined schema, enabling a flexible data model.
Each item is uniquely identified by Primary Key, which is used to assign the item to one of the partitions after calculating a hash of it, splitting the table into smaller chunks of data handled by different nodes, which enables the database to scale and ensure predictable performance.
The Primary Key can consist of a single attribute called Partition Key or combine both Partition Key and Sort Key to provide richer query capabilities.
The attributes can include simple values as well as more complex structures, allowing developers to store whole documents.

DynamoDB provides lookups based on the defined Primary Key and range queries on the Sort Key if it is defined as well as projecting items to limit the number of returned attributes.
The solution allows developers to configure two types of indexes --- Local and Global Secondary Index, which provides the capability to define other Partition Key and Sort Key for the table, effectively duplicating the it and propagating the updates between the connected tables, which is essential to effectively query more complex data schemas.

It is billed on a pay-per-use model, based on the number of operations and used storage, including additional cost for the additional features mentioned below.

\paragraph{Additional DynamoDB capabilities}

The database can be deployed in multi-region and multi-master configuration when enabling the global table feature.
By default it uses the eventually consistent reads, however the query can be configured to obtain the data with strong consistency, by reading from the leader partition.
Moreover, the database allows to perform queries and modification for multiple tables in ACID transactions, under some limitations in terms of number of items and amount of data being processed.
To introduce an additional in-memory write through caching layer for DynamoDB, the DynamoDB Accelerator (DAX) can be configured, distributing the requests across nodes in the cluster which cache the data and reducing the response time for read intensive applications.
To integrate with the event-driven serverless architecture, the DynamoDB Streams allows to trigger the Lambda function when the data is modified and react to these changes as well as integrating with other AWS services \cite{DynamoDBFeatures}.

\paragraph{DynamoDB suitability for serverless based workloads}

DynamoDB is suitable for serverless based workloads, ensuring consistent performance by design, even when running with large datasets as well as providing integrations with other services such as AWS Lambda or AWS AppSync.
It shows the power of NoSQL databases in terms of the scalability and performance aspects, however it comes with a cost, affecting the data model and query capabilities.
DynamoDB does not allow to perform joins on data tables and force to segment the data, which allows easier, horizontal scaling as well as bounds the queries by limiting the results of data retrieved with pagination capabilities, which enables to ensure predictable performance.
On the other hand, it requires developers to think about the data model up front, by analysing the access patterns which are required to design the data accordingly, most frequently in a denormalized way.
It requires a shift in the mental model from the relational modeling to approaches such as single-table design to store multiple, heterogeneous item types in one table to query and retrieve them with prejoined data in one request \cite{SQLNoSQLandScaleHowDynamoDBScalesWhereRelationalDatabasesDont}.
Moreover, the data duplication requires to to propagate the changes across the tables to maintain the consistent data model. \\

Both of the example implementations covered in section \ref{chapter:example-implementations} use the DynamoDB for storing simple data models, which effectively suits the key-value lookup access pattern.
The web application performing the receipt processing, defines the Partition Key based on the username to effectively partition the items and Sort Key as insert date to order the results, which makes it convenient to query all of them for a particular user.
Moreover, the DynamoDB Stream is used to trigger the Lambda function, responsible for notifying the user about the processing results, utilising the GraphQL subscription.

\subsubsection{Amazon Aurora}

Amazon Aurora \cite{Aurora} is a fully managed, relational database compatible with MySQL and PostgreSQL, which is designed to work in the cloud based environment.
Amazon RDS is responsible for managing the database, including administrative tasks such as hardware provisioning, database setup as well as patching and performing required backups.

Aurora’s architecture preserves the core features of the relational databases, however it separates the instances running the MySQL and PostgreSQL engines from the underlying storage layer, which is restructured to use a distributed storage system spread across multiple Availability Zones.
The operations are replicated to 6 nodes in 3 Availability Zones, using quorum to ensure the writes consistency.
Transactions are considered as committed when 4 from 6 nodes confirm the operations, while reads reacquire response from 3 nodes. 
It allows further operability of the database, even when one of the Availability Zones would be unreachable.

The database can be scaled vertically by allocating machines with more resources as well as horizontally by adding up to 15 read replicas.
In that case, the cluster endpoint is pointing to the master node, which is responsible for writing to the distributed storage, while the reader endpoint handles the requests, balancing them across the reader nodes.
The storage is allocated and scaled automatically, according to the volumes of data in 10 GB increments.
Aurora can be also configured to utilise the global database, replicating the data across the regions.
Amazon Aurora is billed using the Aurora Capacity Units, which is equivalent to 2 GB of memory with CPU and network bandwidth allocated proportionally, on a per second basis while the instance is active.

\paragraph{Aurora Serverless}

Amazon Aurora also provides the serverless configuration, which allows an on-demand autoscaling configuration based on the application workloads, removing the need to plan the capacity.
It can automatically start up, scale according to demand matching the application usage and shut down when not in use.
It is designed to help with scaling and cost effectiveness for the applications with infrequent or variable workloads.
Nevertheless, the first version of the service is limited in terms of additional features, which are available for the Amazon Aurora with standard configuration, including among the others configuring read replicas, cloning the database, loading the data from S3 bucket as well as invoking the Lambda functions based on the database functions \cite{AuroraServerless}.
However, AWS works on the second version of the database, which grants better and more granular scaling along with some of the additional features of the standard configuration.

\paragraph{Data API for Aurora Serverless}

Data API for Aurora Serverless allows communication with the Aurora Serverless cluster using the HTTP endpoint instead of managing the connections, which is more suitable for accessing the database for the serverless functions.
It alleviates the need to configure the services to run within Virtual Private Cloud (VPC), which is required when communicating securely with the Aurora cluster, when using the connection based approach.
Moreover, it enables to integrate the database securely with other services using the IAM permissions \cite{AuroraServerlessDataAPI}. \\

The Aurora Serverless configuration, including its autoscaling capabilities with pay-per-use model, based on the size of provisioned resources and time they are active, is a viable solution when it comes to selecting a database suitable for the serverless workloads.
Moreover, the introduction of Data API alleviates the need to establish and manage connections as well as allows use of the IAM based permission model.
Currently, Amazon Aurora is under ongoing development, including the second version of the service in the preview mode, which includes additional improvements.

\section{Client Tier}

Building the web applications using the serverless architecture has a significant impact on the architecture of the compute and storage layer.
The use of ephemeral and short-lived functions executed in an event-driven manner affects the client-server communication, creating the need to use dedicated components, serving the role of publicly available interfaces of a web application.
Embracing the notion of asynchronous communication for more sophisticated compute workloads, introduces additional challenges for the interactive web clients, which are required to work beyond the request-response cycle and receive the updates from the application backend in real-time, once the underlying data is changed or when the operation is completed.
Moreover, the cloud platforms introduce various ways to host the web application clients as well as allow them to incorporate additional client libraries to cooperate with the services hosted in the cloud more tightly.
The section below includes an overview of all of the mentioned topics discussed in more detail.

\subsection{Serverless API services} \label{chapter:serverless-client-communication-patterns-for-serverless-architecture}

As mentioned in section \ref{chapter:public-interfaces}, the architecture of web based services requires dedicated components, serving a role of public interfaces, responsible for exposing an API, which will be consumed by the web application clients as well as forwarding requests to the Lambda function or other hosted services.
Amazon API Gateway and AWS AppSync belong to the most frequently used services used along other serverless components, introducing numerous capabilities which can be integrated when developing the web applications in the serverless architecture.
Both services and their features are covered in more detail below.

\subsubsection{Amazon API Gateway}

Amazon API Gateway \cite{ApiGateway} is a fully managed service, enabling developers to create APIs, serving the role of public web application interfaces as well as integrate directly with other components of the cloud platform and developed backend services.
It handles tasks related with maintaining, monitoring and securing APIs, integrating them with other services, providing authentication and authorization capabilities, traffic management and throttling and versioning management.

The API endpoint can be configured as one of the mentioned type, depending on the traffic origin:

\begin{itemize}
   \item Regional --- recommended for general usage and built for clients in the same region
   \item Edge-optimized --- utilising Amazon CloudFront distribution, designed for globally distributed set of clients
   \item Private --- accessible from within Virtual Private Cloud (VPC) and designed for internal APIs and private microservices
\end{itemize}

Amazon API Gateway supports three types of APIs, including HTTP API, REST API as well as WebSocket API, which are described in more details below:

\begin{itemize}
   \item REST API ---
   Defines a mapping between REST resources according to appropriate HTTP methods to integrate with Lambda functions, AWS services or other HTTP endpoints.
   REST APIs allow developers to use additional capabilities, such as defining usage plans with API keys, caching responses, transforming requests as well as providing validation for the data being sent and received.
   \item HTTP API ---
   Suitable for defining mapping for routes and HTTP methods, forwarding the requests to configured services of the cloud platform.
   HTTP APIs are more lightweight and designed to proxy the requests to the downstream services, introducing less network overhead and being considerably cheaper than the REST APIs, however the number of additional capabilities related with operating on the requests is limited.
   \item WebSocket API ---
   Enables developers to define real-time, full-duplex communication between clients and servers, while maintaining and persisting the connection state as well as handling the data transfer between web clients and backend services.
\end{itemize}

API Gateway can be configured to provide authentication and authorization capabilities based on IAM policies and AWS credentials, when integrating with Amazon Cognito Pool or using Lambda authorizers to validate the tokens or request parameters, granting access to the backend services \cite{IDidntKnowAmazonAPIGatewayDidThat}.
Moreover, it can be integrated with AWS Web Application Firewall (WAF) to secure the API from common vulnerabilities, blocking attacks from specified IP ranges, matching particular requests based on the content or user agent.
Proper logging, monitoring and usage metrics are granted by Amazon CloudWatch, based on the client interactions.
The API definition can be described in OpenAPI format, including custom integration which allows to configure the routes with AWS services, and used further by the Infrastructure as a Code tools to deploy the API Gateway configuration \cite{ApiGatewayFeatures}.

API Gateway is billed based on the number of API calls and transferred data as well as the connection duration when using the WebSocket APIs.

The second example implementation, covered in section \ref{chapter:examples-generating-interactive-slideshow-based-on-latex-files}, uses API Gateway to provide simple REST API, integrating directly with AWS Step Function to start the presentation generation workflow and transform the response from the service to obtain the execution identifier.
It is later used when connecting via the WebSocket API to subscribe for notification, once the particular processing flow is completed and the slideshow is available for the user preview.

\subsubsection{AWS AppSync}

AWS AppSync \cite{AppSync} is a fully managed GraphQL service, responsible for processing requests and mapping their different parts to appropriate resolvers, which query the data from multiple data sources, including databases, microservices or other HTTP endpoints, and aggregate it in one response.
The GraphQL schema allows web clients to define strongly-typed queries, granting greater elasticity by defining desired fields, preventing from overfetching and multiple round-trips to the server. \\

Resolvers can integrate directly with other components hosted on the cloud platform, including DynamoDB, Aurora Serverless, Elasticsearch Service as well as use HTTP requests to invoke other services, such as AWS Step Function, or bridge with regional API Gateway endpoints, forming a facade to AWS services.
The resolvers can use mappings, defined in Apache Velocity Language, when requesting the data and receiving the response, forming simple unit resolvers.
Unit resolvers can be composed into pipeline resolvers, performing a more advanced sequence of operations as well as using Lambda resolvers for processing more complex application logic.

The GraphQL subscriptions allow clients to receive updates in real-time. AppSync manages the WebSocket connections, scaling according to usage and delivering the messages to subscribing clients.
The subscription can be triggered based on the client operation as well as one of the cloud platform services, broadcasting the update to subscribing clients, which can additionally filter the message based on its content. \\

AppSync provides additional authentication and authorization capabilities when performing operations or querying the data.
It can use API keys, IAM permissions, integrate with Cognito User Pool to manage users and groups as well as incorporate any identity provider, which is compliant with OpenID Connect protocol.

Moreover, results of resolver processing can be cached, based on defined caching policy including caching keys and authenticated user parameters.
The feature is based on the ElastiCache instances, serving the role of cache with predefined Time To Live (TTL) values, reducing the compute cost for slowly changing data.

AppSync integrates with other services to bring additional capabilities.
AWS Web Application Firewall (WAF) can be configured to enable additional protection, Amazon CloudWatch gathers logs and metrics, while AWS X-Ray grants greater observability for the processed requests \cite{DevelopServerlessGraphQLArchitecturesUsingAWSAppSync}.

The AppSync can be integrated with the web and mobile applications, using the Amplify DataStore to preserve the data on the client devices and make it available for working offline, synchronising the state when the device will be reconnected to the network \cite{AppSyncFeatures}. \\

AppSync pricing is based on data transfer and the number of performed queries, mutations and subscriptions, with additional billing per time when the clients use WebSocket connections as well as depending on the caching configuration.

The first example implementation performing the receipt processing, described in section \ref{chapter:examples-receipt-processing}, uses AppSync to provide GraphQL API for the client application. It uses Lambda functions to resolve some of the queries along with communicating directly with other services to retrieve the data.
Moreover, the communication is enhanced by the real-time subscriptions, which allow applications to update the clients, once the asynchronous processing flow is completed and the state in the database is updated.
The integration with Amazon Cognito allows the application to handle user identities and ensure the returned data belongs to the particular user.
The DynamoDB Stream triggers the Lambda function, which performs the appropriate mutation, resulting in sending an update to the particular client, selecting subscription based on the identity.

\subsection{Client communication patterns for serverless architecture} \label{chapter:client-communication-patterns-for-serverless-architecture}

Depending on the application requirements and the workflow types it incorporates, the communication with the client can range from simple, synchronous requests with one of the backend services, to more complex setup, which requires additional coordination of the asynchronous application logic from the client side.
Section \ref{chapter:serverless-client-communication-patterns-for-serverless-architecture} covers the components frequently used with the serverless architecture and their capabilities.
The section below describes in more detail, some of the use cases of the discussed components along with the suggested communication patterns used in the web application implementations utilising the serverless architecture.

\subsubsection{Synchronous requests} \label{chapter:client-synchronous-requests}

\begin{figure}[]
   \centering
   \includegraphics[width=0.7\textwidth]{assets/04-serverless-for-web-apps/synchronousRequest.png}
   \caption{Example of synchronous communication with Lambda function, DynamoDB and AWS Step Function using API Gateway}
   \label{fig:pattern-synchronous-request}
\end{figure}

For simple use cases the synchronous communication with the serverless application backend is applicable, as mentioned in section \ref{chapter:public-interfaces}.
Figure \ref{fig:pattern-synchronous-request} shows, how the API Gateway can be applied to integrate directly with DynamoDB to retrieve the data along with AWS Step Function to trigger some more advanced workflows in an asynchronous manner, returning with the response to the client to confirm the invocation.
Similarly, the API Gateway can invoke the Lambda function in a synchronous manner, as covered in section \ref{chapter:lambda-function-execution-model}, to process some more complex business logic.
It is essential to be aware that the more sophisticated workflows may be restricted by the time limits of the components serving a role of the public interfaces.
In that case, the processing can be redesigned to use the asynchronous processing, while the periodic, synchronous requests can poll for the status or results of the processing.

In the application covering the receipt processing, covered in section \ref{chapter:examples-receipt-processing}, AWS AppSync resolves the queries by retrieving the data from DynamoDB as well as by triggering the Lambda function to obtain the presigned URL to S3 bucket.
The second example, described in section \ref{chapter:examples-generating-interactive-slideshow-based-on-latex-files}, shows how the API Gateway can be used to trigger the asynchronous processing of the AWS Step Function, while returning the response with the invocation details immediately to the client.

\subsubsection{GraphQL APIs} \label{chapter:client-graphql-apis}

\begin{figure}[]
   \centering
   \includegraphics[width=0.8\textwidth]{assets/04-serverless-for-web-apps/graphql.png}
   \caption{Example usage of AWS AppSync for hosting GraphQL API using various services as data sources}
   \label{fig:pattern-graphql-api}
\end{figure}

The AWS AppSync, which hosts the GraphQL API, can be effectively used to aggregate the data from multiple data sources in a single response.
Simpler logic can be handled by resolvers written using Apache’s Velocity Template Language and communicating directly with the data sources, including for example querying, inserting or modifying items in DynamoDB or communicating with Elasticsearch Service.
For more complex logic, Lambda function can be configured as a datasource to perform the function code, when requesting a particular field from the GraphQL schema, returning the data back to the client.
The request including communication with several datasources can be obtained when using pipeline resolvers, performing a chain action, involving a sequence of calls to multiple services and aggregating the responses.
However, for more complex mutations, the AWS Step Function can be a better choice if it comes to handling errors and retries more effectively, compared to pipeline resolvers \cite{ServerlessArchitecturalPatternsAndBestPractices}.

The GraphQL APIs are beneficial when working on larger applications, including several independent services to provide the API layer in one place, calling configured services accordingly.
The underlying implementation of the services can be changed, as long as the API contract based on the GraphQL schema is maintained, enabling developers to evolve the application architecture effectively.
Moreover, AWS AppSync handled and manages the WebSocket connection, allowing users to leverage the GraphQL subscriptions to receive the updates in real-time, based on the changes of the underlying data \cite{DevelopServerlessGraphQLArchitecturesUsingAWSAppSync}.

The application responsible for receipt processing, covered in section \ref{chapter:examples-receipt-processing}, uses the GraphQL API hosted using AWS AppSync to communicate with various services to perform queries and mutations as well as utilise subscriptions to notify the users when the asynchronous processing flow is finished.

\subsubsection{Working with files securely --- presigned URL} \label{chapter:client-presigned-urls}

\begin{figure}[]
   \centering
   \includegraphics[width=0.6\textwidth]{assets/04-serverless-for-web-apps/presignedURL.png}
   \caption{Example usage of presigned URL when uploading files to Amazon S3}
   \label{fig:pattern-presigned-url}
\end{figure}

The limitations of the AWS Lambda invocation payload to 6 MB (when invoked synchronously, as mentioned in section \ref{chapter:lambda-function-limitations}) along with the API Gateway payload restriction to 10 MB, makes it unsuitable to upload the larger files using this serverless components.
The solution for such a constraint is to upload the files directly from the client to Amazon S3 bucket.
To perform this operation securely, having the control over accessing the objects in the bucket, the mechanism of presigned URLs can be used.
The Lambda function can be configured to obtain the presigned URL for a given bucket, enabling clients to obtain the particular file or put it into under predefined path, making the URL additionally validated based on credentials or until specified expiration time is not exceeded \cite{ServerlessAtScaleDesignPatternsAndOptimizations}.

\subsubsection{Webhooks} \label{chapter:client-webhooks}

For the asynchronous workflow invocations, the client needs to be notified when the operation finishes or the results of the processing are available.
Webhook pattern can be applied when operating with clients, which are external services capable of exposing an endpoint to receive the callback, once the processing is completed.
Example implementation of such a pattern is covered in Figure \ref{fig:pattern-webhook}.
The client calls the API Gateway and receives the confirmation of submitting the call as soon as the request is inserted and stored durably in SQS.
Once the Lambda or some other service performs the processing, the message is sent to SNS which is responsible for performing the HTTP call to the endpoint, defined during the initial request.
It removes the need for the client to poll for the processing status.
Additionally, some mechanism can be incorporated to ensure the communication with the client is trusted by verifying API key or performing authentication in a different way \cite{ServerlessAtScaleDesignPatternsAndOptimizations}.

\begin{figure}[H]
   \centering
   \includegraphics[width=0.6\textwidth]{assets/04-serverless-for-web-apps/webhook.png}
   \caption{Example webhook implementation}
   \label{fig:pattern-webhook}
\end{figure}

\subsubsection{Pushing updates to clients using WebSockets} \label{chapter:client-websockets}

\begin{figure}[]
   \centering
   \includegraphics[width=0.9\textwidth]{assets/04-serverless-for-web-apps/asyncStepFunction.png}
   \caption{Example diagram showing Step Function invocation responding to client using WebSocket once processing is finished}
   \label{fig:pattern-async-step-function}
\end{figure}

Nevertheless, not all clients have a possibility to expose an endpoint and receive a callback once the asynchronous processing is completed.
For the web clients the WebSocket communication can be established to receive the notification in real-time, containing the state of the processing.
Figure \ref{fig:pattern-async-step-function} shows how the update can be pushed to the client, when the asynchronous workflow of an AWS Step Function is completed.
The example depicted on the diagram uses AWS API Gateway with two types of API - REST and WebSocket API.
Clients use the REST API to invoke the AWS Step Function and obtain in response the execution identifier or some other token.
It is later used to connect to the WebSocket API, calling one of the tasks involved in the orchestration workflow.
When the client is connected as well as the processing is finished, the Step Function invokes the last step of the processing, responsible for sending the message to the client, notifying that the asynchronous processing is completed \cite{ServerlessAtScaleDesignPatternsAndOptimizations}.

Similar approach is used in the example of interactive presentation processing, covered in section \ref{chapter:examples-generating-interactive-slideshow-based-on-latex-files}.
The provided implementation uses DynamoDB additionally to store the association between connected client identifier and the function execution identifier, reading the stored value for each of the chunks of parallel execution.
The opened, bidirectional communication connection can be applied for richer eventing between the backend of the application and the web or mobile clients.

\subsection{Hosting web clients}

Amazon Web Services along with other cloud vendors provide various ways to host the web application clients.
Depending on the type of web clients, which are covered in section \ref{chapter:client-application-tier}, a different approach can be applied.
Along with hosting static assets, Lambda function can be effectively utilised to render the client interfaces on the server side.
Both approaches are discussed in more detail in sections below.

\subsubsection{Hosting static assets}

Majority of cloud vendors provide a possibility to host static files and distribute them to the end users, leveraging the worldwide datacenter network with additional locations serving the role of Content Delivery Network (CDN), hosting the assets from the locations closer to the users.
AWS enables customers to host the files using Amazon S3 buckets, using additionally the Amazon CloudFront Distribution as a Content Delivery Network  \cite{AWSWebHosting}.

It provides necessary capabilities to host the Single Page Applications as well as the static websites.
Single Page Applications, described in detail in section \ref{chapter:client-application-tier-single-page-app}, are dynamically rendered in the web browsers and communicating with the web application backend in an asynchronous manner, providing rich and interactive user experience.
During the build process, such applications are effectively processed into the resulting artifact, which is a set of static files, including mostly JavaScript scripts, but also stylesheets and other static assets.
In terms of statically generated pages, covered in section \ref{chapter:client-application-tier-static-site-generators}, the results of the processing in form of the static files can be similarly hosted in the cloud environment, using automation to update the content of the S3 bucket once the generation process is completed.

An example diagram, describing how the Single Page Application can be hosted in the cloud environment along with communication with the backend services, is presented in the Figure \ref{fig:pattern-client-hosted}.

\begin{figure}[H]
   \centering
   \includegraphics[width=0.9\textwidth]{assets/04-serverless-for-web-apps/hostedClient.png}
   \caption{Example diagram presenting Single Page Application hosted in the cloud environment}
   \label{fig:pattern-client-hosted}
\end{figure}

An example implementation of receipt processing application, described in section \ref{chapter:examples-receipt-processing}, uses this approach to host the Single Page Application in the Amazon S3 bucket, leveraging additionally the CloudFront Distribution as a CDN.
The build process incorporate the Github Actions \cite{GithubAction}, serving the role of processes automation which configures the continuous deployment pipelines, during which the client application is built and deployed in the cloud environment, once the change is merged to the main branch of the repository in which the application code is stored.

\subsubsection{Server-side rendering}

Nevertheless, hosting the static sites is not the only possible solution for hosting web clients.
Lambda function can be used to render the web pages on the server side, utilising the benefits of server-side rendering, discussed in section \ref{chapter:client-application-tier-server-side-rendering}.
When using the reliable connection and efficient processing, the web page can be rendered and sent back to the client, reducing the overhead required to load the whole Single Page Application as well as optimising the website for search engines.

An example implementation of the server-side rendering using the Lambda function is presented in Figure \ref{fig:pattern-client-server-side-rendering}.
Amazon CloudFront is configured to forward the requests to the API Gateway endpoint, which is invoking the Lambda function, responsible for rendering the web page on the server side.
It can additionally call the underlying backend services to request the additional data, required to render content of the page.
When the web page is generated, API Gateway returns the resulting file, utilising additionally CloudFront caching to optimize the client-server communication for the subsequent requests.

\begin{figure}[H]
   \centering
   \includegraphics[width=1\textwidth]{assets/04-serverless-for-web-apps/serverSideRendering.png}
   \caption{Example diagram presenting the server-side rendering of web pages using Lambda function}
   \label{fig:pattern-client-server-side-rendering}
\end{figure}

Moreover, the Lambda@Edge can be utilised to generate the static content at the edge locations closer to the users to reduce the communication latency \cite{BuildingServerSideRenderingForReactInAwsLambda}.
Diagram presenting the usage of Lambda@Edge is presented in Figure \ref{fig:pattern-client-server-side-rendering-edge}.

\begin{figure}[H]
   \centering
   \includegraphics[width=0.9\textwidth]{assets/04-serverless-for-web-apps/serverSideRenderingEdge.png}
   \caption{Example diagram presenting the server-side rendering of web pages using Lambda@Edge}
   \label{fig:pattern-client-server-side-rendering-edge}
\end{figure}

\subsection{Thick and thin client}

The notion of thin clients refers to the application clients, which serve as a thin presentation layer, communicating with the underlying services, which handles the significant part of the application logic.
Contrary to that, the richer and more interactive thick clients can incorporate some part of the business logic on the client side, making the backend services a thinner layer, responsible for validating the performed actions as well as communicating with the database to preserve the changes durably.

At the first glance the architecture of web application clients should not be significantly affected by the changes of the underlying server and storage architecture to use the serverless architecture.
However, when taking into consideration the possibility of direct communication with cloud-based services and changes in the workflows processing, gearing towards asynchronous pipelines, it makes the application clients include more complex logic, including additional integrations with serverless components to access them securely as well as moving some part of the orchestration logic to the client in terms of handling the communication with the serverless infrastructure.

The initial architecture of the CloudGuru learning platform is an example of such rich and thick client web application client, communicating directly with various third party services as well as smaller, cloud based services, as described in sections \ref{chapter:serverless-example-application-backend} and \ref{chapter:serverless-suitability-cloudguru}.
The web application client integrates directly with the Firebase, querying for the data, performing updates as well as receiving the messages in real-time, when the underlying data model changes.
Moreover, the integration with Auth0 as an identity provider, responsible for handling authentication and authorization, is performed on the client side, incorporating additional part of the logic into the web application client. \\

It reduces the role of the server application as a middleware between the client and datastores or other application modules, making the communication directly.
The amount of code responsible for handling the application logic in the server layer is smaller, while some part of it is pushed to the clients, for orchestrating even some more complex workflows including communication with several components.

However, not all logic is suitable for processing on the client and some part of the logic, including working with the sensitive data or requiring additional validation from the security standpoint needs to be implemented outside of the client application logic, because the actions executed in the user's browser cannot be fully trusted for such operations.
For example when finalising the transactions, granting users access to the course or when validating if the user has permissions to access the resources, the additional application logic incorporated in Lambda function is required to protect the underlying resources.
In that case the client application performs certain operations, orchestrating the whole flow, while communicating with the application logic incorporated in the Lambda functions, which validates the privileges.
Similarly, when triggering emails to the groups of users, the processing needs to be performed in the cloud platform, because the emails may include sensitive user data such as emails or their personal data. 

Such a shift is possible thanks to the desktop and mobile hardware improvements, making the devices more powerful to handle the tasks of feature-rich and interactive web application clients.
Moreover, the wide range of third party services exposing various capabilities, including the identity management, protected operations on the files, notifications and real-time data streaming with the offline synchronisation, makes them an appealing solution to incorporate into the interactive and thick clients.
The integration is possible thanks to numerous programming frameworks, enabling developers to write larger and more complex web application clients, incorporating additional libraries to integrate with services or communicating with cloud-based components, using the HTTP protocol with additional token to grant access to predefined resources in a secure manner \cite{ServerlessTheFutureOfSoftwareArchitecture}. \\

The serverless architecture encourages developers to build more interactive and thick clients, handling the direct communication with external services as well as performing some orchestration of the processing, reducing the amount of application logic, which needs to be handled in the serverless functions.
However, some types of the processing needs to be executed in the cloud environment, especially when working with sensitive data, when the operation needs to be additionally validated against the application logic or external services, granting access or authorising the operation when appropriate credentials are provided.
It is possible to build a thin client and incorporate most of the application logic in the server layer of the application, however it may be not suitable with the serverless paradigm, reaching various limitations of the serverless components as well as most frequently resulting with additional cost, when handling the logic and orchestrating the communication using the serverless platform. \\

Similar conclusions can be formed when considering the example implementations.
The application responsible for receipt processing, covered in section \ref{chapter:examples-receipt-processing}, incorporates an additional library to integrate with Amazon Cognito, incorporating the authentication and authorization mechanism in the application.
Moreover, it uses the presigned URL to communicate directly with the Amazon S3, to retrieve and upload the files directly, after the user is authenticated when querying for the URL.
In terms of the application generating the interactive slideshows, described in section \ref{chapter:examples-generating-interactive-slideshow-based-on-latex-files}, it incorporates additional logic responsible for orchestrating the processing and obtaining the results of the generation process.
Initially, it invokes the AWS Step Function to trigger the workflow, however later it needs to establish the WebSocket connection, receiving the updates and retrieving the assets from Amazon S3 once the processing of the serverless workflow is completed.
In both cases, the web application client is enhanced with some additional logic, incorporating it with the cloud platform and adjusting the client application logic to overcome some limitations and make it more suitable for the serverless processing model.