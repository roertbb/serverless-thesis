\chapter{Web applications} \label{chapter:web-apps}

% aplikacje webowe - czym się charakteryzują?
% jakie założenia trzeba spełnić? jakie wymagania stawiane są przed deweloperami?
% ---
% What are the core ideas of web applications?
% what are the requiremenst from the system / components perspective?
% What architectures are commonly used?

\section{Origins}

The idea of web applications has been changing over the years, alongside with the increased interest in using them. Currently, web applications can take a variety of forms, which lead to the emergence of various architectures and solutions suitable to develop efficient and user-friendly software working in the browsers. Niels Abildgaard \cite{PerspectivesOnArchitectureEvolution} analyzed in a great detail how the perspective of web application evolved over the years, discussing the variety of web application architectures and some of the common patterns used in their development nowadays.

The initial idea of the World Wide Web was to exchange information more easily with a broader audience. To achieve that, the client-server architecture was used, in which the client received desired information from the server responsible for storing them. The client could make a request to the server using HTTP protocol and receive some static content (in form of HTML file or other static assets) or dynamically generated content (returned as a result of program execution triggered by the server). The term web applications is more suitable for the latter one, which represents a more interactive type of website, including some business logic and returning the dynamic response based on the data.

The growth of server-side logic and its complexity, influenced the idea of splitting single server application into numerous Web Services, responsible for particular part of application logic and communicating with other clients or servers. Service-Oriented Architecture encouraged developers to split larger applications and enable them to cooperate using standardized communication protocols. Introduction of Representational State Transfer (REST) \cite{RESTPrincipledDesingOfModernWebArchitecture} refined the client-server communication to be stateless, focusing on cacheability to improve performance and communication via uniform  interfaces using HTTP protocol.

Another idea, which enabled developers to deal with the complexity of web application, was to differentiate between specific layers in the web application architectures. Early web applications distinguished presentation layer, displayed to the users, which communicated with the business logic layer, utilising data layer underneath for persisting the application state. Other modern frameworks utilised Model-View-Controller (MVC) approach, in which the visual part was represented by the View, Controller based on the request was responsible to the interaction with Model, containing necessary data and the business logic.

The next factor which led to enhancing the interactivity in the browser, was the emergence of Ajax (Asynchronous JavaScript and XML), allowing to load data dynamically and asynchronously without reloading the whole page. Growth of JavaScript popularity enabled developers to build more interactive and dynamic applications, detached from the initial markup, called Single-Page Applications. Additionally, more business logic started to be implemented on the client side of web applications, resulting in so-called thicker clients.

Alongside the technology evolution, the requirements put in front of the web applications also increased, most often pushing them to the limits to provide better user experience, while working on more demanding computations and processing larger amounts of data than before. To meet such requirements, new patterns and architectures emerged, allowing to achieve the desired capabilities and satisfactory performance, but most commonly resulting with tradeoffs in different areas.

The client applications became detached, self-sufficient and more interactive, taking most frequently the form of Single Page Applications, and providing user experience similar to the mobile and desktop applications. Nevertheless, to address the performance problems, other architectures for the client applications have emerged, and different communication techniques enabling efficient communication and data exchange have arised.

To ensure the scalability and performance of the server application in face of the business logic complexity, the systems have begun to be broken down into smaller, independent components that communicate with each other, leveraging the microservice and event-driven architectures. To ensure reliable communication within such a distributed system, message brokers have started to be used more frequently.

Along with the increase in the amount of data being processed and stored, the new approaches and paradigms of database management systems have been introduced, to meet the expectations of high performance, scalability and resilience. However, to fulfil aforementioned requirements, other constraints needed to be weakened, introducing another area of tradeoffs in the data management systems.

\section{Defining web application}

Taking into account the various types of web applications and numerous purposes they may serve, it can be difficult to clearly present the core ideas of web application. Researchers tried to define the term web application to describe the wide range of web applications. One of the definitions can be found in "Web Application Architecture" published by Leon Shklar and Richard Rosen \cite{WebAppArchitecture}.

\begin{quotation}
A Web application is a client-server application that (generally) uses the Web browser as its client. Browsers send requests to servers, and the servers generate responses and return them to the browsers. They differ from older client-server applications because they make use of a common client program, namely the Web browser
\end{quotation}

However, the perspective of web application has been changing over the years, as well as along with requirements they face. Web servers providing mostly static content changed into more complicated applications, that consist of multiple layers where each of them is responsible for a selected fragment of processing client request. For more complicated use-cases it was not enough and the server application took the form of a distributed system, that consist of multiple services communicating with each other to process the application logic. Alongside with the development of server application, the database management systems have been evolving, presenting new paradigms to meet the requirements of the web applications using them. Lastly, the presentation layer of web applications also changed rapidly, resulting in dynamic and interactive client-side applications including parts of business logic.

Niels Abildgaard also noticed the progress of technologies and architectures used in web application development and proposed another definition of web application based on his research \cite{PerspectivesOnArchitectureEvolution}.

\begin{quotation}
A web application is a client-server application with any number of clients and servers, in which client-server communication happens via HTTP, in which both client and server may be execution environments, and in which the server may be any arbitrarily complex system in itself.
\end{quotation}

Taking into account both definitions, we can refer to web applications used currently as mainly two communicating with each other parts --- client (preferably used by web browser) and server, which can be any complex system by themselves.

\section{Requirements} \label{chapter:web-apps-requirements}

Having described the development of web applications over the years, it is relevant to consider what were the requirements placed on the web application that lead to such a development process. Currently, web applications can operate in numerous areas, and based on the business needs, target various customer groups and other non-functional requirements --- the desired capabilities they need to fulfill may differ. For example, a small e-commerce shop serving a handful of customers a day within one country will have different requirements than a large social network, in which millions of people participate worldwide on a daily basis. Nevertheless, several generic requirements applicable to the majority of web applications can be distinguished, but the importance of them, and to which extent they should be incorporated may differ depending on the application use-case and its scale.

\subsection*{Performance and Scalability}

The performance of web applications can be considered from two separate perspectives --- user and system point of view \cite{DesignDataIntensiveApplications}. In the first case we can think of the user's interactions with the web application, and the time that it needs to respond to requests made by the user. To define how the application should behave in terms such as service level objectives (SLOs) and service level agreements (SLAs) can be introduced to characterise the expected performance and availability of web application (for example if the service median response is served within 200ms and for the 99th percentile is under 1s). Nonetheless, for most of the time the user is not using the application alone. The interactions of numerous users need to be taken into account by various components of the underlying application that need to handle them efficiently. When the load increases (in terms of the traffic, data volume or complexity of actions), the system should scale accordingly to reasonably handle the traffic without performance degradation. 

Scaling can be referred to as the ability to cope with increased load which can take various forms, for example number of requests per second, read to write ratio for database or number of simultaneously active users. To achieve this two different approaches can be used: scaling vertically (using more powerful machines) and scaling horizontally (distributing the load across multiple smaller machines). Some of the systems can automatically scale by allocating new computing resources based on the metrics related with traffic or performance, which is a desirable feature especially when the load is unpredictable. By building applications servers in a stateless manner, the horizontal scaling can be easily used to meet the demand. Unfortunately, stateful components can be harder to scale. Some storage components are designed and implemented to handle scaling gracefully, for other components techniques such as data fragmentation or replication have been introduced to meet the desired performance requirements.

\subsection*{Reliability}

Reliability refers to the fact that the system should continue to work correctly (perform correct processing as user expected at desired level of performance), even when the failure occur \cite{DesignDataIntensiveApplications}. The designed system should be resilient and fault tolerant, preventing the web application from stopping providing the service to users. The aforementioned service level agreement (SLAs) metric can also refer to the uptime of the services, stating for example that not less than 99.9\% of availability time within a month is acceptable.

The origin of failure may be different, but some of the major factors can be hardware faults, software bugs and human errors introduced while developing and managing the application. To prevent them, various techniques can be introduced. Hardware faults in form of hard disk crash, power grid blackout or network cables that are not plugged in properly can be masked by introducing redundant components that are utilised until the broken component is replaced or fixed. Software errors in the form of introduced bugs or other human errors related with configuring services can result in unexpected and undesired behaviour of application or even other components of the system. That area of errors is harder to mitigate, but proper manual and automation testing can prevent from introducing errors, while monitoring and analysing service behaviour in production will raise an alert if some deviation is noticed.

The reliability is expected not only from larger web applications, but also from smaller ones. Downtime and software bugs in the web application can contribute to loss in revenue and damage the reputation.

\subsection*{Security and Compliance}

These days web applications became one of the most popular platforms for exchanging information ranging their utilisation from numerous social media platforms to more critical areas such as bank accounting or accessing sensitive and confidential information stored on some servers in a digital form. The security breach of some web applications could result in compromising large amounts of sensitive data, leading to severe legal and economical consequences. Along with the growth of web application ecosystem and their complexity, the attack surface enlarges and the risk of introducing some security vulnerabilities is increasing.

It is substantial to ensure a proper level of security based on guidelines and frameworks defined by organisations and authorities working in the field of security, hardening the web application and mitigating the possible attacks to ensure compliance with security standards. Web application design should ensure that users identity and sensitive information are properly secured, giving access to resources only when the user identity is verified along with expecting adequate permissions \cite{ASurveyonWebApplicationSecurity}.

\subsection*{Maintainability}

Although maintainability may not be the obvious requirement for web application, it is an important factor for business executives, developers and people working in operations. Majority of the costs from a product development perspective is not the initial version of application, but it is related to the maintenance taking form of keeping the system operational, fixing bugs and resolving failures \cite{DesignDataIntensiveApplications}.

To maintain the software efficiently, the operational aspects such as monitoring, patching, deployment and maintenance of a stable environment should be easy and mostly automated to make sure the system is running smoothly. From an engineering perspective the system should be simple, by removing unnecessary complexity connected with tight coupled modules, tangled dependencies and inconsistency in terminology or architectural patterns. By reducing the complexity the maintainability of the application will be easier. It will also increase agility and make the evolution of the system easier, which will result in reducing time of introducing new features and reacting to architectural changes required to make the system operate efficiently.

% TODO:rb to be considered - maybe mention in intro would be enough
% \subsection*{User Experience}

% Modern web application clients are more frequently becoming more complex, interactive and feature-rich applications including larger parts of business logic. The success of products based on web applications is more frequently connected with the fact how the user perceives it and it is becoming more important to make sure that application interface is aesthetic, while the overall user experience of the application is intuitive. Nevertheless, the user interface is not the only factor impacting how the user perceives the application, the efficiency and performance also plays a significant role, which is correlated with the whole system design.

% The idea of user experience (UX) is quite a broad term on the verge of cognitive science and human-computer interactions. It is considering not only the interface design, but also the system usability, ergonomics and performance, inspecting how the users feels when interacting with the system. Enhancing user experience is one of the main goals of user-centered design, inspecting the perception of the value of the system, how easy and efficiently users can perform various tasks and looking into processes within the system, if they are intuitive and pleasant for users. Many companies are using different analytic tools nowadays  to understand how users utilise the products.

% The diversity of users and platforms also plays an important role. One of the consequences of that is the fact that many applications are suitable to use from mobile phones, because more and more people use smartphones on a daily basis. Also accessibility issues are gaining more awareness to make web applications suitable for people with some of disabilities \cite{WhatIsUserExperienceDesign}.

\section{Modern web application architecture}

There are many potential perspectives considering the architecture of modern web applications, when taking into account the areas they operate on and specified requirements that they need to fulfill. Below, the most common patterns and techniques used in modern web application architectures are discussed, that enable the systems to perform efficiently and meet the desired requirement and specification. 

It is divided into three sections taking into consideration the server application, underlying data layer, and the web application client, with regards to the concepts presented in the figure \ref{fig:web-app-architecture-concepts}. However, some of the discussed patterns goes beyond such classification, especially when they are referring to the communication between layers boundaries.

\begin{figure}[h]
    \centering
    \includegraphics[width=1\textwidth]{assets/03-web-applications/WebAppLayers.png}
    \caption{Considered topics regarding the web application architecture}
    \label{fig:web-app-architecture-concepts}
\end{figure}

\subsection{Server Application Tier}

\subsubsection{Microservice architecture} \label{chapter:web-apps-microservices}

Microservice architecture is one of the trends in the modern web application development gaining more interest, especially when taking into consideration various benefits and success stories of the biggest companies using it to run their services in the web. To consider microservices it is useful to compare it with the monolithic architecture, which refers to building server-side applications as a single logical executable unit responsible for handling the HTTP requests, execute the domain logic, retrieve and update data in the database and send back the adequate response. Such architecture can be effectively applied when building simple applications, but when the system grows and evolves over time it is harder to preserve good modularisation and keep changes within the modules they should belong to, expanding unnecessary complexity into other parts of the system. Altogether with the system growth, it may require to be scaled accordingly to the load. This can be achieved by running multiple instances of the server application behind load balancer, but it requires to scale the whole application, rather than particular components that require more resources. Another downside of the larger applications built as a monolith is the fact that every change requires rebuilding and deploying the whole application including the parts of the system, even not affected by changes.

Microservice architecture \cite{FowlerMicroservices} is described as a solution to these problems. We can refer to microservices as an approach to developing applications as a set of small services, that each of them runs on its own and communicate via message passing using standard data formats and protocols via well-defined interfaces. Each of the services is built around specific business capabilities providing module boundaries within a domain it operates in - mitigating the problem of good modularisation. Moreover, each of the components can be deployed independently reflecting the notion of agile development and encouraging to utilise continuous deployment. Scaling can be also applied in a more granular manner affecting only the services that require more resources to perform effectively under the load.

\paragraph{Decoupling system components}

Separating larger systems into independent services can help with ensuring firm module boundaries within the domain of the system. It makes it easier to maintain the components overtime and introduce required changes independently from other components. Alongside with the system decomposition, various teams can take care of selected services focusing on them in greater details. Moreover, greater granularity enables to focus only on the requirements specified for the particular component, applying required changes to meet them, rebuilding it to be more performant or splitting into separate components when the evolution of managed context will require that.

Due to services separation, each of them can be written using different languages, frameworks and libraries that suits best for the performed task. The underlying data storage can be selected independently as well, because each of the components manages its own database. However, most frequently the set of used technologies is limited to the predefined set for easier maintenance. Such opportunities encourage to experiment and try out new tools that can be applied through a gradual migration when they introduce desired capabilities \cite{FowlerMicroservicesTradeoffs}.

\paragraph{Microservices deployment}

Another consequence of the service separation is the possibility of independent deployments. More teams and products embrace the idea of continuous integration and continuous delivery using automation heavily to test software and verify the quality of it. Another often accompanying process is continuous delivery enabling teams to release a new version of the product to the production environment more frequently, even many times a day. Various modern deployment techniques make it possible to introduce new versions of the software with no service downtime. Such an idea is appealing from the product management perspective, reducing cycle-time between ideas and introducing new features into the product, responding quickly to the market changes. In case of microservices the idea of infrastructure and operational automation is essential to efficiently handle the independent service management and ensure good product quality \cite{FowlerMicroservicesTradeoffs}.

\paragraph{Scaling microservices}

Microservices are typically packed and deployed using containers to any platform supporting containerisation. It enables the operation teams to effortlessly relocate and replicate services across heterogeneous platforms. Each of the system components can be easily replicated, locating multiple services on the same host and dynamically scaling them according to the load, independently from other services which may not experience an increase in traffic. Due to that fact, the system can remain performant by allocating additional hosts when it is needed, and deprovisioning resources, when they are becoming redundant. Such a feature makes microservices a perfect technology to be used in the cloud.

The replication of services and spreading them across different hosts ensures fair distribution of the load and increases the availability of the components in case of the hardware faults, making the system robust and fault tolerant. Due to the fact that microservices create effectively a more complex distributed system compared to monolithic architecture, it is a desired feature to make the components designed for failure. Errors introduced into developed software or hardware faults are inevitable, in case of the unavailability of other suppliers the service needs to respond as gracefully as possible. Due to that, microservices value service contracts introducing patterns like tolerant reader, and other consumer driven contracts enabling to evolve services independently, while on the other hand introducing patterns like circuit breaker to make sure that failure of one of the components will not affect the whole system. To prevent the system degradation in case of failures the application should be tested when some of the services will be unavailable. Proper monitoring setup should detect the system failures quickly and automatically restore the services if possible. Moreover it can be utilised to measure various business relevant metrics, providing warning and triggering alarms when some inconsistencies in the system behaviour will be noticed \cite{MicroservicesHowToMakeYourApplicationScale}.

\paragraph{Downsides of microservice architecture}

Aside from introducing additional complexity, microservices are also not free from other downsides. They can be a good solution when handling large enterprise applications, but for smaller products they can introduce unnecessary overhead. To ensure the acceptable performance in such a distributed system, asynchronous communication is used most frequently. Moreover, the decentralised data management allows each of the services to persist its state independently from others, which makes the transactional operations more complex and costly. However, from the business perspective it is important to ensure high availability of the system. Some of the business processes tolerate temporal inconsistencies, in which transactionless coordination between services or aforementioned asynchronous messaging can be applied, ensuring the eventual consistency. It enables the system to work efficiently, but requires it to be designed to tolerate inconsistency windows between updates propagation.

The frequent and independant deployments are a great advantage of microservice architecture, but handling the rapidly changing set of components is a great operational challenge. In the case of complex solutions, automation is essential to support administration of the system. From the development perspective, working on the smaller components of the system may seem to be easier, but the real complexity might not be eliminated, but rather split across multiple interconnected services. Designing and working with such a system, requires a greater skill and can benefit from using more specified tooling. In that case, embracing DevOps culture can help with greater and tighter collaboration between developers and the operation team working together on the application \cite{FowlerMicroservicesTradeoffs}.

\subsubsection{Event-driven architecture} \label{chapter:event-driven-architecture}

When designing the system it is a common practice to model the fragment of reality as a set of entities and relations between them, then map them directly to the data model. Commonly, there is an application layer within the application utilising a set of service modules containing and performing the application logic, which update the models depending on the result of the processing. The model capabilities are frequently limited to basic create, read, update and delete operations. This approach can be applied successfully in many simpler applications, although when the complexity of business logic grows, it may become more difficult to efficiently manage and develop it \cite{FowlerAnemicModel}.

To deal with the business logic complexity, Eric Evans suggested an established set of practices and processes supporting modularization of large systems based on business context called Domain Driven Design \cite{EvansDDD}. He presents the approach of thinking about the model of a complex system as a set of many smaller models, which are representations of separate business contexts. Each component in the system exists within its bounded context representing autonomous business domains and its model is used only within that scope, reflecting actions suitable for a given domain. It enables developers to better understand the underlying business logic domain and reason about the business processes and system behaviour instead of modeling it as a state of particular objects.

\paragraph{Event sourcing}

Event sourcing is another approach influencing the way application state can be perceived. Instead of modeling the application logic in a structural way as a set of logical entities that are mapped and stored in the physical tables in the database, the events leading to the current application state can be stored. When the event occurs within the system, it is persisted, and appropriate change to the application state is made, based on the event data and application logic responsible for interpreting it. The structural model saves only the current state of the system, while the event sourcing enables auditability in form of the sequence of events leading to particular state. The application at any time can be recreated by applying a sequence of events in an order in which they occured.

Most frequently, to react to some events occurring within the system, some specialized component provides a mediating mechanism enabling for the event transmission in form of message bus or message broker. The latter one allows for indirect communication between numerous components subscribing for certain event occurrences and publishing them to all interested components, when the event occurs within the system \cite{MicroservicesArchitecture}.

\paragraph{Command query responsibility segregation}

Command query responsibility segregation (CQRS) is a design pattern frequently used together with event sourcing. It distinguished separation into two action types --- commands (responsible for modifying the application state) and queries (capable of reading it). Each of the groups uses different models for updates and queries, enabling optimisations that increase performance of such a solution. The write model can take the form of events, while the read model can be formed as a more complex domain model, which can be even denormalized to some extent to minimise the number of operations required to get the data and perform queries faster. For each of the types of the operations, a different database solution can be selected, which satisfies the requirements of web application and enables independent scaling. 

Despite the advantages, event sourcing and CQRS comes with a cost. They increase the implementation complexity of a given solution and due to different components responsible for storing data for each of the models, the application state may not be strongly consistent \cite{MicroservicesArchitecture}.

\subsection{Data Tier}

\subsubsection{Database paradigms} \label{chapter:database-paradigms}

Over the years, traditional relational database managements systems \cite{RDBMS} established their position and proved to be a mature and stable solution for storing and quering the structured data. Nevertheless, with the growth of Internet and data volumes transferred over the network for example by social media or streamed from various sources, the relational model has shown some limitations related with scaling to handle such large amounts of data efficiently and at the same time maintain high-availability and fault tolerance. NoSQL \cite{FowlerNoSQLDefinition} has emerged as a response to the needs of scalable database management systems, handling increasing data volumes and offering higher availability, but at the same time sacrificing consistency quarantees or quering capabilities as a tradeoff. Various NoSQL database management systems have some common characteristics, including among the others, less restrictive and more dynamic data model, support for running within the clusters and providing greater availability and performance metrics, which are desired apabilities for the services running in the Internet nowadays. As with many tools, NoSQL databases are not free from downsides, and other undesired properties in regards to some classes of problems. Relational databases are still playing a significant role and provide a specific combination of desired properties for other types of applications and they will be used alongside NoSQL solutions, depending on the problems and requirements that the system needs to fulfill \cite{FowlerNoSQLGOTO}.

The overview of the most common approaches regarding database management systems is presented below. It is divided into categories presenting the different database paradigms along with examples of such offerings, and their application and most common use cases.

\paragraph*{Relational databases}

Relational model was proposed by Edgar Codd and refers to organising data into relations (tables), which consists of unordered collection of tuples (rows). It is efficient for storing and querying the data with regular and predefined structure, containing the relations between records from various tables based on their identifiers.

The data can be updated or queried using SQL, which is a declarative language, enabling to specify the pattern of the data and leave the execution to the database engine. Most of the traditional, relational databases support strong consistency, transactional processing and are ACID compliant, which makes them suitable for problems of transaction processing (banking transactions, sales) and batch processing (payroll, invoices). Besides that, the relational model is quite flexible and can be generalized for broad range of problems, which is justified by the fact that many web application use that model nowadays.

With the system evolvability and increase in it's complexity, the various situation from reality can be harder to model using the relational model. When the data schema is normalized to avoid the duplication and other undesired features, the queries that include joining multiple tables may require some time to be executed. Another concern refers to the fact that most application is built using the object-oriented programming languages, that require to translate the data between the relational and object-oriented model. Numerous object-relational mapping (ORM) libraries tend to reduce the amount of overhead, and boilerplate code requires to translate the layers \cite{DesignDataIntensiveApplications}.

The most popular databases utilising relational model include among the others PostgreSQL, MySQL and Microsoft SQL Server.

\paragraph*{Key-value databases}

Key-value storage enable to store a set of key-value pairs, and it is similar to data structures like hash map, dictionaries or objects available in some programming languages, but preserve the data in the memory. The data can be stored or retrieved by passing the unique key, which points to some arbitrary value preserved in the storage.
The storage does not assume any structure for the values itself, which can be an arbitrary value or more complex structure, leaving the interpretation of data to the application logic layer.
It exposes most frequently a limitted interface allowing to create, read, update and delete data, without possibilities to perform range queries or other more complicated operations \cite{NoSQLDatabaseSystemsSurveyDecisionGuidance}.

Most frequently, such simple key-value storage persist the data in memory, which enables them to perform operations with low latency and high throughput. Such databases are used most frequently for caching purposes, making it acceptable to lose the data when the machine is restarted. However, some of the offerings provide mechanisms allowing them to preserve the data by utilising special hardware (battery-powered RAM), writing log of changes or making periodic snapshots preserved on the disk or replicate the in-memory state to other machines. Key-value storage owe their low latency by reducing the overhead related with encoding in-memory data structures that need to be written to the disk. Additionally, some of such databases provide additional data structures such as priority queues or sets, that are harder to implement leveraging disk based indexes. Redis \cite{Redis} and Memcached \cite{Memcached} can be distinguished as representatives of that category \cite{DesignDataIntensiveApplications}. 

\paragraph*{Document-oriented databases}

Document databases enable to store documents that are containers for key-value pairs, which form semi-structured documents that are most frequently stored in formats such as JSON. The documents are grouped together in collections that form hierarchical structure, and can be indexed for faster queries. The documents' data schema is implicit and it's interpreted when the documents are read, which grants better flexibility and possibility to store arbitrary keys and values in the documents. On the other hand, it does not guarantee any document structure and attribute possesion.

Instead of normalising the data,and encoding the relations, the document model prefers to embed the data into single document, which is a tradeoff, granting faster reads due to data locality, but on the other hand it can introduce data duplication, it makes the writing and updating documents more complex, which may be not suitable for the relational data forming connected structures. Nevertheless, discussed approach enables developers to query the documents by matching semi-structured data, selecting parts of them or performing aggregation tasks on the underlying data. 
The disadvantage of the model is the fact that the whole document need to be retreived when selecting only part of it and the entire documents need to be rewritten on updates \cite{DesignDataIntensiveApplications}.

The document model can be more convenient from the development perspective, reducing the impedence mismatch by working on the JSON format, which is more similar to the objects compared to the data from relational model. The representants of the document model databases are MongoDB \cite{MongoDB} and CouchDB \cite{CouchDB}.

\paragraph*{Wide-column databases}

Wide-column databases enables to store items as rows described by multiple column families. Most frequently, they are technically implemented as distributed, multi-level sorted map. The first level called row keys, identify rows that themselves consist of key-value pairs. The second level, called column keys, refers to arbitrary set of columns without defined schema, which are partitioned into column families colocating the data that are used together on the discs. Some of the databases introduce third level, which consists of the timestamps.

Such a design enables to efficiently compress the data, collocate the informations used together for faster reads and effectively partition and distribute the data across nodes within the cluster based on the row key value to provide easier scaling, replication across multiple nodes and decentralisation. The model provides limited data querying possibilities, most frequently allowing to constraint the records only on the row keys and does not support joins between tables \cite{NoSQLDatabaseSystemsSurveyDecisionGuidance}.

The wide-column database design is sufficient for operations requiring frequent writes, but infrequent reads and updates, such as recording time series and storing IoT device measurements. Cassandra \cite{Cassandra} and Apache HBase \cite{HBase} are some of the most popular options that belong to this category.

\paragraph*{Graph databases}

When the data is characterized by many connections between modeled entities and connections are becoming more and more complex, it can be beneficial to start modeling the data as a graph built from vertices (modeled entities) and edges (relationships between the entities) that contains additional information about the entities and relations. 

Graph databases are designed to store and efficiently query the data persisted in such graph representation. Along with many algorithms capable of various computations on the graph structure, it can be suitable for modeling social network problems, web graphs, communication related networks or building knowledge graph. It can be also beneficial to use graph-like structure in problems related with knowledge discovery, because the underlying design allows to easily add new connections when the data model evolves and changes frequently.

Along the graph representations, databases provide also a declarative query languages to create and traverse the information preserved in graph representations that frees developers from specifying the execution details and make the optimizer choose the most efficient strategy to retrieve the data. Cypher is one of such graph query languages, and it is created for Neo4j \cite{Neo4j} graph database. \cite{DesignDataIntensiveApplications}.

\paragraph*{Search engines}

Search engines enable the possibility to perform a full-text search of the documents. It is similar to the document-oriented databases, but underneath the search engine analyzes the documents content and creates special indexable search terms, allowing further to perform queries and find documents based on their content. Additionally, different algorithms can be used to rank and filter the documents, perform linguistic analysis, support users suggesting other queries or handle the typos. The document indexing is quite complex and expensive operation and it can bring much overhead when the search engine is meant to run in scale \cite{DesignDataIntensiveApplications}. 

The most popular databases in that category, such as Elasticsearch \cite{Elasticsearch} and Apache Solr \cite{ApacheSolr} are build on top of the Apache Lucene project.

\subsubsection{Data management tradeoffs}

% \cite{PerspectivesOnArchitectureEvolution}

% - (Fox, Gribble, Chawathe, Brewer, Gauthier) - relying on many small machines in orchestration, rather than large machines - identical to the how cloud computing works now
% - in order to make the suggested system work, 3 requirements: incremental scalability and overflow growth provisioning, 24x7 availability through fault masking, cost effectiveness
% - highly scalable network services should scale linearly with the amount of hardware in use - high availability, cost effectiveness, relatively cheaper than forklift upgrades, making deliberate tradeoffs around consistency

\paragraph*{Strong consistency vs. high availability}

As described in chapter \ref{chapter:database-paradigms} the offering of available database management systems is varied. Before selecting one of the options, it is essential to understand the requirements that the database needs to fulfil, based on the business problem definition and guarantees of the suitable data model for it. 

Significant factors implying how the database stores and manipulates the data are the level of consistency and database transaction model. Most of the traditional, relational database management systems are compliant with the ACID semantics (atomicity, consistency, isolation, durability) that ensures strong consistency and serializability of the operations. Some of the business problems such as financial transaction processing or some analytical processing in data warehouses requires from the system to perform many simultaneous transactions while maintaining consistency. Such a model does not tolerate any invalid states and it is even preferable for that system to be unavailable rather than relaxing the ACID constraints \cite{PerspectivesOnArchitectureEvolution}.

Some of the problems does not require such strong consistency requirements and can tolerate small divergences in favour of the greater availability of the system. Most of the NoSQL databases providing a more flexible data model are designed to reflect the properties of BASE semantics (basically available, soft state, eventual consistency), which is less strict and more usable for some cases. Examples of such systems include social media and their feeds, e-commerce platforms or booking services, which tolerate small inconsistencies like stale data, leaving the product in the basket after deletion or allowing for overbooking, in favour of always responding to the user requests \cite{NoSQLDatabaseSystemsSurveyDecisionGuidance}.

\paragraph*{CAP theorem}

Along with the tradeoff between availability and consistency it is important to consider the case of hardware failures, when the machine on which the database is running on fails or the network communication between nodes within a cluster is interrupted. 

The CAP theorem \cite{CAPTheorem} states that at most 2 of the 3 properties from consistency, availability and partition tolerance can be achievable, denoting the upper bound on what is possible in the distributed database management system. The underying database system can be consistent (read and writes are consistent and all clients have the same view on the data) and available (all correct nodes accepts requests and return meaningful responses) Nevertheless, in case of network partition or faulty communication channels losing messages, the systems needs to decide upon consistency by delaying to respond or availability by accepting the requests that can violate the consistency of data \cite{PerspectivesOnArchitectureEvolution}.

However, the CAP theorem highlights the situation of the network partition, which is not as common and does not tell anything about the distributed database system when the hardware behaves correctly. The constraint can be further extended to the PACELC \cite{PACELC}, conveying the same idea in case of hardware failures, but when the system is operating properly the tradeoff can be made between the consistency of operations and the reponse latency \cite{NoSQLDatabaseSystemsSurveyDecisionGuidance}.

% \paragraph*{Eventual consistency}

% \cite{PerspectivesOnArchitectureEvolution}

% - (Fox, Brewer) - strong consistency is desireable property, all systems are probabilistic and are faulty
% - 2 solutions: high availability at the cost of consistency, decompose the systems into smaller components and take independent decisions for each component taking under consideration system requirements
% - eventual consistency - absence of strong consistency, 
% - (Obasi, Asagba) - update in the database will propagate through the system so that all copies will be consistent eventually
% - (Vogels) - lack of strong consistency as weak consistency - type of consistency in which writes in a system does not guarantee that subsequent access will return the updated value, some condition need to be made
% - eventual consistency - type of weak consistency - storage system guarantees that if no new updates are made, eventually all access will return the updated value - time when it happens is determined by communicatin delay, load on the system, number of replicas involved in the replication scheme
% - system - how long it takes to propagate the changes, user - how long it takes to read the fresh value
% - eventual consistency stronger than weak consistency, there are several types of consistency - causal consistency (chronological ordering of events), read your writes (client will never see older value than their latest write to the system), session consistency (read-your-writes within sessions), monotonic read (system never returns older if newer value was seen), monotonic write (writes by the same client applied in order)

\subsubsection{Message brokers}

The message brokers are frequently used to facilitate the construction of decentralized topologies, by decoupling the separate stages of the system and enable them to communicate with each other. The main goal of the message brokers is to provide a messaging middleware, which is capable of storing, validating, routing and delivering the messages to appropriate destinations, without knowing where the receivers are placed or if they are currently available. The exchanged messages are translated between formal messaging protocols in a platform agnostic way, which enables services written using different languages and technologies to communicate effectively.

To provide a greater reliability and guarantee message delivery, the message brokers rely on a group of message queues, storing the messages until they are consumed by the receivers. Such a type of inter-application communication allows to prevent the loss of valuable data and enables the system to continue running efficiently even with the connectivity or latency issues. Depending on the message broker implementation, different capabilities can be ensured, such as message ordering, exactly once delivery semantic or persisting the messages in the memory to prevent from the data loss in case of failures.
Moreover, various message distribution patterns can be utilised. The most common include point-to-point communication, in which only one recipient receives the message and consumes it once, or publish-subscribe model, where producers can publish a message to a certain topic and it will be distributed to multiple consumers subscribing to it.

The message brokers are frequently utilised in the systems built based on the microservice architecture, that can benefit from greater flexibility and scalability due to intermediate communication middleware. Each component of the system can be independently scaled, dynamically increasing the number of publishers and consumers of messages as well as redeployed and restarted without affecting other services, improving resiliency and fault tolerance of the system. Message Brokers find applications in various systems, whenever reliable, inter-process communication and message delivery is required, such as financial transaction and online payment processing or e-commerce order fulfillment services \cite{MessageBrokers}.

\subsection{Client Application Tier}

\subsubsection{Client application architectures}

\paragraph*{Single page applications}

The idea of the Single Page Application \cite{SPA} refers to the web application, clients running within the web browsers, rendering the web page content dynamically, based on the user interactions and the data received from the asynchronous communication with the server. The content of web applications can be highly interactive and contain significant part of the application business logic, thanks to the JavaScript code listening to the user-triggered events and the responses from the server.
Contrary to the static pages, the user interactions does not require to reload the view of the application, which enhances the user experience and makes it similar to the mobile and desktop clients \cite{PerspectivesOnArchitectureEvolution}.

The client-side rendering is performed in the browser using JavaScript containing all the application logic, data fetching mechanism, the templating and routing handled entirely on the client side. On the first request, the entire application code is loaded and executed making the website more responsive, which alleviates the need of requesting additional assets, besides the data required to be rendered as the content. Nevertheless, it comes with the significant and additional cost on the initial load, impacting the time when the application is ready to be used. Moreover, relying on the dynamic content introduces more problems, including among the others difficulties with parsing the page content by search engines. To address this problem, various techniques can be used, such as aggressive code-splitting along with the lazy-loading of the JavaScript code that is required for a given part of the application or caching the Application Shell, which is the core application logic, enabling interaction with the user \cite{GoogleRenderingOnTheWeb}.

Despite the disadvantages, Single Page Applications are extensively used for the highly interactive web applications with the frequently changing, dynamic content.

\paragraph*{Static site generators}

For the web pages including mostly static content, which does not change frequently, using the static site generators can be sufficient. Every time when the data changes, the generated content is rebuilt entirely or partially, to release an updated version.

The statically generated websites can be enhanced with the JavaScript code, dynamically loading parts of the content to make the initial content available fast, while enabling the dynamic parts when the page is fully loaded. Moreover, it prevents from the large initial load overhead known from the single page applications, by prerendering the static pages ahead of time and making it available based on the request \cite{GoogleRenderingOnTheWeb}.

Such approach gained additional interest and popularity recently, forming the idea of JAMstack, which refers to usage of JavaScript, reusable APIs and content preserved in the Markup files to render the websites. It highlights the performance benefits related with the prerendering of the static content, which can be further enhanced utilising content delivery network (CDN) to serve the assets closer to the users with lower latency. The maintainability and ease of the deployment of client architecture are lower due to reduced overhead limited only to serving the static content. Pre-rendering the sites is also beneficial for the search engine optimisation \cite{JAMstack}.

The usage of static site generators may not be feasible, when the set of URLs to be rendered ahead of time is large or it is not known initially.

\paragraph*{Server-side rendering}

The idea of server-side rendering refers to rendering the web pages on the server side, leveraging the reliable connection and returning the initial content to the client. After the browser renders the page structure, the JavaScript code can be loaded, attaching the event listeners to the HTML elements during the rehydration process. Running the initial logic and rendering the webpage on the server side addresses the problem of the large initial load and reduces the amount of JavaScript code sent to the client, improving the performance and user experience.

Most of the modern UI frameworks are capable of executing in both browser and server environments by providing the interface for generating the initial HTML, which later can be hydrated inside the browser. Moreover, the server-side rendering can be utilised only for the subset of pages, especially the computation-heavy, leveraging the benefits of this approach. Nevertheless, the dynamic nature of rendering pages on the server side brings additional overhead and resource utilisation to ensure it is executed in an efficient way \cite{GoogleRenderingOnTheWeb}.

\subsubsection{Communication protocols}

With the development of the technology driving web application clients, they started incorporating more business logic and became more sophisticated, which along with the asynchronous communication with the server application, made them more similar to the mobile and desktop clients. In order to make this possible, the server applications expose a well defined interface, establishing the communication contract between both parts, that most frequently utilise the HTTP protocol. Web clients running inside the web browsers, along with the mobile and desktop clients can use the API to make requests to the server application, in order to retrieve the data or perform some action. Moreover, one server can itself be a client to another service, which is a common approach in aforementioned microservice architecture, when the whole system is decomposed into numerous, separate services communicating with each other.

Over the years the web application architecture has evolved, along with the communication protocols enabling them to operate over the network, most frequently in a client-server manner. 


Despite its various forms, one of the main tasks of the server application is to expose the application-specific API, enabling its clients to utilise a predefined set of inputs and outputs determined by the application logic to obtain the data and restrict the actions that can be performed \cite{DesignDataIntensiveApplications}.

Different styles of building the web application APIs address different problem spaces and expose various tradeoffs. For example by granting more flexibility and customizability, the data cacheability can be more difficult to implement effectively.
The overview of the most commonly used patterns and protocols for client-server communication in the web application field is described below.

\paragraph*{REST}

The Representational State Transfer (REST) is an architectural style with design principles heavily based on the HTTP protocol, including simple data formats, usage of URL addresses for identifying resources and other HTTP features for authentication, content type negotiation and cache control \cite{RESTPrincipledDesingOfModernWebArchitecture}. It does not define the entire communication process, but rather a set of constraints over the interaction between components and interpretation of the data elements to minimize the communication latency and maximize the scalability of the system.

Scalability and visibility of the system is achieved by stateless communication between the client and server, which alleviates the need to store the session data on the server, which simplifies the communication and reduces the amount of used resources. On the other hand, it imposes a restriction on all the requests to be self-sufficient by containing all the necessary data to process them. Moreover, the data coming from requests and responses can be marked as cacheable, which in addition to the system composed from multiple layers can provide efficient data caching, leading to the performance improvement.

To ensure simplicity and evolvability, REST introduces the idea of resources, that are abstraction over the information stored in the application, their representations and operations that can be performed on them.
The idea of uniform interface in the client-server communication is achieved by preserving four constraints: the resources are identified based on the URI, they are manipulated through their representation, operations on resources needs to be descriptive based on resource address (URI), action (HTTP method) and the metadata (included in HTTP headers). The last restriction refers to the hypermedia, which contains the relations between linked resources and possible interactions between them.

Thanks to the mentioned advantages and its simplicity, REST gained great popularity and it is commonly used as a primary technology for building public APIs nowadays \cite{APIDesignInDistributedSystems}.

\paragraph*{Websockets}

WebSocket \cite{WebSocketRFC} is a protocol providing two-way asynchronous communication between the client and the server. It is developed as part of the HTML5 standard and natively supported in all major web browsers with the WebSocket API available in most of the commonly used programming languages. The protocol addresses the need for efficient, full-duplex communication in web sessions as an alternative to long-polling techniques and establishing multiple HTTP connections used for that purpose before.

WebSockets provide similar functionalities as regular TCP, but the protocol utilises HTTP as transport layer, which makes the communication backward compatible with underlying web infrastructure and inherits all its benefits, such as origin based security model as well as proxy and firewall traversal. It provides minimal framing to make the protocol frames cacheable in the communication intermediaries and support distinction between text and binary data, storing the message metadata in the application layer. Moreover, the protocol adds addressing and protocol naming mechanism, that allows it to support multiple host names on a single IP address and multiple devices on one port.

WebSocket meets the requirements of the modern and highly interactive web applications, enabling the asynchronous, bidirectional client-server communication. Examples of the protocol application include any web-based and real-time communication required chats, games and multimedia systems \cite{PerfomranceEvaluationonWebsocketProtocol}.

\paragraph*{GraphQL}

GraphQL \cite{GraphQL} is a declarative query language for fetching data from APIs and a runtime to fulfill the requests with the data. It was developed internally in Facebook in 2012 to address the performance problems during Facebook's shift to native mobile clients. The specification was open-sourced in 2015.

GraphQL presents a different approach, compared to REST and other web service architectures, in which clients can define the structure of the data, which is returned from the server. The API schema gives a complete description of the data supported by the API, describing its structure, underlying types and relations between entities, forming a central communication point to the clients.
Clients can use Queries to retrieve selected fragments of the data exposed by API, by defining the entities, their relations and specifying the fields that need to be obtained. To write or modify the data, Mutations can be used to change the preserved data and are allowed to cause side effects.
Additionally, clients can use Subscriptions, which are most commonly implemented using WebSockets, to listen for the data changes and receive them in real-time.

GraphQL enables to perform multiple queries and operations within a single request and retrieve specified properties of the resources, as well as resolve the references for nested entities. Most frequently, the GraphQL API is used in conjunction with HTTP and exposed as a single endpoint, accepting the POST requests and transporting the operation payload in the HTTP body. By defining the multiple queries in a request as well as specifying only the data required, GraphQL gives great flexibility and prevents from the data overfetching, by reducing the number of requests and amount of data being sent. Nevertheless, the greater flexibility introduces additional overhead in terms of resolving the queries on the server side and makes the effective data caching significantly more difficult to implement. GraphQL is most frequently used as API for web and mobile applications to optimise the amount of data being sent to the client side \cite{APIDesignInDistributedSystems}.

\paragraph*{gRPC}

gRPC \cite{gRPC} is an open-source, high performant Remote Procedure Call framework, which allows clients to make a request to a remote network service, in a similar manner as with calling a function or a method on the application level.
As with many other RPC tools, gRPC enables developers to define the service interface along with the methods that can be called by remote clients, with their types and parameters defined in a language agnostic way.
gRPC uses Protocol Buffers by default, which is a mechanism developed by Google for serializing structured data in a binary format, to define the service interface and the structure of the messages being serialized.
Based on this, the stubs code for the client and server can be generated, allowing to serialize and parse the data and providing accessors to the fields of the defined structures.

The binary encoding provided by Protocol Buffers allows to achieve better performance compared to other serialisation techniques, which is an appealing feature along with the benefits of HTTP/2 used underneath, enabling bidirectional streaming capabilities and real-time communication between the client and server.
gRPC allows for four kinds of service methods, the basic one utilise a regular request-response call. It can be enhanced with the streaming capabilities, in which sequence of messages can be sent from the server to the client, the opposite way or in both directions simultaneously.
The remote procedure execution is not only limited to the synchronous invocation, but can also be invoked asynchronously or terminated after a specified time, when the deadline is defined. Moreover, it supports tracing, load-balancing, health-checking and fully pluggable authentication.

Most frequently, the gRPC is used in the microservice architecture for efficient communication between services within the same organisation \cite{DesignDataIntensiveApplications}.

% ---
% NOTES
% ---

% \cite{DesignDataIntensiveApplications}

% noSQl - nonrelational diverged into 2 directions
% - document databases - data comes in self-container documents, relationships betewen documents are rare
% - graph databases - targeting use cases when anything is potentially related to everyhing

% - document and graphs db don't enforce a schema for the data stored - apps still asumes some structures
% ---

% - transaction - group of reads and writes taht form a logical unit
% - transactions don't need to be necessarily ACID

% different access patterns based on usage - differently for inline transaction processing (OLTP) and data analytics (extensive queries, scanning over large number of recors, calculates aggregates rather than returning raw data)
% - queries written by business analysts to help management do better business decisions - business inteligence - online analytics processing OLAP
% - trend for companies to stop using OLTP system for analytics and run analytics on separate database - data warehouse
% - OLTP - high availability and process transactions with low latencym since they are critical to the operation of the business

% data warehouse - separate database for analyst - containes read-only copy of the data in all various OLTP systems in the company - aggregate the data - data  from OLTP dumped periodically or as continuous stream transformed into analysis-firendly schema - the process known as Extract-Transform-Load (ETL)
% - warehouse can be optimized ofr analytical access patterns

% Main read pattern
% Main write pattern
% Primarily used by
% What data represents
% Dataset size
% ---
% Transaction processing systems (OLTP)
% Small number of records per query, fetched by key 
% Random-access, low-latency writes from user input 
% End user/customer, via web application
% Latest state of data (current point in time) 
% Gigabytes to terabytes
% ---
% Analytic systems (OLAP)
% Aggregate over large number of records 
% Bulk import (ETL) or event stream
% Internal analyst, for decision support 
% History of events that happened over time 
% Terabytes to petabytes

% - may seem similar, SQL - internals look different - optimized for very different query patterns

% start schema
% - fact-table - event that occured, individual level = max flexibility of analysis later - references to other rables called dimension tables
% - can be extremaly large - the fact table in the middle of the star surrounded by dimension tables

% snowflake schema - dimensions are further broken down into subdimensions - separate tables for brands and product categories

% ---

% 4. Encoding and Evolution

% % 7. Transaction - the meaning of ACID

% Linearizability - The CAP theorem - page 336

% 11. Stream Processing - page 439

% \cite{APIDesignInDistributedSystems}

% https://www.baqend.com/files/NoSQL-survey.pdf
% https://arxiv.org/pdf/1704.00411.pdf