\chapter{Web applications}

% aplikacje webowe - czym się charakteryzują?
% jakie założenia trzeba spełnić? jakie wymagania stawiane są przed deweloperami?
% ---
% What are the core ideas of web applications?
% what are the requiremenst from the system / components perspective?
% What architectures are commonly used?

\section{Origins}

The idea of web applications has been changing over the years alongside the increased interest in using them. Currently, web applications can take a variety of forms, which lead to the emergence of various architectures and solutions suitable to develop efficient and user-friendly software working in the browsers. Niels Abildgaard \cite{PerspectivesOnArchitectureEvolution} analyzed in a great detail how the perspective of web application developed over the years, discussing the variety of web application architectures and some of the common patterns and architectures used in their development nowadays.

The initial idea of the World Wide Web was to exchange informations easily with a broader audience. To achieve that the client-server architecture was used allowing clients to request servers for desired information. The client could make a request to the server using HTTP protocol and receive some static content (in form of HTML file or other static assets) or dynamically generated content (returned as a result of program execution triggered by the server). The term web applications is more suitable for the latter one, which represents a more interactive type of website, including some business logic and returning the dynamic response based on that.

The growth of server-side logic and it's complexity influenced the idea of splitting it into numerous Web Services responsible for particular part of application logic and communicating with other clients or servers. Service-Oriented architecture encouraged developers to split larger applications and enable them to cooperate using standardized communication protocols. Introduction of Representational State Transfer (REST) refined the client-server communication to be stateless, enabling cacheability and uniform communication using HTTP protocol.

Another idea which enabled developers to deal with the complexity of web application was to differentiate between specific layers in web application architectures. Early web applications distinguished presentation layers displayed to the users, which communicated with the business logic layer utilising data layer for persisting data. Other modern frameworks utilised Model-View-Controller (MVC) approach, in which the visual part was represented by the View, Controller based on the request was responsible to interact with the Model containing necessary data and business logic.

Next factor which led to enhancing the interactivity in the browser was the emergence of Ajax (Asynchronous JavaScript and XML), allowing to load data dynamically and asynchronously without reloading the whole page. Growth of JavaScript popularity enabled developers to build more interactive and dynamic applications detached from the initial markup, called Single-Page Applications. Additionally, more and more business logic started to be implemented on the client side of web applications, which became thicker clients providing better user experience similar to desktop applications.

Alongside the technology evolution, the requirements that the web applications need to fulfill are also developing, most often pushing them to the limits, providing better user experience while working on more demanding computation and processing larger amounts of the data than before. To meet such requirements some modern architectures and approaches emerged allowing to achieve desired capabilities, most frequently resulting with some tradeoffs.

---

TODO:rb - sum up patterns and architectures described later in the chapter

% server

% - scalable and efficiently handle large number of users making many frequent data retrieval and updates
% - growth of complexity of business logic - various patterns to handle that - smarter models than CRUD -> DDD, thinking about events
% - SOA -> to fix tangled dependencies, microservices that can be independently developed, managed and scaled

% client

% - web browsers capable of executing dynamic client-side behaviour
% - thick, highly rich and interactive clients with desktop like experience
% - increase in javascript engines in modern web browsers has lead to the development of many new web application architectures
% - large initial load of SPA, static site generators/JAMstack for context that rarery changes
% - PWA as web apps behaving as native mobile of web application, caching, working offline, accessing more native features of mobile devices
% - techniques to have high interactivity, but on the other side to optimise load time, performance
% - moving beyond req/res pattern, to proivde more direct communication - websockets

% database

% - users consume and produce lot of content, manipulate large amount of persisted data
% - emergence of NoSQL solutions - more scalable and efficient especially when working on large amount of data concurrently
% - NoSQL databases tend to have more relatex relations to the principles (ACID) in favour of speed and simple replication abilities


\section{Defining web application}

Taking into account the various types of web applications and numerous purposes they may serve, it can be difficult to clearly present the core ideas of web application. Researchers tried to define the term web application to describe the wide range of web applications. One of the definitions can be found in ''Web Application Architecture'' published by Leon Shklar and Richard Rosen \cite{WebAppArchitecture}.

\begin{quotation}
A Web application is a client-server application that (generally) uses the Web browser as its client. Browsers send requests to servers, and the servers generate responses and return them to the browsers. They differ from older client-server applications because they make use of a common client program, namely the Web browser
\end{quotation}

Nevertheless, the perspective of web application has been changing over the years as well as along with requirements they face. Web servers providing mostly static content changed into more complicated applications, that consist of multiple layers where each of them is responsible for a selected fragment of processing client request. For more complicated use-cases it was not enough and the server application took the form of a distributed system, that consist of multiple services communicating with each other to process the application logic. Alongside with the development of server application, the database management systems have been evolving, presenting new paradigms to meet the requirements of the web applications using them. Lastly, the presentation layer of web applications also changed rapidly, resulting in dynamic and interactive client-side applications including parts of business logic.

Niels Abildgaard also noticed the progress of technologies and architectures used in web application development and proposed another definition of web application based on his research \cite{PerspectivesOnArchitectureEvolution}.

\begin{quotation}
A web application is a client-server application with any number of clients and servers, in which client-server communication happens via HTTP, in which both client and server may be execution environments, and in which the server may be any arbitrarily complex system in itself.
\end{quotation}

Taking into account both definitions, we can refer to web applications used currently as mainly two communicating with each other parts - client (preferably used by web browser) and server, which can be any complex system by themselves.

\section{Requirements}

Having described the development of web applications over the years it is relevant to consider what were the requirements placed on the web application that lead to such a development process. Currently, web applications can operate in numerous areas and based on the business needs, target various customer groups and other non-functional requirements - the desired capabilities they need to fulfill may differ. For example, a small e-commerce shop serving a handful of customers a day within one country will have different requirements than a large social network in which millions of people participate worldwide, on a daily basis. Nevertheless, several generic requirements applicable to the majority of web applications can be distinguished, but the importance of them and to which extent they should be incorporated may differ depending on the application use-case and its scale.

\subsection*{Performance and Scalability}

The performance of web applications can be considered from two separate perspectives - user and system point of view \cite{DesignDataIntensiveApplications}. In the first case we can think of the user's interactions with the web application and the time that it needs to respond to requests made by the user. To define how the application should behave terms such as service level objectives (SLOs) and service level agreements (SLAs) can be introduced to characterise the expected performance and availability of web application (for example if the service median response is served within 200ms and for the 99th percentile is under 1s). Nonetheless, for most of the time the user is not using the application alone. The interactions of numerous users need to be taken into account by various components of the underlying application that need to handle them efficiently. When the load increases (in terms of the traffic, data volume or complexity of actions) the system should scale accordingly to reasonably handle the traffic without performance degradation. 

Scaling can be referred to as the ability to cope with increased load which can take various forms, for example number of requests per second, read to write ratio for database or number of simultaneously active users. To achieve that two different approaches can be used: scaling vertically (using more powerful machines) and scaling horizontally (distributing the load across multiple smaller machines). Some of the systems can automatically scale by allocating new computing resources based on the metrics related with traffic or performance, which is a desirable feature especially when the load is unpredictable. By building applications servers in a stateless manner, the horizontal scaling can be easily used to meet the demand. Unfortunately, stateful components can be harder to scale. Some storage components are designed and implemented to handle scaling gracefully, for other components techniques such as data fragmentation or replication have been introduced to meet the desired performance requirements.

\subsection*{Reliability}

Reliability refers to the fact that the system should continue to work correctly (perform correct processing as user expected at desired level of performance) even when the faults occur \cite{DesignDataIntensiveApplications}. The designed system should be resilient and fault-tolerant, preventing the web application from stopping providing the service to users. The aforementioned service level agreement (SLAs) metric can also refer to the uptime of the services, stating for example that not less than 99.9\% of availability time within a month is acceptable.

The origin of fault may be different, but some of the major factors can be hardware faults, software errors and bugs and human errors introduced while developing and managing the application. To prevent them various techniques can be introduced. Hardware faults in form of hard disk crash, power grid blackout or network cables that are not plugged in properly can be masked by introducing redundant components that are utilised until the broken component is replaced or fixed. Software errors in the form of introduced bugs or other human errors related with configuring services can result in unexpected and undesired behaviour of application or even other components of the system. That area of errors is harder to mitigate, but proper manual and automation testing can prevent from introducing errors, while monitoring and analysing service behaviour in production will raise an alert if some deviation is noticed.

The reliability is expected not only from larger web applications, but also from smaller one. Downtime and software bugs in the web application can contribute to loss in revenue and damage the reputation.

\subsection*{Security and Compliance}

These days web applications became one of the most popular platforms for exchanging information ranging their utilisation from numerous social media platforms to more critical areas such as bank accounting or accessing sensitive and confidential information stored on some servers in a digital form. The security breach of some web applications could result in compromising large amounts of sensitive data, leading to severe legal and economical consequences. Along with the growth of web application ecosystem and their complexity, the attack surface enlarges and the risk of introducing some security vulnerabilities is increasing.

It is substantial to ensure a proper level of security based on guidelines and frameworks defined by organisations and authorities working in the field of security, hardening the web application and mitigating the possible attacks to ensure compliance with security standards. Web application design should ensure that users identity and sensitive information are properly secured, giving access to resources only when the user identity is verified along with expecting adequate permissions \cite{ASurveyonWebApplicationSecurity}.

\subsection*{Maintainability}

Although maintainability may not be the obvious requirement for web application, it is an important factor for business executives, developers and people working in operations. Majority of the costs from a product development perspective is not the initial version of application, but it is related to the maintenance taking form of keeping the system operational, fixing bugs and resolving failures \cite{DesignDataIntensiveApplications}.

To maintain the software efficiently, the operational aspects such as monitoring, patching, deployment and maintenance of a stable environment should be easy and mostly automated to make sure the system is running smoothly. From an engineering perspective the system should be simple, by removing unnecessary complexity connected with tight coupled modules, tangled dependencies and inconsistency in terminology or architectural patterns. By reducing the complexity the maintainability of the application will be easier. It will also increase agility and make the evolution of the system easier, which will result in reducing time of introducing new features and reacting to architectural changes required to make the system operate efficiently.

% TODO:rb to be considered - maybe mention in intro would be enough
% \subsection*{User Experience}

% Modern web application clients are more frequently becoming more complex, interactive and feature-rich applications including larger parts of business logic. The success of products based on web applications is more frequently connected with the fact how the user perceives it and it is becoming more important to make sure that application interface is aesthetic, while the overall user experience of the application is intuitive. Nevertheless, the user interface is not the only factor impacting how the user perceives the application, the efficiency and performance also plays a significant role, which is correlated with the whole system design.

% The idea of user experience (UX) is quite a broad term on the verge of cognitive science and human-computer interactions. It is considering not only the interface design, but also the system usability, ergonomics and performance, inspecting how the users feels when interacting with the system. Enhancing user experience is one of the main goals of user-centered design, inspecting the perception of the value of the system, how easy and efficiently users can perform various tasks and looking into processes within the system, if they are intuitive and pleasant for users. Many companies are using different analytic tools nowadays  to understand how users utilise the products.

% The diversity of users and platforms also plays an important role. One of the consequences of that is the fact that many applications are suitable to use from mobile phones, because more and more people use smartphones on a daily basis. Also accessibility issues are gaining more awareness to make web applications suitable for people with some of disabilities \cite{WhatIsUserExperienceDesign}.

\section{Modern web application architecture}

There are many potential perspectives considering the architecture of modern web applications when taking into account the areas they operate on and specified requirements that they need to fulfill. Below some of the used patterns and techniques used in modern web application architectures are discussed, allowing systems to meet the desired requirement and specification while working efficiently. It is divided into three sections taking into consideration the web application client which is the user interface that the user interacts with, which communicates with the server-side application tier responsible for performing the business logic using the underlying data layer to persist the application state. However, some of the discussed patterns goes beyond such classification, especially when they are referring to the communication between layers boundaries.

\subsection{Server Application Tier}

% TODO:rb graphics - how connected, what will be in the chapter

\subsubsection{Microservice architecture}

Microservice architecture is one of the trends in modern web application development gaining more interest, especially when taking into consideration various benefits and success stories of the biggest companies using it to run their services in the web. To consider microservices it is useful to compare it with the monolithic architecture, which refers to building server-side applications as a single logical executable unit responsible for handling the HTTP requests, execute the domain logic, retrieve and update data in the database and send back the adequate response. Such architecture can be effectively applied when building simple applications, but when the system grows and evolves over time it is harder to preserve good modularisation and keep changes within the modules they should belong to, expanding unnecessary complexity into other parts of the system. Altogether with the system growth, it may require to be scaled accordingly to the load. It can be achieved by running multiple instances of the server application behind load balancer, but it requires to scale the whole application, rather than particular components that require more resources. Another downside of the larger applications built as a monolith is the fact that every change requires rebuilding and deploying the whole application including the parts of the system, even not affected by changes.

Microservice architecture is described as a solution for these problems. We can refer to microservices as an approach to developing applications as a set of small services, that each of them run on its own and communicate via message passing using standard data formats and protocols via well-defined interfaces. Each of the services is built around specific business capabilities providing module boundaries within a domain it operates in - mitigating the problem of good modularisation. Moreover, each of the components can be deployed independently reflecting the notion of agile development and encouraging to utilise continuous deployment. Scaling can be also applied in a more granular manner affecting only the services that require more resources to perform effectively under the load. \cite{FowlerMicroservices}

\paragraph{Decoupling system components}

Separating larger systems into independent services can help with ensuring firm module boundaries within the domain of the system. It makes it easier to maintain the components overtime and introduce required changes independently from other components. Alongside with the system decomposition, various teams can take care of selected services focusing on them in greater details. Moreover, greater granularity enables to focus only on the requirements specified for the particular component, applying required changes to meet them, rebuilding it to be more performant or splitting into separate components when the evolution of managed context will require that.

Due to services separation, each of them can be written using different languages, frameworks and libraries that suits best for the performed task. The underlying data storage can be alse selected independently, because each of the components manages its own database. However, most frequently the set of used technologie is limited to the predefined set for easier maintenance. Such opportunities encourage to experiment and try out new tools that can be applied through a gradual migration when they introduce desired capabilities \cite{FowlerMicroservicesTradeoffs}.

\paragraph{Microservices deployment}

Another consequence of the service separation is the possibility of independent deployments. More teams and products embrace the idea of continuous integration and continuous delivery using automation heavily to test software and verify the quality of it. Another often accompanying process is continuous delivery enabling teams to release a new version of the product to the production environment more frequently, even many times a day. Various modern deployment techniques make it possible to introduce new versions of the software with no service downtime. Such an idea is appealing from the product management perspective, reducing cycle-time between ideas and introducing new features into the product, responding quickly to the market changes. In case of microservices the idea of infrastructure and operational automation is essential to efficiently handle the independent service management and ensure good product quality \cite{FowlerMicroservicesTradeoffs}.

\paragraph{Scaling microservices}

Microservices are typically packed and deployed using containers to any platform supporting containerisation. It enables the operation teams to effortlessly relocate and replicate services across heterogeneous platforms. Each of the system components can be easily replicated, locating multiple services on the same host and dynamically scaling them according to the load, independently from other services which may not experience an increase in traffic. Due to that system can remain performant by allocating additional hosts when it is needed and deprovisioning resources when they are becoming redundant. Such a feature makes microservices a perfect technology to use in the cloud.

The replication of services and spreading them across different hosts ensures fair distribution of the load and increases the availability of the components in case of the hardware failures making the system robust and fault tolerant. Due to the fact that microservices are effectively a more complex distributed system compared to monolithic architecture, it is a desired feature to make the components designed for failures. Errors in software and hardware failures are inevitable, in case of the unavailability of other suppliers the service needs to respond as gracefully as possible. Due to that microservices value service contracts introducing patterns like tolerant reader and other consumer driven contracts enabling to evolve services independently, while on the other hand introducing patterns like circuit breaker to make sure that failure of one of the components will not affect the whole system. To prevent the system degradation in case of failures the application should be tested when some of the services will be unavailable. Proper monitoring setup should detect the system failures quickly and automatically restore the services if possible. Moreover it can be utilised to measure various business relevant metrics, providing warning and triggering alarms when some inconsistencies in the system behaviour will be noticed \cite{MicroservicesHowToMakeYourApplicationScale}.

\paragraph{Downsides of microservice architecture}

Nonetheless, microservices introduce additional complexity and are not free from downsides. It can be a good solution when handling large enterprise applications, but for smaller products it can introduce unnecessary overhead in contract to simpler monolithic architecture. Microservices architecture operates as a system composed from multiple communicating with each other services, effectively forming a distributed system. To ensure the acceptable performance asynchronous communication model is used most frequently. It has an implication on the system design, introducing more potential for failures, but on the other hand with the components designed for failure it can improve the resilience and reliability of the system.

The decentralised data management allows each of the services to persist its state independently from others. In the monolithic architecture it is easier to update several records within one transaction ensuring the consistency of application state, but in the microservices it is much more costly to perform such an update. From the business perspective it is important to ensure high availability of the system. For some of the business processes that are resistant to temporal inconsistencies, transactionless coordination between services can be applied or some sort of aforementioned asynchronous processing which ensures eventual consistency. It enables the system to work efficiently, while requiring it to be designed to be tolerant to inconsistency windows between updates propagation.

The frequent and independant deployments are a great feature of microservice architecture, but handling the rapidly changing set of components is a great operational challenge. In the case of complex solutions, automation is essential to support administration of the system. From the development perspective working on smaller components of the system may seem to be easier, but the real complexity might not be eliminated, but rather split across multiple interconnected services. Designing and working with such a system requires a greater skill and can benefit from using more specified tooling. In that case, embracing DevOps culture can help with greater and tighter collaboration between developers and the operation team working together on the application \cite{FowlerMicroservicesTradeoffs}.

\subsubsection{Event-driven architecture}

When designing the system it is a common practice to model the fragment of reality as a set of entities and relations between them, then map them directly to the data model. Commonly there is an application layer within the application utilising a set of service modules containing and performing the application logic, which update the models depending on the result of the processing. The model capabilities are frequently limited to basic create, read, update and delete operations. This approach can be applied successfully in many simpler applications, although when the complexity of business logic grows, it may become more difficult to efficiently manage and develop it \cite{FowlerAnemicModel}.

To deal with the business logic complexity Eric Evans suggested an established set of practices and processes supporting modularization of large systems based on business context called Domain Driven Design. He presents the approach of thinking about the model of a complex system as a set of many smaller models, which are representations of separate business contexts. Each component in the system exists within its bounded context representing autonomous business domains and its model is used only within that scope, reflecting actions suitable for a given domain. It enables developers to better understand the underlying business logic domain and reason about the business processes and system behaviour instead of modeling it as a state of particular objects \cite{EvansDDD}.

\paragraph{Event sourcing}

Event sourcing is another approach influencing the way application state can be perceived. Instead of modeling the application logic in a structural way as a set of logical entities that are mapped and stored in the physical tables in the database, the events leading to the current application state can be stored. When the event occurs within the system, it is persisted and appropriate change to the application state is made, based on the event data and application logic responsible for interpreting it. The structural model saves only the current state of the system, while the event sourcing enables auditability in form of the sequence of events leading to particular state. The application at any time can be recreated by applying a sequence of events in an order in which they occured.

Most frequently to react to some events occurring within the system, some specialized component provides a mediating mechanism enabling for the event transmission in form of message bus or message broker. The latter one allows for indirect communication between numerous components subscribing for certain event occurrences and publishing them to all interested components, when the event occurs within the system.

\paragraph{Command query responsibility segregation}

Command query responsibility segregation (CQRS) is a design pattern frequently used together with event sourcing. It distinguished separation into two action types - commands (responsible for modifying the application state) and queries (capable of reading it). Each of the groups uses different models for updates and queries enabling optimisations that increase performance of such a solution. The write model can take the form of events, while the read model can be formed as a more complex domain model, which can be even denormalized to some extent to minimise the number of operations required to get the data and perform queries faster. For each of the types of the operations a different database solution can be selected, which satisfies the requirements of web application and enables independent scaling. 

Despite the advantages, event sourcing and CQRS comes with a cost. They increase the implementation complexity of a given solution and due to different components responsible for storing data for each of the models, the application state may not be strongly consistent \cite{MicroservicesArchitecture}.

\subsection{Data Tier}

\subsubsection{Database paradigms} \label{chapter:database-paradigms}

Over the years, traditional relational database managements systems established their position and proved to be a mature and stable solution for storing and quering the structured data. Nevertheless, with the growth of Internet and data volumes transferred over the network for example by social media or streamed from various sources, the relational model has shown some limitations related with scaling to handle such large amounts of data efficiently and at the same time maintain high-availability and fault-tolerance. NoSQL has emerged as a response to the needs of scalable database management systems, handling increasing data volumes and offering higher availability, but at the same time sacrificing consistency quarantees or quering capabilities as a tradeoff. Various NoSQL database management systems have some common characteristics including among the others less restrictive and more dynamic data model, support for running within the clusters and providing greater availability and performance metrics, which are desired apabilities for the services running in the Internet nowadays. As with many tools, NoSQL databases are not free from downsides and other undesired properties in regards to some class of problems. Relational databases are still playing a significant role and provide a specific combination of desired properties for other types of applications and they will be used alongside NoSQL solutions, depending on the problems and requirements that the system needs to fulfill \cite{FowlerNoSQL}.

The overview of the most common approaches regarding database management systems is presented below. It is divided into categories presenting the different database paradigms along with examples of such offerings and their application and most common use cases.

\paragraph*{Relational databases}

Relational models was proposed by Edgar Codd and refers to organising data into relations (tables) which consists of unordered collection of tuples (rows). It is efficient for storing and querying the data with regular and predefined structure, containing the relations between records from various tables based on their identifiers.

The data can be updated or queried using SQL which is a declarative language, enabling to specify the pattern of the data and leave the execution to the database engine. Most of the traditional, relational database supports strong consistency, transactional processing and are ACID compliant, that make them suitable for problems of transaction processing (banking transactions, sales) and batch processing (payroll, invoices). Besides that, the relational model is quite flexible and can be generalized for broad range of problems, which is justified by the fact that many web application us that model nowadays.

With the system evolvability and increase in it's complexity, the various situation from reality can be harder to model using the relational model. When the data schema is normalized to avoid the duplication and other undesired features, the queries that include joining multiple tables may require some time to be executed. Another concern refers to the fact that most application is built using the object-oriented programming languages, that require to translate the data between the relational and object-oriented model. Numerous object-relational mapping (ORM) libraries tends to reduce the amount of overhead and boilerplate code requires to translate the layers \cite{DesignDataIntensiveApplications}.

The most popular databases utilising relational model include among the others PostgreSQL, MySQL and Microsoft SQL Server.

\paragraph*{Key-value databases}

Key-value storage enable to store a set of key-value pairs and it is similar to data structures like hash map, dictionaries or objects available in some programming languages, but preserve the data in the memory. The data can be stored or retrieved by passing the unique key, which points to some arbitrary value preserved in the storage.
The storage does not assume any structure for the values itself, which can be an arbitrary value or more complex structure, leaving the interpretation of data to the application logic layer.
It exposes most frequently a limitted interface allowing to create, read, update and delete data, without possibilities to perform range queries or other more complicated operations \cite{NoSQLDatabaseSystemsSurveyDecisionGuidance}.

Most frequently, such simple key-value storage persist the data in memory, which enable them to perform operations with low latency and high throughput. Such databases are used most frequently for caching purposes, making it acceptable to lose the data when the machine is restarted. However, some of the offerings provides mechanism allowing them to preserve the data by utilising special hardware (battery-powered RAM), writing log of changes or making periodic snapshots preserved on the disk or replicate the in-memory state to other machines. Key-value storage owe their low latency by reducing the overhead related with encoding in-memory data structures that need to be written to the disk. Additionally some of such databases provide additional data structures such as priority queues or sets, that are harder to implement leveraging disk based indexes. Redis and Memcached can be distinguished as representatives of that category \cite{DesignDataIntensiveApplications}. 

\paragraph*{Document-oriented databases}

Document databases enable to store documents that are container for key-value pairs which form semi-structured documents, that are most frequently stored in formats such as JSON. The documents are grouped together in collections that form hierarchical structure and can be indexed for faster queries. The documents data schema is implicit and it's interpreted when thee documents are read, which grants better flexibility and possibility to store arbitrary keys and values in the documents, but on the other hand it does not guarantee any document structure and attribute possesion.

Instead of normalising the data and encoding the relations, the document model prefer to embed the data into single document, which is a tradeoff granting faster reads due to data locality, but on the other hand it can introduce data duplication, it makes the writing and updating documents more complex, which may be not suitable for the relational data forming connected structures. Nevertheless, discussed approach enables developers to query the documents by matching semi-structured data, selecting parts of them or performing aggregation tasks on the underlying data. 
The disadvantage of the model is the fact that the whole document need to be retreived when selecting only part of it and the entire documents need to be rewritten on updates.

The document model can be more convenient from the development perspective, reducing the impedence mismatch by working on the JSON format, which is more similar to the objects compared to the data from relational model. The representants of the document model databases are MongoDB and DynamoDB \cite{DesignDataIntensiveApplications}.

\paragraph*{Wide-column databases}

Wide-column databases enables to store items as rows described by multiple column families. Most frequently, they are technically implemented as distributed, multi-level sorted map. The first level called row keys, identify rows that themselves consist of key-value pairs. The second level called column keys, refers to arbitrary set of columns without defined schema, which are partitioned into column families colocating the data that are used together on the discs. Some of the databases introduce third level storing the timestamps. 

Such design enable to efficiently compress the data, collocate the informations used together for faster reads and effectively partition and distribute the data across nodes within the cluster based on the row key value to provide easier scaling, replication across multiple nodes and decentralisation. The model provides limited data querying possibilities most frequently allowing to constraint the records only on tthe row keys and does not support joins between tables.

The wide-column database design is sufficient for operations requiring frequent writes, but infrequent reads and updates such as recording time series, storing IoT device measurements. Cassandra and Apache HBase are some of the most popular options that belong to this category \cite{NoSQLDatabaseSystemsSurveyDecisionGuidance}.

\paragraph*{Graph databases}

When the data is characterized by many connections between modeled entities and connections are becoming more and more complex it can be beneficial to start modeling the data as graph built from vertices (modeled entities) and edges (relationships between the entities), that contain additional informations about the entities and relations. 

Graph databases are designed to store and efficiently query the data persisted in such graph representation. Along with many algorithms capable of various computations on the graph structure, it can be suitable for modeling social network problems, web graphs, communication related networks or building knowledge graph. It can be also beneficial to use graph-like structure in problems related with knowledge discovery, because the underlying design allow to easily add new connections when the data model evolces and changes frequently.

Along the graph representations, databases provide also a declarative query languages to create and traverse the information preserved in graph representations that frees developers from specifying the execution details and make the optimizer choose the most efficient strategy to retrieve the data. Cypher is one of such graph query languages and it is created for Neo4J graph database \cite{DesignDataIntensiveApplications}.

\paragraph*{Search engines}

Search engines enable the possibility to perform a full-text search of the documents. It is similar to the document-oriented databases, but underneath the search engine analyze the documents content and create special indexable search terms, allowing further to perform queries and find documents based on their content. Additionally different algorithms can be used to rank and filter the documents, perform linguistic analysis, support users suggesting other queries or handle the typos. The document indexing is quite complex and expensive operation and it can bring much overhead when the search engine is meant to run in scale \cite{DesignDataIntensiveApplications}. 

The most popular databases in that category such as ElasticSearch and Apache Solr are build on top of the Apache Lucene project.

\subsubsection{Data management tradeoffs}

% \cite{PerspectivesOnArchitectureEvolution}

% - (Fox, Gribble, Chawathe, Brewer, Gauthier) - relying on many small machines in orchestration, rather than large machines - identical to the how cloud computing works now
% - in order to make the suggested system work, 3 requirements: incremental scalability and overflow growth provisioning, 24x7 availability through fault masking, cost effectiveness
% - highly scalable network services should scale linearly with the amount of hardware in use - high availability, cost effectiveness, relatively cheaper than forklift upgrades, making deliberate tradeoffs around consistency

\paragraph*{Strong consistency vs. high availability}

As described in chapter \ref{chapter:database-paradigms} the offering of available database management systems is varied. Before selecting one of the options, it is essential to understand the requirements that the database needs to fulfil based on the business problem definition and guarantees of the suitable data model for it. 

Significant factors inmplying how the database store and manipulate the data are the level of consistency and database transaction model. Most of the traditional, relational database management systems are compliant with the ACID semantics (atomicity, consistency, isolation, durability) that ensures strong consistency and serializability of the operations. Some of the business problems such as financial transaction processing or some analytical processing in data warehouses requires from the system to perform many simultaneous transactions while maintaining consistency. Such model does not tolerate any invalid states and it is even preferable for that system to be unavailable rather than relaxing the ACID constraints \cite{PerspectivesOnArchitectureEvolution}.

Some of the problems does not require such strong consistency requirements and can tolerate small divergences in favour of the greater availability of the system. Most of the NoSQL databases providing a more flexible data model are designed to reflect the properties of BASE semantics (basically available, soft state, eventual consistency), which is less strict and more usable for some cases. Examples of such systems include social media and their feeds, e-commerce platforms or booking services, which prefer small inconsistencies like stale data, leave the product in hte basket after deletion or allow for overbooking, in favour of always responding to the user requests \cite{NoSQLDatabaseSystemsSurveyDecisionGuidance}.

\paragraph*{CAP theorem}

Along with the tradeoff between availability and consistency it is important to consider the case of hardware fault, when the machine on which the database is running on fails or the network communication between nodes within a cluster is interrupted. 

The CAP theorem states that at most 2 of the 3 properties from consistency, availability and partition tolerance can be achievable, denoting the upper bound on what is possible in the distributed database management system. The underying database system can be consistent (read and writes are consistent and all clients have the same view on the data) and available (all correct nodes accepts requests and return meaningful responses) Nevertheless, in case of network partition or faulty communication channels losing messages, the systems needs to decide upon consistency by delaying to respond or availability by accepting the requests that can violate the consistency of data \cite{PerspectivesOnArchitectureEvolution}.

However, the CAP theorem highlights the situation of the network partition, which is not as common and does not tell anything about the distributed database system when the hardware behaves correctly. The constraint can be further extended to the PACELC, conveying the same idea in case of hardware failures, but when the system is operating properly the tradeoff can be made between the consistency of operations and the reponse latency \cite{NoSQLDatabaseSystemsSurveyDecisionGuidance}.

% along with consistenct availability tradeoff...
% Additional factor that's worth to consider is the fact htat the hardware is fault and can fail

% - inherent tradeoff in distributed systems - wide range of NoSQL systems shows there is a wide spectrum

% \cite{FowlerNoSQL}
% - CAP - if Partition - choose Consistency or Availability (PACELC - partition rarte, than trading off Consistency or Latency)

% \cite{PerspectivesOnArchitectureEvolution}

% - (Fox, Brewer) - CAP theorem - only 2 properties are achievable from Consistency, Availability, Partition tolerance
% - highly consistent systems would rather deny writes, than have incorrent reads, in distributed systems harder to achieve - confirming writes concerns the correct state of all nodes

% - (Abadi) adds latency as property that is commonly considered as a tradeoff with other, CAP is very rigid - posits limitations in a face of certain types of failures, does not constraint system capabilities during normal operation
% - 3 implementations for implementing data replication: system sends data to all replicas (low latency, low consistency), to aggreed upon master node or single, arbitrary node (high consistency, at a cost of high latency for update propagation)
% - PACELC - when given partition - tradeoff between availability and consistency, else tradeoff between latency and consistency
% - partition is a rare case, in case of no partition - tradeoff between latency and consistency may be likewise - latency should be considered when amking tradeoffs in a distributed systems databases

% \cite{NoSQLDatabaseSystemsSurveyDecisionGuidance}

% - CAP Theorem by Eric Brewer, proven by Gilbert and Lynch - truly influencial impossibility result in the field of distributed computing - upper bound on what can be accomplished

% - it states - sequential consistency read/write registers eventually responding to every request cannot be realised in async system prone to network partition
% - at most 2 of the 3 properties are achievable at the time
%     - Consistency - read and writes executed atomically, are strictly consistent, all clients have the same view on the data
%     - Availability - every non-failing node it he system always accepts read/write requests and return meningful response
%     - Partition-Tolerance - system is still consistent and available in the presence of message loss between nodes or partition in the system

% - when Partition - responding system will be available breaking Consistency, waiting system won't Availabile to preserve Consistency

% - CAP theorem does not tell us anything about the system - it tells only about favouring availability or consistency in the face of partition

% - PACELC - addressed by Danel Abadi, CAP theorem fails to capture the trade-off between latency and consistency during normal operations
% - more influencial on the desing of distributted systems thean the availability-consistency in failure scenario
% - Partition - Availability vs. Consistency / Else - Latency vs. Consistency

% \paragraph*{Eventual consistency}

% \cite{PerspectivesOnArchitectureEvolution}

% - (Fox, Brewer) - strong consistency is desireable property, all systems are probabilistic and are faulty
% - 2 solutions: high availability at the cost of consistency, decompose the systems into smaller components and take independent decisions for each component taking under consideration system requirements
% - eventual consistency - absence of strong consistency, 
% - (Obasi, Asagba) - update in the database will propagate through the system so that all copies will be consistent eventually
% - (Vogels) - lack of strong consistency as weak consistency - type of consistency in which writes in a system does not guarantee that subsequent access will return the updated value, some condition need to be made
% - eventual consistency - type of weak consistency - storage system guarantees that if no new updates are made, eventually all access will return the updated value - time when it happens is determined by communicatin delay, load on the system, number of replicas involved in the replication scheme
% - system - how long it takes to propagate the changes, user - how long it takes to read the fresh value
% - eventual consistency stronger than weak consistency, there are several types of consistency - causal consistency (chronological ordering of events), read your writes (client will never see older value than their latest write to the system), session consistency (read-your-writes within sessions), monotonic read (system never returns older if newer value was seen), monotonic write (writes by the same client applied in order)

\subsubsection{Message brokers?}

% % A Survey of Distributed Message Broker Queues - https://arxiv.org/pdf/1704.00411.pdf

% % \cite{DesignDataIntensiveApplications}

% % message brokers advantages over RPC
% % - buffer when recipienrs are unavailable or overloaded - imrpove reliability
% % - can redeliver messages when process has crushed - prevent messages from being lost
% % - sender doesn't need to know receiver IP/port directly - useful with cloud deployment when machines are changing frequently
% % - broadcast message to several recipients
% % - logically decouple sender from the recipients

% % compared to RPC it's one way communication - received will need to send "response" as message the other way
% % - asynchronous communication pattern

% % popular implementations - RabbitMQ, ActiveMQ, Apache Kafka

% % - the semantics of delivery varies between implementations and configuration, but generally
% % - message brokers are immediate component
% % - sender sends message to named queue/topic and the broker is ensures that the message is delivered to one or more consumer or subscribers in queue/topic - there can be many producers and mony consumers on the same topic

% % - one way data-flow
% % - message brokers don't enforce any particular data model - message is just a sequence of bytes = flexibility

% ---

% \subsubsection*{[???] Database scalability}

% \cite{FowlerNoSQL}

% - NoSQL - running in clusters rather to grant scalability
% - sharding/fragmentation - puttin parts of data on different machines
% - replication - new class of consistency problem emerges - Consistency vs Availability - based on the business rules it could be one or another way, domain choice - risk of overbooking hotel rooms - Dynamo always available

% NoSQL database systems: a survey and decision guidance - https://www.baqend.com/files/NoSQL-survey.pdf

% \paragraph*{Sharding / Fragmentation}

% \cite{NoSQLDatabaseSystemsSurveyDecisionGuidance}

% - NoSQL - sharding, SQL - fragmentation
% - system consist from many servers that are not sharing memory, private discs, are connected through the network
% - high scalability and high throughput achieved by sharding data across different nodes

% - 3 distribution techniques: 
% - range-sharding - ordered and ocntiguous value ranges - requires coordiantion through a master to manage assignments - BigTable, HBase (wide-columnd), document store (MongoDB)
% - hash-sharding -  assigned to shard server according to hash value build using primary key, don'trequire master, ensure equal distirbution with good hash function, disadvantage - only allows lookups and makes scan unfeasible - Cassandra, Azure Tables, unfeasible for elastic systems when nodes can be added or removed, use of consistent hashing to distribute records across logical partitions which can be further reasigned to servers - elastic system - Dynamo, Cassandra, Riak
% - entity-group sharding - goal enabling single partition transactions on co-located data - group can be explicitly declared or derived from transactions' access patterns

% \paragraph*{Replication}

% \cite{NoSQLDatabaseSystemsSurveyDecisionGuidance}

% - RDBSM - single-server mode - entire system becomes unavailable on machine failures
% - NoSQL - designed ofr data and requet volume not possible to be handled by 1 machine - running on clusters of servers

% - failures are inevitable in any large distributed systems - need to cope with them somehow
% - many cases of network partitioning and outages in cloud data centers reported
% - REPLICATION = maintain availability and durability in face of such failures
% - storig the same record on different machines - synchronisation - tradeoff between consistency, latency and availability

% when
% - eager/synchronous replication - propagate incoming changes synchronously before commit and reply to the client - consistency, but higher latency, needs to wait for other replicas
% - lazy/asynchronous - applies changes only at receiving replica and passes them asynchronously - faster, but possible divergence

% where
% - master-slave (single-master) - changes can be only accepted by 1 replica, unavailble when master fails
% - update anywhere (multi-master) - every replica can accept requests, complex mechanism for resolving conflicts - versioning, vector clocks, gossiping, read repair (dynamod), convergent or commutative datatypes (riak)

% - four combinations are possible
% - distributed relational db - usually eager master-slave replication tp maintains strong consistency
% - NoSQL typically go for lazy maser-slave (MongoDB, HBase), update anywhere (Dynamo, Cassandra)
% - some of the NoSQL db leave the choice to the client - waiting for reponse from any replica, or by majority to prevent stale data
% - geo-replication protect against complete data loss, improve read latency

\subsection{Client Application Tier}

\subsubsection{Static and dynamic clients}

\paragraph*{Single page applications}

\paragraph*{Static pages}

\paragraph*{Hybrid solution}

\subsubsection{Communication protocols}

% API Design in Distributed Systems- A Comparison between GraphQL and REST
% GraphQL, gRPC or REST? Resolving the API Developer's Dilemma - Rob Crowley - https://youtu.be/l_P6m3JTyp0

% \cite{DesignDataIntensiveApplications}
% Dataflow Through Services: REST and RPC

% - when communicatin over the network - the most common arrangement includes 2 roles - client and the server
% - server expose some API, taht clinets can connect to and make requests, the API exposed by server is known as service
% - clients most frequently utilise web browsers - make a request to the server to get some data or perform some action
% - native app running on mobile devices and desktop apps can also be the API clients
% - moreover, a server can itself be a client to another service - used when decomposing the larges system into smaller services - aforementioned microservices architecture

% - services expose an application-specific API that only allows predefined set of inputs and outputs determined by the business logic - encapsulation and fine-grained restrictions on what clients can do

% - web services - when HTTP protocol is used for communication
%     - client application on user device - native app, web app running in the browser - making request to a service over HTTP
%     - 1 service making request to another service within the same organization, within the same datacenter - as in microservice architecture
%     - one service making requet to the another service from different organisation - data exchange between different organisations, some public service with public API - credit card processing, OAuth

\paragraph*{REST}

% \cite{DesignDataIntensiveApplications}

% - design philosophy to build upon principles of HTTP - simple data fromats, use URLs for identifying resources, using HTTP features for cache control, authentication, content-type negotiation
% - API designed according to the principles of REST is RESTful API
% - favour simple approaches, 

% - good for experimentation and debugging - visibility, support all mainstream programming languages and platfforms with vast ecosystem of tools
% - predominant style for public APIs

\paragraph*{Websockets}

\paragraph*{GraphQL}

\paragraph*{gRPC}

% \cite{DesignDataIntensiveApplications}

% - based on idea of Remote Procedure Call - tries to make a request to a remote network service look the same as calling a function or method in your programming language
% - recently various RPC frameworks have been build - most popular is gRPC which is an RPC implementation using Protocol Buffers
% - new generation of RPC frameworks - more explicit about the fact that it's a remote request and its different from local function call
% - some frameworks expose futures (promises) - encapsulate asynchronous action that can fail, these can be later combined when making requests to multiple services in parallel - similar to promises used in the web
% - gRPC support streams - in which call consists from a series of request and responses over time
% - custom RPC protocols with binary encoding can achieve better performance than JSON over REST
% - main ofcus on RPC frameworks is on requests between services owned by the same organization within the same datacenter - microservices architecture

% - Protocol Buffers - binary encoding libraries - originally developed by Google, Thrift by Facebook, require a schema for any data that is encoded
% - binary schema-driven formats, Protocol Buffers, Thrift - compact, efficient encoding, clearly defined forward and backward compatibility semantics,
% - schema usefull for documentation
% - code generation for statically typed languages
% - downside - hard to understand by human

% - JSON, XML - don't prescribe schema - need to include all boject field names within encoded data
% - binary encoding - both JON and XML use mor espace compared to binary formats

% Protocol Buffers (protobuf), Apache Trift (Facebook) - binary encoding libraries
% - require schema for any data that is encoded
% - each of them come with a code generation tool that takes a schema definition and produces classes/implementation schema in various programming languages
% - both of them provide backward and forward compatibility compatibility when preserving some predefined rules enabling the schema to change

% ---
% NOTES
% ---

% \cite{DesignDataIntensiveApplications}

% noSQl - nonrelational diverged into 2 directions
% - document databases - data comes in self-container documents, relationships betewen documents are rare
% - graph databases - targeting use cases when anything is potentially related to everyhing

% - document and graphs db don't enforce a schema for the data stored - apps still asumes some structures
% ---

% - transaction - group of reads and writes taht form a logical unit
% - transactions don't need to be necessarily ACID

% different access patterns based on usage - differently for inline transaction processing (OLTP) and data analytics (extensive queries, scanning over large number of recors, calculates aggregates rather than returning raw data)
% - queries written by business analysts to help management do better business decisions - business inteligence - online analytics processing OLAP
% - trend for companies to stop using OLTP system for analytics and run analytics on separate database - data warehouse
% - OLTP - high availability and process transactions with low latencym since they are critical to the operation of the business

% data warehouse - separate database for analyst - containes read-only copy of the data in all various OLTP systems in the company - aggregate the data - data  from OLTP dumped periodically or as continuous stream transformed into analysis-firendly schema - the process known as Extract-Transform-Load (ETL)
% - warehouse can be optimized ofr analytical access patterns

% Main read pattern
% Main write pattern
% Primarily used by
% What data represents
% Dataset size
% ---
% Transaction processing systems (OLTP)
% Small number of records per query, fetched by key 
% Random-access, low-latency writes from user input 
% End user/customer, via web application
% Latest state of data (current point in time) 
% Gigabytes to terabytes
% ---
% Analytic systems (OLAP)
% Aggregate over large number of records 
% Bulk import (ETL) or event stream
% Internal analyst, for decision support 
% History of events that happened over time 
% Terabytes to petabytes

% - may seem similar, SQL - internals look different - optimized for very different query patterns

% start schema
% - fact-table - event that occured, individual level = max flexibility of analysis later - references to other rables called dimension tables
% - can be extremaly large - the fact table in the middle of the star surrounded by dimension tables

% snowflake schema - dimensions are further broken down into subdimensions - separate tables for brands and product categories

% ---

% 4. Encoding and Evolution

% % 7. Transaction - the meaning of ACID

% Linearizability - The CAP theorem - page 336

% 11. Stream Processing - page 439

% \cite{APIDesignInDistributedSystems}

% https://www.baqend.com/files/NoSQL-survey.pdf
% https://arxiv.org/pdf/1704.00411.pdf