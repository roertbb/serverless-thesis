\chapter{Serverless computing}

% \begin{itemize}
%     \item Martin Fowler - https://martinfowler.com/articles/serverless.html
%     \item Mike Roberts (same author as above) - https://www.symphonia.io/what-is-serverless.pdf
%     \item Cloud Native Computing Foundation - https://github.com/cncf/wg-serverless/tree/master/whitepapers/serverless-overview 
%     \item Serverless Inc - https://serverless.github.io/guide/
%     \item Cloud Programming Simplified: A Berkeley View on Serverless Computing - https://arxiv.org/pdf/1902.03383.pdf
%     \item Serverless Computing: A Survey of Opportunities, Challenges and Applications - https://arxiv.org/pdf/1911.01296.pdf
%     \item Serverless architecture with AWS Lambda - https://d1.awsstatic.com/whitepapers/serverless-architectures-with-aws-lambda.pdf
%     \item Examples - AWS Serverless Applications Lens - https://docs.aws.amazon.com/wellarchitected/latest/serverless-applications-lens/wellarchitected-serverless-applications-lens.pdf
%     \item Examples - https://www.youtube.com/playlist?list=PLhr1KZpdzukdeX8mQ2qO73bg6UKQHYsHb
%     % \item https://leanpub.com/serverless-go/read
% \end{itemize}


\section{Origins}

The raise and growth of cloud computing significantly influenced the way how we think about servers management and server applications development. Around fifteen years ago most of the companies were almost entirely responsible for managing their software altogether with hardware it was running on \cite{RobertsChapin2017}. Around that time cloud computing raised and enabled companies to rent machines by time used, outsourcing bigger part of the operational overhead to the cloud provider. 

Amazon Web Services was one of the first providers that enabled to rent compute capacity by enabling Elastic Compute Cloud (EC2) in August 2006. It was one of the first Infrastructure as a Service (IaaS) product on the market, which allowed companies to run their server applications on provisioned hosts that were available within minutes from requesting them. 

Leveraging these services brings handful of benefits. It reduces labour cost by outsourcing hardware management to the provider and infrastructure cost by paing based on actual usage. Moreover, it enables to scale easily the number and type of servers in the correlation with traffic and the demand for processing. Finally it encourages new businesses to test their solution by decreasing the lead time utilising favourable accounting flexibility.

The cost competitiveness of the infrastructural outsourcing is made possible by economy of scale that providers operate on and technical improvements such as the hardware virtualization.

Shortly after that, other companies such as Google, Microsoft and DigitalOcean embraced the notion of public cloud and started providing services and resources from their data centers. At the same time, tools like Open Stack enabled to spin up similar services on self-hosted data centers forming private cloud.

The next step in the cloud evolution was Platform as a Service (PaaS). As a layer on top of IaaS added the operating system to the outsourced infrastructure stack and enabled to deploy the application code. In that model the platform takes responsibility for managing the operating system, patching, monitoring and running the application. Heroku can be distinguished as one of the most popular PaaS provider, while from self-hosted variants - open source project Cloud Foundry can be mentioned.

The growth of containerisation technologies such as Docker allowed to deliberate on the application system requirements more clearly and separate it from the operating system. Services capable of managing and orchestrating containers can be reffered to as Container as a Service (CaaS). Amazon Elastic Container Service (Amazon ECS) can be distinguished as one of the examples of public cloud services.  Furthermore, there are also self-hosted solutions such as Mesos and Kubernetes (which is also used by vendors, e.g. in the form of Google Kubernetes Engine (GKE)). 

Each of described services are further generation of infrastructure outsourcing, raising the level of abstraction and allowing to hand of more of our responsibilities and technology to the cloud vendor.
Despite the fact for each of the mentioned services the smallest unit of processing is some sort of server application working on the virtual machine or within the container.

Serverless is considered as a next step in the cloud computing progression. One of the first use of the serverless term can be assigned to an article written by Ken Fromm \cite{KenFromm}. It describes the notion of architecture migration from monolitic applications running on servers into distributed systems that consist of multiple applications, processes and data stores with the goal to perform various tasks and process numerous flows. One of the highlights is the fact that servers are still involved, but developers don't need to worry about managing them any longer. Computing resources can be used as services, that enable developers to think of their application on the tasks level, taking away the complexity of the backend infrastructure existing undeneath.

Such an architecture model was leveraged firstly by applications build on top of hosted database solutions like Parse (later accquired by Facebook) and Firebase around 2012 (obtained by Google). Nonetheless, the most groundbreaking event shaping Serverless architecture wat the announcement of AWS Lambda by Amazon in 2014, altogether with introduction of API Gateway in 2015. By the middle of 2016 other cloud vendors, such as Microsoft and Google embraced the new architecture and started offering their services for developing serverless applications. At the same time numerous open-source projects emerged.

\section{Defining serverless}

Despite the fact that the term serverless was first used nearly ten years ago it was widely adopted by leading cloud providers. Currently, it covers range of technologies and components of cloud architecture, nevertheless it lacks a clear and concise view on what serverless is. Various vendors, organisations and research groups tried to define what the serverless term means for them.

Cloud Native Computing Foundation is one of the organization working towards standarization of numeruous cloud-related components as well as maintaining sustainable ecosystems for cloud native software by bringing together and colalborating with various members of cloud community. According to ''CNCF Serverless Whitepaper'' \cite{CNCF}

\begin{quotation}
    \noindent Serverless computing refers to the concept of building and running applications that do not require server management. It describes a finer-grained deployment model where applications, bundled as one or more functions, are uploaded to a platform and then executed, scaled, and billed in response to the exact demand needed at the moment.
\end{quotation}

% Amazon Web Services has been the first cloud vendor that introduced serverless component to their service portfolio. Since that time it established position of one of the most mature cloud providers constatly offering innovations for various cloud services, having the largest market share. In ''Serverless Architectures with AWS Lambda'' \cite{AWS2017} whitepaper there is another definition

% \begin{quote}
%     Serverless most often refers to serverless applications. Serverless applications are ones that don't require you to provision or manage any servers. You can focus on your core product and business logic instead of responsibilities like operating system (OS) access control, OS patching, provisioning, right-sizing, scaling, and availability. By building your application on a serverless platform, the platform manages these responsibilities for you.
% \end{quote}

% Moreover, the same document emphasise the capabilities that platform or service needs to provide, to be considered as a serverless. Beforementioned features are listed below:

% \begin{itemize}
%     \item No server management 
%     \item Flexible scaling 
%     \item High availability and fault tolerance build into serverless components
%     \item No idle capacity with charging based on resource consumption
% \end{itemize}

Another definition, that put emphasis on serverless service capabilities can be found in ''What is Serverless'' by Mike Roberts and John Chapin \cite{RobertsChapin2017}:

\begin{quotation}
    \noindent A Serverless service:
    \begin{itemize}
        \item Does not require managing a long-lived host or application instance
        \item Self auto-scales and auto-provisions, dependent on load
        \item Has costs that are based on precise usage, up from and down to zero usage
        \item Has performance capabilities defined in terms other than host size/count
        \item Has implicit high availability
    \end{itemize}        
\end{quotation}

The cited definitions include insightful informations and emphasize the essential features of serverless architecture. Based on both of them we can distinguish important features of serverless architecture.

\begin{itemize}
    \item Serverless architecture defines the new model of developing and executing applications - Serverless architecture introduces new model of building applications that consist of multiple components configured together to execute the business logic defined by application code.
    \item It does not require maintain, provision and monitor servers or application instances - Serverless does not mean that there's no server, but enables to outsource the infrastructural overhead to the cloud provider.
    \item Deployment model is more granular - Having the application build from multiple independent components, the deployment model can use it to update only selected parts with granularity to the individual components
    \item Platform is responsible for provisioning, executing and auto-scaling application based on load - Underlying cloud platform is responsible for managing and provisioning machines that will execute the application code. With almost infinite resource pool and possibility to quickly allocate them applications can be scaled to the required demand almost instantly    
    \item The cost is based on usage - With more granular architecture that consist of multiple components and execution model supporting auto-scaling based on usage, each of the parts of the system can be billed based on actual usage of resources - with an accuracy of hundreds of milliseconds or number of executed operations
    \item The performance is not related with the host size - Some of the cloud providers enables customers to choose how much RAM allocate for the environment (that also scales proportionally with the CPU usage). Nevertheless, the configuration is abstracted from the capabilities of underlying machine that is used as application execution environment.
    \item The serverless components has build in high availability and fault tolerance - Despite the fact that developers are no longer concerned with servers, the underlying vendor's machines can still fail. Using serverless services we expect that cloud vendor will provide transparent high availability for it's services. Although, as developers it may be necessary to handle some errors and failure occurrences properly.
\end{itemize}

\section{Serverless components}

When considering serverless components we refer to numeruous range of technologies provided by cloud platform. Two different but overlapping areas can be distinguished - both of them are covered in more details in subsequent sections.

\begin{itemize}
    \item Backend as a Service (BaaS) - third-party API-based services capable of replacing some element of application
    \item Function as a Service (FaaS) - environment for running application code executed in event-driven model
\end{itemize}

% TODO: small summary?

Even though presented areas provide different building blocks of the application, they share similar features that fit into the assumptions of serverless architecture. They are listed below: 

\begin{itemize}
    \item require no resource management and are entirely provisioned by cloud provider
    \item their cost is proportional to usage
    \item utilise the event-driven processing model
\end{itemize}

\subsection{Backend as a Service}

% TODO - revisit after cloud providers chapter
% \cite{RobertsChapin2017} - Defining Serverless - BaaS

Nowadays, we are surrounded by many products and applications that touch upon various aspects of our lives. Despite the diversity of the subject of this software, we are able to distinguish and generalize several tasks that are fulfilled by most of the delivered products. 

Most of the applications need to store and manage some data. Depending the on the requirements we could think of storing it in a structural way in databases or regardless of shape in file storage.

Looking at the application logic, most of the products need some functionality to provide and manage user identity. It is not only limited to registering and logging users, but most of the time include integration with various social media platform aswell. 

The concept of Backend as a Service (BaaS) addresses aforementioned functionalities and describes vast area of external services that are capable of replacing some functionalities or parts of application by integration with third-party components hosted by cloud vendor. They can be accessed and integrated with the business logic through API defined by service provider. 

When thinking about backend components we can divide them into two areas. 

The first one will contain all of the replacement components for services that were hosted and managed in headquarters or own datacenters. We can assign to this category all kinds of database and storage services. Depending on complexity of application architecture we could also distinguish various message brokers and elements responsible for processing data streams coming from various sources.

To the second category we could include all services capable of replacing part of application logic. Instead of developing separate module responsible for identifying and authenticating users, the third-party service providing such functionalities can be integrated with existing application logic.  In most cases they will also provide other functionalities that can be integrated with the existing application through the API provided by the provider. For the discussed example, it could be the possibility to of registration via social media platforms, that would only require the appropriate configuration of the service. 

Specific services that fit into the BaaS model will be described in more details in the ... chapter divided into cloud providers

All of the components shares the common features characterized by the serverless architecture approach. They are provided by cloud vendor, who takes responsobility for managing, provisioning and scaling them depending on the load. Moreover, these allow to iterate faster on application development by replacing commonly repeated application features.

\subsection{Function as a Service}

% TODO: diagram similar to - https://github.com/cncf/wg-serverless/raw/master/whitepapers/serverless-overview/image_1.png

The second area of serverless components refers to Function as a Service (FaaS) that introduces a new architectural approach in terms of developing, structuring and deploying the application code oriented around individual operations and functions.

Mike Roberts briefly describes idea of FaaS in his article \cite{MartinFowler} based on his conclusions refering to AWS Lambda \cite{AWSLambda}. 
Similar to other other concepts of deploying and running applications using containers or Platform as a Service, the responsibility of managing server architecture and application processes is delegated to the cloud provider. However, the key difference is the fact that the function execution time is restricted in contrast to long-lived server processes. 
The functions are invoked in response to an event coming from various sources including inbound HTTP requests, database or storage updates, messages added to message queues or time triggered events. 
Based on them cloud provider handles underlying resources allocation, executing function in ephemeral containers, that are created purely based on runtime needed and destroyed shortly after the incoming event is processed by the application logic within the function.
Such a fine-grained execution model enables providers to easily and automatically scale function executions horizontally. 
It also affects the way how different pieces of application can be deployed - developers can upload new versions of one of the functions and the provider does all the work necessary for provisioning resources, managing processes and executing the code.

Based on "CNCF Serverless" whitepaper \cite{CNCF} selected operational aspects of serverless processing model are explained in more details.

\subsubsection*{Function lifecycle}

Before analyzing the execution model in more details, it's essential to examine the deployment process accompanying the development of example function. 

The developer is responsible to create the function code alongside with specifying one or many events upon which function should be triggered. Additionally, some metadata defining for example the function version, environmental variables, execution role and many other configuration values may be specified. 
The function prepared in that way is uploaded to the cloud and processed by dedicated builder entity, resulting with an artifact (that depending on platform and chosen runtime can be a binary file, some package or container image) that is later placed in sufficient data storage. 
Next, it is deployed on cluster managed by FaaS controller responsible for provisioning, controlling and monitoring function instances based on incoming events. 

Serverless framework may provide additional actions related to function management such as executing it, creating, publishing, updating, deleting the function or its metadata. Furthermore, particular versions can be labeled or aliased and runtime usage statistics and execution logs can be obtained.

The function execution model is based on event-driven model and happens within strictly limited period of time. 
The whole process begins when the event triggering particular function is dispatched. It is detected and registered by platform service.
The controller responsible for managing function executions looks for function associated with incoming event, obtains it's code and allocates addequate amount of resources from large resource pool. 
The new execution environment is created inside lightweight container and language runtime for the function is bootstrapped. 
When it's ready the triggering event is redirected and processed by the application logic contained in the function code. It's results can be send back to the event dispatcher if particular behaviour is defined.
The execution duration is limited by majority of service providers up to couple of minutes. After that time the function host is completed with timeout.
The lightweight container with the execution environment is destroyed. 

Most of the providers delay the container deletion up to a few minutes due to optimisation related with reusing a container instance when next event occurs in the system. Reusing the already initialised execution environment named ''warm start'', allows to reduce the startup latency related with resource allocation and runtime preparation. The opposite situation, when the new container instance with the function host process need to be created is called ''cold start''.

\subsubsection*{Function environment}

The specification of the function environment has been already drafted while describing the lifecycle. Due to improvements in techniques of software virtualization, cloud vendors can benefit from elastic and dynamic management of large resource pool. 

It enabled vendors to allocate lightweight and ephemeral containers serving as a execution environment for functions. These are created and scaled based on demand, executing code of one of the functions at a time and destroyed shortly after. Even though for the optimization purposes containers can be reused for subsequent event occuring in the system, but it is not quaranteed by the platform. 

Stateless containers architecture allows for automatically horizontal scaling. When multiple concurrent events occurs within the system, platform is capable of creating a separate container to process each of them in a matter of seconds. In order to handle spikes of traffic effectively, quick provisioning of containers and reducing their startup latency is required and it is topic of many research and improvements made in that area by supliers.

On the other hand, local state of executed function is much more limited. The ephemeral containers with their internal data are dismissed shortly after the execution. There is no quarantee that the internal storage will be preserved across multiple function execution making them efectivelly stateless. To address this issue, some external component need to be used to persist the state. Nevertheless, it introduces the need to communicate with external components and is often associated with additional delays resulting from the communication overhead.

\subsubsection*{Function invocation}

It has been already mentioned that serverless components work in an event-driven model. The developer's task is to define the appropriate configuration for mapping evets emitted by event sources with appropriate functions. Each of dispatched events can trigger one or more functions as well as function can be invoked by one or more event sources - there is a many-to-many mapping between function and event source. The mapping can also refer to particular version of the function or alias, which can greatly simplify the deployment process by replacing the function code for a given alias without modifying the configuration.

Among the various types of data sources, we can distinguish:

\begin{itemize}
    \item Endpoint Services - Most of the time associated with API Gateway component, which introduces mapping between requests coming from APIs such as REST or Websocket and associate them with corresponding function.
    \item Storage Services - BaaS components provided by a cloud vendor like databases, file storage or cache services, capable of storing various types of data. Events can be emitted based on various operations on data like creation, deletion or modification.
    \item Messaging Services - Services providing mechanisms for data streaming or working in publish-subscribe architecture.
    \item Scheduled Events - Emit events periodically at a given time or at a selected interval.
\end{itemize}

Due to nature of the data source, we can categorize them based on types of calls:

\begin{itemize}
    \item Synchronous Request - Covers the case when the client sends a request and waits for a response. Most frequently refers to HTTP requests.
    \item Asynchronous Message Queue Requests - Most frequently connected with publish-subscribe mechanism. Consists of the message publication for a defined group of subscribers, that there is not strict ordering. 
    \item Event Streams - It is based on streams of messages, logs or files, partitioning them into multiple shards. Collects records maintaining their order.
    \item Batch Jobs - Refers to splitting the bigger computation job into smaller tasks that can be proccessed in paralel by multiple functions. The entire process is completed when all subsequent tasks are finished.
\end{itemize}

\section{Benefits and challenges}

The emergence of serverless architecture has met with great interest from various companies that noticed numerous advantages of utilising the serverless approach. In addition to the promise of a significant cost reduction there are many others benefits related with development and operational side that made serverless architecture desirable. 

Nevertheless, serverless architecture also has many disadvantages inherently connected with it's nature. Some of them has been addressed by various cloud vendors working towards improving their services and mitigating the problems. Despite the fact that many companies adapted it the technology is not fully mature yet. There are many research underway in various areas by both researchers and cloud vendors.

Among the many reports and articles \cite{MartinFowler}\cite{Berkeley2019}, you can find listings of benefits and challenges assigned to the serverless architecture. These has been categorized and listed below.

% https://martinfowler.com/articles/serverless.html#benefits 
% challenges deeply - https://arxiv.org/pdf/1902.03383.pdf
% https://arxiv.org/pdf/1911.01296.pdf

\subsection{Benefits}

\subsubsection*{Operational cost reduction}

Serverless is another step in process of infrastructural outsourcing that started a few years ago when idea of Infrastructure as a Service emerged. 
The responsibility for managing and operating systems and other components it transfered to cloud provider, which is a common feature with IaaS and PaaS solutions.
What's different is the fact that serverless architecture introduces various BaaS elements that can replace selected parts of the application and bring significant reduction of development cost.
This brings serverless architecture closer to its promise in which developers can focus on application logic while the responsibility of managing resources and delivering required services is moved on to the underlying cloud platform.

\subsubsection*{Autoscaling with proportional cost}

Another key feature of serverless architecture is automatic and elastic horizontal scaling handled entirely by provider. 
It is also directly connected with the cost that is proportional to amount of resources needed to handle the demand for processing. 
With granular billing plan for most of the components it can bring significant cost reduction especially for services characterized by inconsistent or occasional traffic.
The load is handled autoamtically by cloud vendor platform which manage resource allocation based on demand and reduce amount of working components when system utilization decreases. 
It mitigates also problems of under-provisioning (when the system capacity was optimised for average traffic that could not be sufficient for heavier load) and over-provisioning (when the capacity of the system was prepared to work efficienly when traffic reached the top, but for modest load the available capacity was not entirely utilised).

\subsubsection*{Development improvements}

The application development process can be accelerated by leveraging numeroues BaaS services. Some of the well-developed and tested components can be integrated with the application logic replacing self developed modules. 
It can significantly decrease the initial development process from basic idea to working prototype build from handful of functions and components configured to interact with each other building the business logic. 
Furthermore, deployment of serverless application is also more convenient. Each of the function is separatelly packed and deployed with metadata to the cloud platform that takes responsibility for the rest. There is no need for adjusting and managing configuration or software on the host machines.

\subsubsection*{Reduced time to market}

Utilising the benefits of easier infrastructure outsourcing, reducing operational management and accelerating development speed, serverless architecture appears to be a concept desired by many product owners. 

In the era of lean and agile processes it can be used to quickly test and develop new features for existing applications... %  TOOD

% It can be leveraged to launching new features quickly.
% ... lean architecture -> towards continuous deployment

\subsubsection*{Greener computing}

% ---

\subsection{Challenges}

\subsubsection*{Vendor dependence}

loss of control

\begin{itemize}
    \item by using BaaS / cloud platform - giving up control of system
    \item serverless systems can also have downtime - provider's issues
    \item unexpected limits for particular services
    \item changes in functionalities / forced API changes
\end{itemize}

vendor lock-in

multitenancy problem

\begin{itemize}
    \item multiple instances of different customers running on the same machine - efficient for customer (economy of scale)
    \item improvements in virtualization - for customer may seem that we're alone in the system - some times problems with security (see data of others), robustness (error for one customer propagates to other), performance (one customer takes most of the resources)
\end{itemize}

security

\begin{itemize}
    \item using BaaS directly from mobile platforms - loss of protection from server side
    \item each function + metadata = resources security policy - give lowest possible access, how to manage them properly across whole organisation
\end{itemize}

\subsubsection*{Architecture limitations}

start up latency - cold start

stateless - need external storage to persist data

\subsubsection*{Development challenges}

testing

debugging

complexity of development

\subsubsection*{Operational challenges}

deployment

monitoring

% ---

% \subsubsection{Reduced operational cost}
 
% Outsourcing solution resulting in less operational work - cloud provider handles managing, operating systems, databases and other componnets. Patches and updates are also handled by vendor. Deployment and provisioning services is outsourced as well. There's no need to configure software such as Puppet/Chef od Docker to manage running server processes. Monitoring is also simplified and can be limited to  to mroe application-oriented metrics and statistics interesting for our customers, instead of verifying free disk space or CPU usage.

% \subsubsection{Reduced development cost}

% Cloud vendor provides services with common functionalities that can be integrated with application. There's less code to define, develop and test that saves engineering time and cost.
% \begin{itemize}
%     \item Auth0 - entire authentication flow, no need to develop that feature on its own
%     \item Firebase - client can communicate directly with server-side database, that removes database administration overhead
%     \item Mailgun - service responsible for processing, sending and receiving emails
% \end{itemize}

% \subsubsection{Horizontal auto-scaling with proportional cost}

% \begin{itemize}
%     \item automatic horizontal scaling managed by provider 
%     \item pay-as-you-go model - paying for the compute power used, no paying for idle time, granular cost per 100ms of execution. It's a source of savings for occasional requets, inconsistent traffic (paying extra for spikes, no need to have servers handling max traffic). Moreover it does not matter how many hosts we'll run, the cost will be the same for 100 lambdas running sequentially as for 100 lambdas running in concurrently.
% \end{itemize}

% \subsubsection{Easier operational management}

% \begin{itemize}
%     \item with autoscaling on provider side - there's no need to handle manual scaling or spend time on setup and maintanance of autoscaling for non-FaaS solution
%     \item reduced deployment complexity - provider handles autoscaling, no configuration of management tools, executing scripts, deploying containers - a fully serverless solution requires zero system administration
%     \item reducing time to market and allows for continuous experimentation - teams and products becoming increasingly geared towards agile processes and continuous deployment and delivery. New things can be developed and its deployment requires minutes - reduction in lead time
% \end{itemize}

% % Typically when operating services on servers we needed to plan how resources (RAM and CPU) we'll need for each of our servers and databases. Once the plan was ready, we needed to obtain the hosts, allocate it's resources and provision and maintain the running services later. There was a risk of over-provisioning, which is having resources capable of handling our peak expected load, that could happen just a few time within the year. 

% \subsubsection{Greener computing}

% \begin{itemize}
%     \item investing in cost-effective data centers, handling workloads of many customers allows vendor to manage resources more efficient, reducing impact on environment
% \end{itemize}

% \subsection{Challenges}

% \subsubsection{Vendor dependence}

% By outsourcing management and provisioning of computation, we rely on 3rd party vendor - lack of control, system downtime and outages, unpredictable behaviour during some spans, required forced API upgrades. 

% Going serverless involves giving up the full control of execution our software stack to cloud provider. From customer perspective there's not that much we can do to configure the environment where our code is running on. 

% Similar as with configuration, there's not much control over performance of our application and underlying serverless platform. Multiple layers of virtualization and abstraction allows to handle running our code on underlying platform, that alters the scheduling priorities and allocates resources in response to demand. Based on benchmarks some execution of lambdas can have drastically different performance characteristics. 

% \subsubsection{Multi-tenancy problem}

% Multitenancy - multiple instances for several different tenants (customers) running on the same machines, the same host application. The illusion that the customer is using the resources on it's own, but there are concerns around security (seing data of other customer), robustness (error of 1 customer propagates to other), performance (high-load for one customer allocates most of the resources)

% \subsubsection{Vendor lock-in}

% \begin{itemize}
%     \item different operational tools (deployment, monitoring), 
%     \item different FaaS interfaces and parameters, 
%     \item differences in serverless components that are used, 
% \end{itemize}

% Despite the fact that vendors offer services that can be generalized, there could be some implementation details that makes difference when comparing similar services between various vendors. On the other hand, there are open source platforms that are not tight to any vendor.

% \subsubsection{Unpredictable startup latency}
% % https://blog.symphonia.io/posts/2017-11-14_learning-lambda-part-8

% "Cold starts" are one of the most common performance issues. These can occur when function was invoked for the first time since in a while or when the lambdas configuration has been altered. It requires to download the lambda code, allocate resources for the lambda and initialize environemtn for our code. Once it's done, instantiated container can be reused by subsequent event that need to be processed, which is called a "warm start". That's one of the main sources of inconsistent performance of function execution.

% \subsubsection{Testing and debugging}

% \begin{itemize}
%     \item testing - unit testing lambdas is pretty straightforward due to stateless nature. On the other hand integration tests are much more challenging, especially if relied on external 3rd party services, can and should you stub these? VEndors enables developesr to run and test functions locally, but does it properly simulate the cloud environment. Integration tests are especially required due to granularity of lambdas and relying on 3rd arty services.
%     \item debugging - AWS and Azure provides possibility to debug functions locally. Remote debuging on vendor environment supported only by Azure
% \end{itemize}

% \subsubsection{Deployment, monitoring and observability}

% \begin{itemize}
%     \item deployment - Tools for deployment interact with underlying serverless platform via API. Most of the application are built from multiple components that need to be orchestrated in some order. Due to these factors deployment of entire serverless application can be more challenging.
%     \item monitoring - As one of the serverless benefits mentioned earlier - there's no need to monitor host related metrics. We can focus on gathering data associated with business functionalities. Cloud platforms include tools for gathering logs and monitoring system behaviour, but most of them are limited and don't provide a log analysis platform. Moreover, distributed monitoring across multipel serverless components is more challenging. 
%     \item observability - 
% \end{itemize}

% \subsubsection{Security concerns}

% \begin{itemize}
%     \item Relying solely on vendor - as customers we don't need to take care about that, but all the vulnerabilities of vendor becomes our problem
%     \item using BaaS components directly from mobile clients - losing protective barrier, needs to be coverred properly when designing and developing the application
%     \item configuration of proper security policies for every lambda connected with various services
% \end{itemize}

\section{Examples}

\begin{itemize}
    \item showing the impact of serverless architecture on building the application - example of 3 tier application (client, server, database) + implication of migrating into serverless architecture
    \item what's not working due to nature of FaaS / serverless components
    \item production examples - AWS show me your architecture
\end{itemize}

% \subsection{UI-driven application}

% Lack of classic 3 tier architecture - Client, Server and Database. 

% Servers side logic has been distributed between multiple components: 3rd party BaaS responsible for Authentication Auth0). Cleint can access subset of database (Google Firebase). Compute intensive operations like search executed based on request (event) from API Gateway. Purchase functionality replaced with another FaaS due to security. There's no central server - choreography of components over orchestration. It gives more flexibility, division of concerns, but on the other hand requires destributed monitoring, there's greater number of moving parts

% \subsection{Message-driven application}

% Based on message comming to the system (event), many functions can be executed - asynchronous message processing (event-driven). Both message broker and Faas env exposed by provider, multiple functions can be executed in parallel.

\section{Summary}

% \section{Implications/characteristics of serverless architecture (in context of web applications)}

% characteristics/summary based on literature and examples

% Alternatives

% To prevent vendor lock-in - building applications from universal, open source components - not always leveraging full potential of what cloud provider can offer

% Deployable solutions - Hasura on Postgres as replacement for Firebase